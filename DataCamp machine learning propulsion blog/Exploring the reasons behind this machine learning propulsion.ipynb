{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why this sudden machine learning propulsion?\n",
    "\n",
    "Learn about the factors that helped drive/ are helping drive this machine learning outrage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning systems have been around since the 1950s, so why are we suddenly seeing breakthroughs in so many diverse areas? Let me give you some names and numbers to make it even more intriguing. \n",
    "\n",
    "- Let’s start with a very simple yet popular name; Linear Regression. It was discovered by [Galton](\"https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910537\") in the year of 1875. \n",
    "- Next, I would pick Support Vector Machines. The original SVM algorithm was invented by [Vapnik](\"https://en.wikipedia.org/wiki/Vladimir_Vapnik\") in 1963. \n",
    "- Now let’s come to the mighty Neural Networks! The predecessor of Neural Networks i.e. the Perceptrons were developed by [Frank Rosenblatt](\"https://en.wikipedia.org/wiki/Frank_Rosenblatt\") in 1950s. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds astonishing! Isn’t it? \n",
    "\n",
    "When I discovered about these facts I was quite amused. So, I decided to dig it deeper and find out the reasons(s) why such a well-studied and well-researched thing like Machine Learning was underground. In this post, I am going to share my experience I gathered from this finding. \n",
    "\n",
    "I believe, three essential factors are at play here: enormously increased data which is the fuel of machine learning, significantly improved algorithms, and substantially more-powerful hardware resources. Over the past two decades (depending on the application) data availability has increased as much as 100x times, key algorithms have improved 10x to 100x, and hardware speed has improved by at least 100x. According to MIT’s Tomaso Poggio, these can combine to generate improvements of up to a millionfold in applications such as the pedestrian-detection vision systems used in self-driving cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, you will take a closer look at these factors more closely. Let's get started. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "Music CDs, movie DVDs, and web pages have been adding to the world’s stock of digitally encoded information for decades, but over the past few years the rate of creation has exploded. Signals from sensors in smartphones and industrial equipment, digital photos and videos, a nonstop global torrent of social media, and many other sources combine to put us in a totally unprecedented era of data abundance. Ninety percent of the digital data in the world today has been [created in the past two years](\"https://www.ibm.com/analytics/us/en/technology/big-data/\") alone. With the burgeoning internet of things (IoT) promising to connect billions of new devices and their data streams, it’s a sure bet we’ll have far more digital data to work with in the coming decade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms \n",
    "\n",
    "The data deluge is important not only because it makes existing algorithms more effective, but also because it encourages, supports, and accelerates the development of better algorithms. The algorithms and approaches that now dominate the discipline — such as deep supervised learning and reinforcement learning — share a vital basic property: Their results improve as the amount of training data they’re given increases. The performance of an algorithm usually levels off at some point, after which feeding it more data has little or no effect. But that does not yet appear to be the case for many of the algorithms being widely used today. At the same time, new algorithms are transferring the learning from one application to another, making it possible to learn from fewer and fewer examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer hardware\n",
    "\n",
    "Moore’s Law — that integrated circuit capability steadily doubles every 18 to 24 months — celebrated its 50th anniversary in 2015, at which time it was still going strong. Some have commented recently that it’s running up against the limits of physics and so will slow down in the years to come; and indeed, clock speed for standard microprocessors has leveled off. But by a fortuitous coincidence, a related type of computer chip, called a graphic processing unit, or GPU, turns out to be very effective when applied to the types of calculations needed for neural nets. In fact, speedups of 10X are not uncommon when neural nets are moved from traditional central processing units to GPUs. GPUs were initially developed to rapidly display graphics for applications such as computer gaming, which provided scale economies and drove down unit costs, but an increasing number of them are now used for neural nets. As neural net applications become even more common, several companies have developed specialized chips optimized for this application, including Google’s tensor processing unit, or TPU. According to Shane Legg, a cofounder of Google DeepMind, a training run that takes one day on a single TPU device would have taken a quarter of a million years on an 80486 from 1990. This can generate about another 10-fold improvement.\n",
    "\tAlthough, the hardware resources are very expensive and are unaffordable at times, but there are companies like Amazon, Google which provide cloud level resources. These cloud resources are quite affordable and can easily be used for wide scale machine learning development. In fact, a lot of start-ups and giant companies use them. So, this is an added plus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it altogether\n",
    "\n",
    "These improvements have a synergistic effect on one another. For instance, the better hardware makes it easier for engineers to test and develop better algorithms and, of course, enables machines to crunch much larger datasets in a reasonable amount of time. Some of the applications being solved today — converting sound waves from speech into meaningful text, for example — would take literally centuries to run on 1990s-vintage hardware. Successes motivate more bright researchers to go into the field and more investors and executives to fund further work.\n",
    "\n",
    "Further amplifying these synergies are two additional technologies: global networking and the cloud. The mobile internet can now deliver digital technologies virtually anywhere on the planet, connecting billions of potential customers to AI breakthroughs. Think about the intelligent assistants you’re probably already using on your smartphone, the digital knowledge bases that large companies now share globally, and the crowdsourced systems, like Wikipedia and Kaggle, whose main users and contributors are very wide spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all for this post. I sincerely hope that you find this post to be informative. Let me know your views/comments in the comments section.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
