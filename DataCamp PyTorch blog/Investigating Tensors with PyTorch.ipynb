{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Investigating Tensors with PyTorch</center></h1>\n",
    "<br><span><a href=\"https://floydhub.com/run?template=https://github.com/sayakpaul/DataCampTensorWithPyTorchBlog\">\n",
    "    <img src=\"https://static.floydhub.com/button/button-small.svg\" alt=\"Run\">\n",
    "</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning it is common to see a lot of discussion around tensors as the cornerstone data structure. Tensor even appears in name of Google’s flagship machine learning library: “TensorFlow”. Tensors are a type of data structure used in linear algebra, and like vectors and matrices, you can calculate arithmetic operations with tensors.\n",
    "\n",
    "On the other hand, <a href = \"https://pytorch.org\">PyTorch</a> is a python package built by <b>Facebook</b> that provides two high-level features: 1) Tensor computation (like Numpy) with strong GPU acceleration and 2) Deep Neural Networks built on a tape-based automatic differentiation system.\n",
    "\n",
    "In this tutorial, you will discover what tensors are and how to manipulate them in Python with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial you will cover the following:\n",
    "- Introduction to Tensors\n",
    "- Introduction to PyTorch\n",
    "- Installation steps of PyTorch\n",
    "- Few tensor operations with PyTorch\n",
    "- A simple neural network with PyTorch\n",
    "\n",
    "So, without further ado let's get started with the introduction to Tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array. According to the popular deep learning book called \"Deep Learning\" (Goodfellow et al.) - \n",
    "\n",
    "<i>\"In the general case, an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor.\"</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scalar is zero-order tensor or rank zero tensor. A vector is a one-dimensional or first order tensor and a matrix is a two-dimensional or second order tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following info-graphic describes tensors in a very convenient way: \n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/max/2000/1*_D5ZvufDS38WkhK9rK32hQ.jpeg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the intuition behind tensors in a more lucid way now. \n",
    "\n",
    "A tensor is the basic building block of modern machine learning. At its core its a data container. Mostly it contains numbers. Sometimes it even contains strings, but that is rare. So think of it as a bucket of numbers.\n",
    "\n",
    "But often, people confuse tensors with multi-dimensional arrays. As per StackExchange:\n",
    "\n",
    "Tensors and multidimensional arrays are different types of object. The first is a type of function, the second is a data structure suitable for representing a tensor in a coordinate system.\n",
    "\n",
    "Mathematically, tensors are defined as a multi-linear function. A multi-linear function consists of various vector variables. A tensor field is a tensor-valued function. For a rigorous mathematical explanation you can read <a href = \"http://math.stackexchange.com/questions/10282/an%C2%ADintroduction%C2%ADto%C2%ADtensors?%20noredirect=1&lq=1\">here</a>.\n",
    "\n",
    "So, tensors are functions or containers which you need to define. The actual calculation happens when there is data fed.  What you see as arrays or multi-dimensional (1D, 2D, …, ND) can be considered as generic tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's talk a bit about <b>Tensor notation</b>. \n",
    "\n",
    "Tensor notation is much like matrix notation with a capital letter representing a tensor and lowercase letters with subscript integers representing scalar values within the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://image.ibb.co/jDDhW8/Capture.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the operations that can be performed with scalars, vectors, and matrices can be reformulated to be performed with tensors.\n",
    "\n",
    "As a tool, tensors and tensor algebra is widely used in the fields of physics and engineering. It is a term and set of techniques known in machine learning in the training and operation of deep learning models can be described in terms of tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a Python based scientific computing package targeted for:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs.\n",
    "- A deep learning research platform that provides maximum flexibility and speed.\n",
    "\n",
    "Let's quickly summarize the unique features of PyTorch - \n",
    "\n",
    "- PyTorch provides a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, math operations, linear algebra, reductions. And they are fast.\n",
    "\n",
    "- PyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n",
    "\n",
    "- Most frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world. One has to build a neural network, and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.\n",
    "\n",
    "- PyTorch uses a technique called Reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as autograd, autograd, Chainer, etc.\n",
    "\n",
    "(While this technique is not unique to PyTorch, it’s one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.)\n",
    "\n",
    "- PyTorch has minimal framework overhead. We integrate acceleration libraries such as Intel MKL and NVIDIA (CuDNN, NCCL) to maximize speed. At the core, it’s CPU and GPU Tensor and Neural Network backends (TH, THC, THNN, THCUNN) are written as independent libraries with a C99 API.\n",
    "They are mature and have been tested for years.\n",
    "\n",
    "(Hence, PyTorch is quite fast – whether you run small or large neural networks.)\n",
    "\n",
    "- The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives. PyTorch's creators have written custom memory allocators for the GPU to make sure that your deep learning models are maximally memory efficient. This enables you to train bigger deep learning models than before.\n",
    "\n",
    "This <a href=\"https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b\">blog</a> does the comparison between PyTorch and Tensorflow very well. \n",
    "\n",
    "<b><i>P.S.: <a href = \"https://www.datacamp.com/community/tutorials/tensorflow-tutorial\">This DataCamp blog</a> is very good starter for getting started with Tensorflow. </i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of PyTorch is pretty straight forward. As PyTorch supports efficient GPU computation, it efficiently communicates with your Cuda drivers and performs things faster. \n",
    "\n",
    "You will be needing <i>torch</i> and <i>torchvision</i> for using PyTorch. Let's install them for a <b>Windows</b> environment. Later, you will see the steps for Linux and Mac as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have Python 3.5 with <i>Anaconda</i> setup. Also, I don't have CUDA. Run the following commands to install torch an torchvision:\n",
    "\n",
    "- pip3 install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp35-cp35m-win_amd64.whl \n",
    "- pip3 install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Please keep in mind that PyTorch does not support Python 2.7. So it is must have a Python version => 3.5.)\n",
    "\n",
    "If you have CUDA enabled on your machine feel free and run the following (this is for CUDA 9.0): \n",
    "- pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.1-cp35-cp35m-win_amd64.whl \n",
    "- pip3 install torchvision\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the installation steps for a <b>Linux</b> environment with Python 3.5 and no CUDA support:\n",
    "\n",
    "- pip3 install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp35-cp35m-linux_x86_64.whl \n",
    "- pip3 install torchvision\n",
    "\n",
    "Yes, you guessed it right! It is the same as the Windows' one. \n",
    "\n",
    "Now, if you have CUDA support (9.0) then the step would be: \n",
    "\n",
    "- pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Mac environment with Python 3.5 and no CUDA support the steps would be: \n",
    "- pip3 install torch torchvision  \n",
    "\n",
    "And, with CUDA support (9.0): \n",
    "- pip3 install torch torchvision \n",
    "\n",
    "(MacOS Binaries don't support CUDA, install from source if CUDA is needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope, you are done with the installation of PyTorch by now! \n",
    "\n",
    "Now, let's straight dive to some Tensor arithmetic with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor arithmetic with PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the PyTorch installation was successful, then running the above lines of code won't give you any errors. \n",
    "\n",
    "Now, let's construct a 5x3 matrix, uninitialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5991,  0.9365,  0.6120],\n",
      "        [ 0.3622,  0.1408,  0.8811],\n",
      "        [ 0.6248,  0.4808,  0.0322],\n",
      "        [ 0.2267,  0.3715,  0.8430],\n",
      "        [ 0.0145,  0.0900,  0.3418]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a matrix filled zeros and of data type long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensor directly from data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.5000,  3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you understood Tensors correctly, tell me what kind of Tensor <i>x</i> is in the comments section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a tensor based on an existing tensor. These methods will reuse properties of the input tensor, e.g. dtype (data type), unless new values are provided by user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]], dtype=torch.float64)\n",
      "tensor([[-1.2174,  1.1807,  1.4249],\n",
      "        [-1.1114, -0.8098,  0.4003],\n",
      "        [ 0.0780, -0.5011, -1.0985],\n",
      "        [ 1.8160, -0.3778, -0.8610],\n",
      "        [-0.7109, -2.0509, -1.2079]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double)    \n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    \n",
    "print(x)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get its size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Note that \n",
    "``torch.Size`` is in fact a tuple, so it supports all tuple operations.</p>\n",
    "\n",
    "Now, let's do an addition operation on Tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor addition:\n",
    "The element-wise addition of two tensors with the same dimensions results in a new tensor with the same dimensions where each scalar value is the element-wise addition of the scalars in the parent tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://image.ibb.co/f22q18/Capture_2.jpg\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2174,  1.1807,  1.4249],\n",
      "        [-1.1114, -0.8098,  0.4003],\n",
      "        [ 0.0780, -0.5011, -1.0985],\n",
      "        [ 1.8160, -0.3778, -0.8610],\n",
      "        [-0.7109, -2.0509, -1.2079]])\n",
      "tensor([[ 0.8285,  0.7619,  0.1147],\n",
      "        [ 0.1624,  0.8994,  0.6119],\n",
      "        [ 0.2802,  0.2950,  0.7098],\n",
      "        [ 0.8132,  0.3382,  0.4383],\n",
      "        [ 0.6738,  0.2022,  0.3264]])\n",
      "tensor([[-0.3889,  1.9426,  1.5396],\n",
      "        [-0.9490,  0.0897,  1.0122],\n",
      "        [ 0.3583, -0.2061, -0.3887],\n",
      "        [ 2.6292, -0.0396, -0.4227],\n",
      "        [-0.0371, -1.8487, -0.8815]])\n"
     ]
    }
   ],
   "source": [
    "# Syntax 1 for Tensor addition in PyTorch\n",
    "y = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3889,  1.9426,  1.5396],\n",
      "        [-0.9490,  0.0897,  1.0122],\n",
      "        [ 0.3583, -0.2061, -0.3887],\n",
      "        [ 2.6292, -0.0396, -0.4227],\n",
      "        [-0.0371, -1.8487, -0.8815]])\n"
     ]
    }
   ],
   "source": [
    "# Syntax 2 for Tensor addition in PyTorch\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do Tensor subtraction now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor subtraction:\n",
    "The element-wise subtraction of one tensor from another tensor with the same dimensions results in a new tensor with the same dimensions where each scalar value is the element-wise subtraction of the scalars in the parent tensors4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://image.ibb.co/hFRa18/Capture_3.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at <b>Tensor Product</b>:\n",
    "### Tensor Product:\n",
    "\n",
    "Performs a matrix multiplication of the matrices mat1 and mat2.\n",
    "\n",
    "If mat1 is a (n×m) tensor, mat2 is a (m×p) tensor, out will be a (n×p) tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do Tensor products in PyTorch like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9490, -0.6503, -1.9448],\n",
      "        [-0.7126,  1.0519, -0.4250]])\n",
      "tensor([[ 0.0846,  0.4410, -0.0625],\n",
      "        [-1.3264, -0.5265,  0.2575],\n",
      "        [-1.3324,  0.6644,  0.3528]])\n",
      "tensor([[ 3.6185, -0.0901, -0.9753],\n",
      "        [-0.8892, -1.1504,  0.1654]])\n"
     ]
    }
   ],
   "source": [
    "mat1 = torch.randn(2, 3)\n",
    "mat2 = torch.randn(3, 3)\n",
    "print(mat1)\n",
    "print(mat2)\n",
    "print(torch.mm(mat1, mat2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that torch.mm() does not broadcast. Let's discuss a bit about <i><b>broadcasting</b></i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting:\n",
    "The term broadcasting describes how arrays are treated with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two tensors are “broadcastable” if the following rules hold:\n",
    "\n",
    "- Each tensor has at least one dimension.\n",
    "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand this with PyTorch using the following code snippet: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=torch.empty(5,7,3)\n",
    "y=torch.empty(5,7,3)\n",
    "# same shapes are always broadcastable (i.e. the above rules always hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=torch.empty((0,))\n",
    "y=torch.empty(2,2)\n",
    "# x and y are not broadcastable, because x does not have at least 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# can line up trailing dimensions\n",
    "x=torch.empty(5,3,4,1)\n",
    "y=torch.empty(  3,1,1)\n",
    "# x and y are broadcastable.\n",
    "# 1st trailing dimension: both have size 1\n",
    "# 2nd trailing dimension: y has size 1\n",
    "# 3rd trailing dimension: x size == y size\n",
    "# 4th trailing dimension: y dimension doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# but:\n",
    ">>> x=torch.empty(5,2,4,1)\n",
    ">>> y=torch.empty(  3,1,1)\n",
    "# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have got a fair idea of broadcasting let's see if two tensors are \"broadcastable\" then what would be their resulting tensor.\n",
    "\n",
    "If two tensors x, y are “broadcastable”, the resulting tensor size is calculated as follows:\n",
    "\n",
    "If the number of dimensions of x and y are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n",
    "Then, for each dimension size, the resulting dimension size is the max of the sizes of x and y along that dimension.\n",
    "For Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 4, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can line up trailing dimensions to make reading easier\n",
    "x=torch.empty(5,1,4,1)\n",
    "y=torch.empty(  3,1,1)\n",
    "(x+y).size()\n",
    "torch.Size([5, 3, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 7])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but not necessary:\n",
    "x=torch.empty(1)\n",
    "y=torch.empty(3,1,7)\n",
    "(x+y).size()\n",
    "torch.Size([3, 1, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-72fb34250db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "x=torch.empty(5,2,4,1)\n",
    "y=torch.empty(3,1,1)\n",
    "(x+y).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get the concept now !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor product is the most common form of tensor multiplication that you may encounter, but there are many other types of tensor multiplications that exist, such as the <i>tensor dot product</i> and the <i>tensor contraction</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting a Torch Tensor to a NumPy array and vice versa is a breeze. The concept is called <b>Numpy Bridge</b>. Let's take a look at that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Bridge: \n",
    "The Torch Tensor and NumPy array will share their underlying memory locations, and changing one will change the other.\n",
    "\n",
    "Converting a Torch Tensor to a NumPy Array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1.,  1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "# A 1D tensor of 5 ones\n",
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Convert the Torch tensor to a NumPy array\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the NumPy array changed in value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the above sections you did some basic Tensor arithmetic like addition, subtraction and tensor products. Now in the following section you will implement a basic neural network in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a simple neural network using PyTorch:\n",
    "If you need a refresher on neural nets then <a href=\"https://www.datacamp.com/community/news/a-gentle-introduction-to-neural-networks-for-machine-learning-np2xaq5ew1\">this DataCamp article</a> is the best to look for. You might want to check the following links as well:\n",
    "\n",
    "- https://matrices.io/deep-neural-network-from-scratch/\n",
    "- http://neuralnetworksanddeeplearning.com\n",
    "- https://www.youtube.com/watch?v=bxe2T-V8XRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the implementing, let's discuss a bit about a concept called <i>Automatic Differentiation</i> which is central to all neural networks in PyTorch. This is particularly useful for calculating gradients in course of doing backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <i>autograd</i> package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backpropagation is defined by how your code is run, and that every single iteration can be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see Automatic Differentiation in action with a very simple code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a Rank-2 tensor of all ones\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do an addition operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  3.],\n",
      "        [ 3.,  3.]])\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some more operations on <i>y</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27.,  27.],\n",
      "        [ 27.,  27.]]) tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s backprop now because <i>out</i> contains a single scalar, out.backward() is equivalent to out.backward(torch.tensor(1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.5000,  4.5000],\n",
      "        [ 4.5000,  4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "\n",
    "# print gradients d(out)/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have got a matrix of 4.5. Let’s call the out Tensor “o”. We have that \\begin{align}o = \\frac{1}{4}\\sum_i z_i  \n",
    "\\end{align} where \\begin{align} z_i = 3(x_i+2)^2 \\ \\end{align}\n",
    "and  \\begin{align} z_i\\bigr\\rvert_{x_i=1} = 27 \\end{align}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, \\begin{align}\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)\\end{align}, hence \\begin{align}\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 \\end{align}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know about Automatic Differentiation and how PyTorch handles it, let's straight get to its implementation in PyTorch: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll create a simple neural network with one hidden layer and a single output unit. You will use <a href = \"https://medium.com/alchemicai/activation-functions-demystified-661d1183f5f8\">the ReLU activation in the hidden layer and the sigmoid activation in the output layer</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to import the PyTorch library. Neural networks can be constructed using the <b>torch.nn</b> package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you e define the sizes of all the layers and the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_in, n_h, n_out, batch_size = 10, 5, 1, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, you will create some dummy input data <b>x</b> and some dummy target data <b>y</b>. You will use PyTorch Tensors to store this data. PyTorch Tensors can be used and manipulated just like NumPy arrays but with the added benefit that PyTorch tensors can be run on the GPUs. But you will simply run them on the CPU for this tutorial. Although, it is quite simple to transfer them to a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, n_in)\n",
    "y = torch.tensor([[1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, you will define our model in one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_h, n_out),\n",
    "                     nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a model that looks like input -> linear -> relu -> linear -> sigmoid. There is another way to define your models which is used to define more complicated and custom models. It is done by defining our model in a class. You can read about it <a href = \"https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-custom-nn-modules\">here</a>.\n",
    "\n",
    "Now, it is time to construct your loss function. You will use the <b>Mean Squared Error Loss</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, don’t forget to define your optimizer. You will use the mighty Stochastic Gradient Descent in this one and a learning rate of 0.01. model.parameters() returns an iterator over your model’s parameters (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will run Gradient Descent for 50 epochs. This does the forward propagation, loss computation, backward propagation and parameter updation in that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.2399429827928543\n",
      "epoch:  1  loss:  0.23988191783428192\n",
      "epoch:  2  loss:  0.23982088267803192\n",
      "epoch:  3  loss:  0.2397598922252655\n",
      "epoch:  4  loss:  0.23969893157482147\n",
      "epoch:  5  loss:  0.23963800072669983\n",
      "epoch:  6  loss:  0.23957709968090057\n",
      "epoch:  7  loss:  0.23951618373394012\n",
      "epoch:  8  loss:  0.23945537209510803\n",
      "epoch:  9  loss:  0.23939454555511475\n",
      "epoch:  10  loss:  0.23933371901512146\n",
      "epoch:  11  loss:  0.23927298188209534\n",
      "epoch:  12  loss:  0.23921218514442444\n",
      "epoch:  13  loss:  0.23915143311023712\n",
      "epoch:  14  loss:  0.2390907108783722\n",
      "epoch:  15  loss:  0.23903003334999084\n",
      "epoch:  16  loss:  0.23896940052509308\n",
      "epoch:  17  loss:  0.23890872299671173\n",
      "epoch:  18  loss:  0.23884813487529755\n",
      "epoch:  19  loss:  0.23878750205039978\n",
      "epoch:  20  loss:  0.23872694373130798\n",
      "epoch:  21  loss:  0.2386663407087326\n",
      "epoch:  22  loss:  0.2386058121919632\n",
      "epoch:  23  loss:  0.23854532837867737\n",
      "epoch:  24  loss:  0.23848481476306915\n",
      "epoch:  25  loss:  0.23842433094978333\n",
      "epoch:  26  loss:  0.2383638620376587\n",
      "epoch:  27  loss:  0.23830339312553406\n",
      "epoch:  28  loss:  0.2382429838180542\n",
      "epoch:  29  loss:  0.23818258941173553\n",
      "epoch:  30  loss:  0.2381247729063034\n",
      "epoch:  31  loss:  0.2380656749010086\n",
      "epoch:  32  loss:  0.23800739645957947\n",
      "epoch:  33  loss:  0.2379491776227951\n",
      "epoch:  34  loss:  0.2378900945186615\n",
      "epoch:  35  loss:  0.23783239722251892\n",
      "epoch:  36  loss:  0.23777374625205994\n",
      "epoch:  37  loss:  0.23771481215953827\n",
      "epoch:  38  loss:  0.23765745759010315\n",
      "epoch:  39  loss:  0.23759838938713074\n",
      "epoch:  40  loss:  0.23753997683525085\n",
      "epoch:  41  loss:  0.2374821901321411\n",
      "epoch:  42  loss:  0.23742322623729706\n",
      "epoch:  43  loss:  0.23736533522605896\n",
      "epoch:  44  loss:  0.23730707168579102\n",
      "epoch:  45  loss:  0.23724813759326935\n",
      "epoch:  46  loss:  0.23719079792499542\n",
      "epoch:  47  loss:  0.23713204264640808\n",
      "epoch:  48  loss:  0.23707345128059387\n",
      "epoch:  49  loss:  0.2370160073041916\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    # Forward Propagation\n",
    "    y_pred = model(x)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- y_pred gets the predicted values from a forward pass of our model. You pass this, along with target values y to the criterion which calculates the loss. \n",
    "\n",
    "- Then, optimizer.zero_grad() zeroes out all the gradients. You need to do this so that previous gradients don’t keep on accumulating. \n",
    "\n",
    "- Then, loss.backward() is the main PyTorch magic that uses PyTorch’s Autograd feature. Autograd computes all the gradients w.r.t. all the parameters automatically based on the computation graph that it creates dynamically. Basically, this does the backward pass (backpropagation) of gradient descent. \n",
    "\n",
    "- Finally, you call optimizer.step() which does a single updation of all the parameters using the new gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you have made till the end. In this post, you covered a whole bunch of things starting from Tensors to Automatic Differentiation and what not! You also implemented a simple neural net using PyTorch and its tensor system. \n",
    "\n",
    "If you want to learn more about PyTorch and want to dive deeper into it, take a look at PyTorch’s official documentation and tutorials. They are really well-written. You can find them <a href = \"https://pytorch.org\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are some references which helped me writing this tutorial: \n",
    "\n",
    "- A Student’s Guide to Vectors and Tensors, 2011. <a href = \"http://amzn.to/2kmUvvF\">link</a>\n",
    "- PyTorch's official documentation (I already mentioned the link).\n",
    "- A gentle introduction to Tensors - <a href = \"https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/\">Machine Learning Mastery blog</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me know if you have something to ask or discuss in the comments section!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
