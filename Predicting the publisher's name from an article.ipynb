{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the publisher's name from an article: A case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will be building a machine learning model for predicting the publisher's name from an article using various Google Cloud technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before you proceed,the problem statement needs to have a bit more reality to it:\n",
    "\n",
    "> Imagine being the moderator of an online news forum and you're responsible for determining the source (publisher) of the news article. Doing this manually can be a very tedious task as you'll have to read the news articles and then derive the source. So, what if you could automate this task? So, at a very diluted level the problem statement becomes can I predict the publisher's name from a given article?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem can now be modeled as a text classification problem. In the rest of the article, you will be building a machine learning model to solve this. The summary of the steps looks like so:\n",
    "* Gather data\n",
    "* Preprocess the dataset\n",
    "* Get the data ready for feeding to a sequence model\n",
    "* Build, train and evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System setup\n",
    "\n",
    "You will be using Google Cloud Platform (GCP) as the infrastructure. It's easy to configure a system I would need for this project starting from the data to the libraries for building the model(s). \n",
    "You will start off by spinning off a Jupyter Lab instance which comes as a part of GCP's AI Platform. To be able to spin off a Jupyter Lab instance on GCP's AI Platform, you will need a [billing-enabled GCP Project](https://cloud.google.com/bigquery/docs/visualize-jupyter#beforeyoubegin). One can navigate to the Notebooks section on the AI Platform very easily:\n",
    "\n",
    "![](https://i.ibb.co/rby8yPD/Screenshot-from-2019-07-29-20-23-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clicking on the Notebooks, a dashboard like the following lands up:\n",
    "\n",
    "![](https://i.ibb.co/55BxghD/Screenshot-from-2019-07-29-20-25-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be using `TensorFlow 2.0` for this project, so choose accordingly:\n",
    "\n",
    "![](https://i.ibb.co/m62CNsq/Screenshot-from-2019-07-29-20-26-00.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clicking on *With 1 NVIDIA Tesla K80*, you will be shown a basic configuration window. Keep it default, just tick off the GPU driver installation box and then click on CREATE.\n",
    "\n",
    "![](https://i.ibb.co/GQWdyrz/Screenshot-from-2019-07-29-20-26-55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take some time to get the instance (~ 5 minutes). You just need to click on *OPEN JUPYTERLAB* to access the notebook instance after the instance is ready.\n",
    "\n",
    "![](https://i.ibb.co/dfqhqZT/Screenshot-from-2019-07-29-20-28-17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also be using [BigQuery](https://cloud.google.com/bigquery/) in this project and that too via the notebooks. So, as soon as I=you get the notebook instance, open up a terminal to install the BigQuery notebook extension:\n",
    "\n",
    "```\n",
    "pip3 install --upgrade google-cloud-bigquery\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for system setup part.\n",
    "\n",
    "> BigQuery is a serverless, highly-scalable, and cost-effective cloud data warehouse with an in-memory BI Engine and [machine learning](https://cloud.google.com/bigquery/#bigqueryml) built-in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do I get the data?\n",
    "\n",
    "It may not always be the case that the data will be readily available for the problem you're trying to solve. Fortunately, in this case, there is already a dataset which is good enough to start with.\n",
    "\n",
    "The dataset you are going to use is already available as a BigQuery public dataset ([link](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=hacker_news&page=dataset)). But the dataset needs to be shaped a bit aligning to with respect to the problem statement. You'll come to this later. \n",
    "\n",
    "This dataset contains all stories and comments from **Hacker News** from its launch in 2006 to present. Each story contains a story ID, the author that made the post, when it was written, and the number of points the story received.\n",
    "\n",
    "To get the data right in my notebook instance, you'll need to configure the GCP Project within the notebook's environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Project ID\n",
    "import os\n",
    "PROJECT = 'your-project-name'\n",
    "os.environ['PROJECT'] = PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `you-project-name` with the name of your GCP project. You are now ready to run a query which would access the BigQuery dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT data_preview\n",
    "SELECT\n",
    "  url, title, score\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  LENGTH(title) > 10\n",
    "  AND score > 10\n",
    "  AND LENGTH(url) > 0\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down a few things here:\n",
    "\n",
    "- `%%bigquery --project $PROJECT data_preview`: `%%bigquery` is a [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html) which lets you run SQL like queries (compatible for BigQuery) from your notebook. `--project $PROJECT` is used to guide BigQuery which GCP Project you're using. `data_preview` is the name of the Pandas DataFrame to which you're going to save results of the query (isn't this very useful?). \n",
    "- `hacker_news` is the name of the BigQuey public dataset and stories is the name of the table residing inside it. \n",
    "- Three columns only: `url` of the article, `title` of the article and `score` of the article. You'll be using the article titles to determine their sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You chose to include only those entries where the length of the article title and article's corresponding URL is greater than 10. The query returned 402 MB of data. \n",
    "\n",
    "Here are the first five rows from the DataFrame data_preview:\n",
    "\n",
    "![](https://i.ibb.co/Zg5bG14/Screenshot-from-2019-07-30-08-15-08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collection part is now done for the project. At this stage, I was good to proceed to the next steps: cleaning and preprocessing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning data wrangling\n",
    "\n",
    "The problem in the current data is in place of `url` I need the source of the URL. For example, `https://github.com/Groundworkstech/Submicron` should appear as `github`. I would also want to rename the `url` column to source. But before doing that, I figured out the distribution in the titles belonging to several sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT source_num_articles\n",
    "SELECT\n",
    "  ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "  COUNT(title) AS num_articles\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "  AND LENGTH(title) > 10\n",
    "GROUP BY\n",
    "  source\n",
    "ORDER BY num_articles DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the `source_num_articles` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>num_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blogspot</td>\n",
       "      <td>41386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>github</td>\n",
       "      <td>36525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>30891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youtube</td>\n",
       "      <td>30848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nytimes</td>\n",
       "      <td>28787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source  num_articles\n",
       "0    blogspot         41386\n",
       "1      github         36525\n",
       "2  techcrunch         30891\n",
       "3     youtube         30848\n",
       "4     nytimes         28787"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_num_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BigQuery provides a number of functions like `ARRAY_REVERSE()`, `REGEXP_EXTRACT()` and so on for useful tasks. With the above query, I first split the URLs with respect to // and / and then I extracted the domains from the URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the project needs different data - a dataset which will contain the articles along with their sources. The stories table contains a lot of article sources other than the ones shown above. So, to keep it a bit more light-weighted, let's go with these five ones: blogpost, github, techcrunch, youtube and nytimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT full_data\n",
    "SELECT source, LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title FROM\n",
    "  (SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "    title\n",
    "  FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "  WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "  )\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR \n",
    "       source = 'techcrunch' or source = 'blogspot' OR\n",
    "       source = 'youtube')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing the `full_data` DataFrame, you get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>github</td>\n",
       "      <td>feminist-software-foundation complains about r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>github</td>\n",
       "      <td>expose sps as web services on the fly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>github</td>\n",
       "      <td>show hn  scrwl   shorthand code reading and wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>github</td>\n",
       "      <td>geoip module on nodejs now is a c   addon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>github</td>\n",
       "      <td>show hn  linuxexplorer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                                              title\n",
       "0  github  feminist-software-foundation complains about r...\n",
       "1  github             expose sps as web services on the fly.\n",
       "2  github  show hn  scrwl   shorthand code reading and wr...\n",
       "3  github          geoip module on nodejs now is a c   addon\n",
       "4  github                             show hn  linuxexplorer"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data understanding is vital for machine learning modeling to work well and to be understood. Let's take some time out and perform some basic EDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding\n",
    "\n",
    "You will start the process of EDA by investigating the dimensions of the dataset. In this case, the dataset prepared in the above step had 168437 rows including 2 columns as can be seen in the preview. \n",
    "\n",
    "Following is the class distribution of the articles:\n",
    "\n",
    "![](https://i.ibb.co/phyx9SQ/Screenshot-from-2019-07-30-08-25-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately enough there is no missing values in the dataset and the following little tweedle can help you knowing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source    0\n",
       "title     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing value inspection\n",
    "full_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common question that arises while dealing with text data like this is - *how is the length of the titles distributed?*\n",
    "\n",
    "Fortunately, Pandas provides a lot of useful functions to answer questions like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    168437.000000\n",
       "mean         46.663174\n",
       "std          17.080766\n",
       "min          11.000000\n",
       "25%          34.000000\n",
       "50%          46.000000\n",
       "75%          59.000000\n",
       "max         138.000000\n",
       "Name: title, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data['title'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a minimum length of 11 and a maximum length of 138. I will come to this again in a moment. \n",
    "\n",
    "EDA is incomplete without plots! In this case, a very useful plot could be **Count vs. Title lengths**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "text_lens = full_data['title'].apply(len).values\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.set()\n",
    "g = sns.distplot(text_lens, kde=False, hist_kws={'rwidth':1})\n",
    "g.set_xlabel('Title length')\n",
    "g.set_ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost bell, isn't it? From the plot, it is evident that the counts are skewed for title lengths < 20 and > 80. So, you may have to be careful in tackling them. Let's perform some manual inspections to figure out:\n",
    "- how many titles fall above the minimum title length (11)?\n",
    "- how many titles have the maximum length (138)?\n",
    "\n",
    "Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(text_lens <= 11).sum(), (text_lens == 138).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be getting 513 and 1 respectively. You will now remove the entry denoting the maximum article length from the dataset since it's just 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data[text_lens < 138].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing you'll be doing in this step was splitting the dataset into train/validation/test sets in a ratio of 80:10:10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134749, 2), (16844, 2), (16843, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80% for train\n",
    "train = full_data.sample(frac=0.8)\n",
    "full_data.drop(train.index, axis=0, inplace=True)\n",
    "\n",
    "# 10% for validation\n",
    "valid = full_data.sample(frac=0.5)\n",
    "full_data.drop(valid.index, axis=0, inplace=True)\n",
    "\n",
    "# 10% for test\n",
    "test = full_data\n",
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data dimensions are: ((110070, 2), (13759, 2), (13759, 2)). Just to be a little more certain on the class distribution, you will now verify that across the three sets:\n",
    "\n",
    "![](https://i.ibb.co/m9z91rJ/Screenshot-from-2019-07-30-08-33-40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions are relatively same across the three sets. Let's serialize these three sets to Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/train.csv', index=False)\n",
    "valid.to_csv('data/valid.csv', index-False)\n",
    "test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still some amount of data preprocessing need - as computers only understand numbers, you'll to prepare the data accordingly to stream to the machine learning model:\n",
    "\n",
    "- Encoding the classes to some numbers (label encoding/one-hot encoding)\n",
    "- Creating a vocabulary from the training corpus - tokenization\n",
    "- Numericalizing the titles and pad them to a fixed-length\n",
    "- Preparing the embedding matrix with respect to pre-trained embeddings like [GloVe](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "Let's proceed accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional data preprocessing\n",
    "\n",
    "First, you'll define the constants that would be necessary here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode\n",
    "CLASSES = {'blogspot': 0, 'github': 1, 'techcrunch': 2, 'nytimes': 3, 'youtube': 4} \n",
    "# Maximum vocabulary size used for tokenization\n",
    "TOP_K = 20000 \n",
    "# Sentences will be truncated/padded to this length\n",
    "MAX_SEQUENCE_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll define a tiny helper function which would take a Pandas DataFrame and would\n",
    "- prepare a list of titles from the DataFrame (needed for further preprocessing)\n",
    "- take the sources from the DataFrame, map them to integers and append to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the davos question. what one thing must be done to make the world a better place in 2008  4\n"
     ]
    }
   ],
   "source": [
    "def return_data(df):\n",
    "    return list(df['title']), np.array(df['source'].map(CLASSES))\n",
    "\n",
    "# Apply it to the three splits\n",
    "train_text, train_labels = return_data(train)\n",
    "valid_text, valid_labels = return_data(valid)\n",
    "test_text, test_labels = return_data(test)\n",
    "\n",
    "print(train_text[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll use the `text` and `sequence` modules provided by `tensorflow.keras.preprocessing` to tokenize and pad the titles. You'll start with tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, MaxPooling1D, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary from training corpus\n",
    "tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "tokenizer.fit_on_texts(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be using the *GloVe* embeddings to represent the words in the titles to a dense representation. The embeddings' file is of more than 650 MB and the GCP team has it stored in a [Google Storage Bucket](https://cloud.google.com/storage/docs/json_api/v1/buckets). This was incredibly helpful since it would allow you to directly use it in the notebook at a very fast speed. You'lle be using the `gsutil` command (available in the Notebooks) to aid this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "!gsutil cp gs://cloud-training-demos/courses/machine_learning/deepdive/09_sequence/text_classification/glove.6B.200d.txt glove.6B.200d.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would need a helper function which would map the words in the titles with respect to the Glove embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(word_index, embedding_path, embedding_dim):\n",
    "    embedding_matrix_all = {}\n",
    "    with open(embedding_path) as f:\n",
    "        for line in f:  # Every line contains word followed by the vector value\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_matrix_all[word] = coefs\n",
    "# Prepare embedding matrix with just the words in our word_index dictionary\n",
    "    num_words = min(len(word_index) + 1, TOP_K)\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= TOP_K:\n",
    "            continue\n",
    "        embedding_vector = embedding_matrix_all.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "  \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all you will need to stream the text data to the yet-to-be-built machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Horcrux: A sequential language model\n",
    "\n",
    "Let's specify a couple of hyperparameter values towards the very beginning of the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the hyperparameters\n",
    "filters=64\n",
    "dropout_rate=0.2\n",
    "embedding_dim=200\n",
    "kernel_size=3\n",
    "pool_size=3\n",
    "word_index=tokenizer.word_index\n",
    "embedding_path = 'glove.6B.200d.txt'\n",
    "embedding_dim=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be using a Convolutional Neural Network based model which would basically start by convolving on the embeddings fed to it. Locality is important in sequential data and CNNs would allow me to capture that effectively. The trick is to do all the fundamental CNN operations (convolution, pooling) in 1D.\n",
    "\n",
    "You'll be following the typical Keras paradigm - you'll first instantiate the model, then will define the topology and then compile the model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "model = models.Sequential()\n",
    "num_features = min(len(word_index) + 1, TOP_K)\n",
    "# Add embedding layer - GloVe embeddings\n",
    "model.add(Embedding(input_dim=num_features,\n",
    "              output_dim=embedding_dim,\n",
    "              input_length=MAX_SEQUENCE_LENGTH,\n",
    "              weights=[get_embedding_matrix(word_index, \n",
    "                              embedding_path, embedding_dim)],\n",
    "              trainable=True))\n",
    "model.add(Dropout(rate=dropout_rate))\n",
    "model.add(Conv1D(filters=filters,\n",
    "              kernel_size=kernel_size,\n",
    "              activation='relu',\n",
    "              bias_initializer='he_normal',\n",
    "              padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(Conv1D(filters=filters * 2,\n",
    "              kernel_size=kernel_size,\n",
    "              activation='relu',\n",
    "              bias_initializer='he_normal',\n",
    "              padding='same'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(rate=dropout_rate))\n",
    "model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "# Compile model with learning parameters.\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture looks like so:\n",
    "\n",
    "![](https://i.ibb.co/rt5Cxgd/cnn-txt-cls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more step that was remaining at this point was Numericalizing the titles and pad them to a fixed-length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the train, validation and test sets\n",
    "# Tokenize and pad sentences\n",
    "preproc_train = tokenizer.texts_to_sequences(train_text)\n",
    "preproc_train = sequence.pad_sequences(preproc_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "preproc_valid = tokenizer.texts_to_sequences(valid_text)\n",
    "preproc_valid = sequence.pad_sequences(preproc_valid, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "preproc_test = tokenizer.texts_to_sequences(test_text)\n",
    "preproc_test = sequence.pad_sequences(preproc_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, you're prepared to kickstart the training process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit(preproc_train,\n",
    "         train_labels,\n",
    "         validation_data=(preproc_valid, valid_labels),\n",
    "         batch_size=128,\n",
    "         epochs=10,\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a snap of the training log:\n",
    "\n",
    "![](https://i.ibb.co/rb2pWHR/Screenshot-from-2019-07-30-09-01-57.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network does overfit and the training graph also confirms it:\n",
    "\n",
    "![](https://i.ibb.co/rdY4c4b/training-history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the model yields an accuracy of ~66% which is not upto the mark given the developments of the hour. But it is a good start. Let's now write a little function to use the network to predict the on individual samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to test on single samples\n",
    "def test_on_single_sample(text):\n",
    "    category = None\n",
    "    text_tokenized = tokenizer.texts_to_sequences(text)\n",
    "    text_tokenized = sequence.pad_sequences(text_tokenized,maxlen=50)\n",
    "    prediction = int(model.predict_classes(text_tokenized))\n",
    "    for key, value in CLASSES.items():\n",
    "    if value==prediction:\n",
    "        category=key\n",
    "  \n",
    "  return category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the samples accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the samples\n",
    "github=['Invaders game in 512 bytes']\n",
    "nytimes = ['Michael Bloomberg Promises $500M to Help End Coal']\n",
    "techcrunch = ['Facebook plans June 18th cryptocurrency debut']\n",
    "blogspot = ['Android Security: A walk-through of SELinux']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, test `test_on_single_sample()` on the above samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in [github, nytimes, techcrunch, blogspot]:\n",
    "    print(test_on_single_sample(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "github\n",
    "techcrunch\n",
    "techcrunch\n",
    "blogspot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was it for this project. In the next section, you'll find my comments on the future directions for this project and then some references used for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future directions and references\n",
    "\n",
    "Just like in the computer vision domain, we expect models that understand the domain to be robust against certain transformations like rotation and translation, in the sequence domain, it's important then models be robust to changes in the length of the pattern. Keeping that in mind, here's a list of what I would try in the near future:\n",
    "- Try other sequence models\n",
    "- A bit of hyperparameter tuning\n",
    "- Learn the embeddings from scratch\n",
    "- Try different embeddings like universal sentence encoder, nnlm-128 and so on\n",
    "\n",
    "After I have a decent model (with at least ~80% accuracy), I plan to serve the model as a REST API and deploy it on AppEngine.\n",
    "\n",
    "Following are the references that were very useful for this project:\n",
    "- [Guide on Text Classification](https://developers.google.com/machine-learning/guides/text-classification/) by Google\n",
    "- [Deep Learning for Time Series Forecasting](https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/) by Jason Brownlee (Machine Learning Mastery)\n",
    "- [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) by François Chollet\n",
    "- [Sequence Models for Time Series and Natural Language Processing](https://www.coursera.org/learn/sequence-models-tensorflow-gcp/home/welcome), a course designed and developed by the Google Cloud team (offered via Coursera)\n",
    "\n",
    "It's an end to the article here. I wrote this article to walk the readers through the approach I generally take for a machine learning problem. Of course, there's more to it but the steps I showed above are the most important ones for me. Thank you for taking the time to read the article and I will see you next time :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
