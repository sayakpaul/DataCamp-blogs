{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Diving deep with Imbalanced Data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following situation - \n",
    "\n",
    "You are working on your dataset. You create a classification model and get 90% accuracy immediately. The results seem fantastic to you. You dive a little deeper and discover that almost entirety of the data belongs to one class. Damn! Imbalanced data can cause you a lot of frustration.\n",
    "\n",
    "You feel very frustrated when you discovered that your data has imbalanced classes and that all of the great results you thought you were getting turn out to be a lie. What is even more frustrating is the good books don't even cover this topic in a holistic manner.\n",
    "\n",
    "This is an example of an **imbalanced** dataset and the frustrating results it can cause.\n",
    "\n",
    "In this tutorial, you will discover the techniques that you can use to deliver good results on datasets with imbalanced data. Specifically, you will cover:\n",
    "\n",
    "- What is imbalanced data?\n",
    "- Challenges faced with imbalanced datasets\n",
    "- Approaches for handling imbalanced data\n",
    "- Further reading on the topic\n",
    "\n",
    "Let's first see what is imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is imbalanced data?\n",
    "\n",
    "Imbalanced data typically refers to a problem with classification tasks where the classes are not represented equally.\n",
    "\n",
    "For example, you may have a binary classification problem with 100 instances. A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.\n",
    "\n",
    "This is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 80:20 or more concisely 4:1.\n",
    "\n",
    "You can have a class imbalance problem on two-class classification problems as well as multi-class classification problems. Most techniques discussed in this tutorial can be used on either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image is also a representative of imbalanced data - \n",
    "\n",
    "<img src = \"https://www.kdnuggets.com/wp-content/uploads/imbalanced-data-1.png\"></img>\n",
    "\n",
    "[Source](\"https://www.kdnuggets.com/wp-content/uploads/imbalanced-data-1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be it a Kaggle competition or real test dataset, class imbalance problem is one of the most common ones. \n",
    "\n",
    "Most of the real-world classification problems display some level of class imbalance, which is when each class does not make up an equal portion of your data-set. It is important to properly adjust your metrics and methods to adjust for your goals. If this is not done, you may end up optimizing for a meaningless metric in the context of your use case.\n",
    "\n",
    "There are problems where a class imbalance is not just common, it is expected. For example, in datasets like those that characterize fraudulent transactions are imbalanced. The vast majority of the transactions will be in the “Not-Fraud” class and a very small minority will be in the “Fraud” class.\n",
    "\n",
    "Another example is customer churn datasets, where the vast majority of customers stay with the service (the “No-Churn” class) and a small minority cancel their subscription (the “Churn” class).\n",
    "\n",
    "When there is a modest class imbalance like 4:1 in the example above it can cause problems.\n",
    "\n",
    "Let's now take a look at the challenges faced with imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges faced with imbalanced datasets:\n",
    "\n",
    "One of the main challenges faced by the utility industry today is _electricity theft_. Electricity theft is the third largest form of theft worldwide. Utility companies are increasingly turning towards advanced analytics and machine learning algorithms to identify consumption patterns that indicate theft.\n",
    "\n",
    "However, one of the biggest stumbling blocks is the humongous data and its distribution. Fraudulent transactions are significantly lower than normal healthy transactions i.e. accounting it to around 1-2 % of the total number of observations. The ask is to improve identification of the rare minority class as opposed to achieving higher overall accuracy.\n",
    "\n",
    "Some examples of imbalanced datasets: \n",
    "- Datasets to identify customer churn where a vast majority of customers will continue using the service. Specifically, Telecommunication companies where Churn Rate is lower than 2 %.\n",
    "- Datasets to identify rare diseases in medical diagnostics etc.\n",
    "- Natural Disaster like Earthquakes\n",
    "\n",
    "Machine learning algorithms tend to produce unsatisfactory classifiers when faced with imbalanced datasets. For any imbalanced data set, if the event to be predicted belongs to the minority class and the event rate is less than 5%, it is usually referred to as a rare event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before studying the approaches, let's first learn about **Accuracy Paradox** which is very relevant for this topic.\n",
    "\n",
    "### Accuracy Paradox:\n",
    "\n",
    "The accuracy paradox is the name for the exact situation in the introduction to this post.\n",
    "\n",
    "It is the case where your accuracy measures tell the story that you have excellent accuracy (such as 90%), but the accuracy is only reflecting the underlying class distribution.\n",
    "\n",
    "It is very common, because classification accuracy is often the first measure you use when evaluating models on your classification problems. \n",
    "\n",
    "**What is going on in our models when you train on an imbalanced dataset?**\n",
    "\n",
    "As you might have guessed, the reason you get 90% accuracy on an imbalanced data (with 90% of the instances in Class-1) is because your models look at the data and cleverly decide that the best thing to do is to always predict “Class-1” and achieve high accuracy.\n",
    "\n",
    "This is best seen when using a simple rule based algorithm. If you print out the rule in the final model you will see that it is very likely predicting one class regardless of the data it is asked to predict.\n",
    "\n",
    "Instead, a properly calibrated method may achieve a lower accuracy, but would have a substantially higher true positive rate (or recall), which is really the metric you should have been optimizing for.\n",
    "\n",
    "Following section will now cover some of the approaches for tackling imbalanced datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches for handling imbalanced data:\n",
    "\n",
    "### Evaluation metric: \n",
    "\n",
    "Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading.\n",
    "\n",
    "There are metrics that have been designed to tell you a more truthful story when working with imbalanced classes. Let's go through them one by one.\n",
    "\n",
    "**Confusion matrix**: Evaluation of a classification algorithm performance is measured by the Confusion Matrix which contains information about the actual and the predicted class.\n",
    "\n",
    "<img src = \"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/03/16142827/ICP1.png\"></img>\n",
    "\n",
    "[Source](\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/03/16142827/ICP1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**: Precision is the number of True Positives divided by the number of True Positives and False Positives. Put another way, it is the number of positive predictions divided by the total number of positive class values predicted. It is also called the Positive Predictive Value (PPV).\n",
    "\n",
    "Precision can be thought of as a measure of a classifier's *exactness*. A low precision can also indicate a large number of False Positives.\n",
    "\n",
    "**Recall**: Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Sensitivity or the True Positive Rate.\n",
    "\n",
    "Recall can be thought of as a measure of a classifier's *completeness*. A low recall indicates many False Negatives.\n",
    "\n",
    "Now, you will go in a bit-more details about these two terms with an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An example illustrating Precision and Recall**:\n",
    "\n",
    "The [breast cancer dataset](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer) is a standard machine learning dataset. It contains 9 attributes describing 286 women that have suffered and survived breast cancer and whether or not breast cancer recurred within 5 years.\n",
    "\n",
    "It is a binary classification problem. Of the 286 women, 201 did not suffer a recurrence of breast cancer, leaving the remaining 85 that did.\n",
    "\n",
    "False Negatives are probably worse than False Positives for this problem. More detailed screening can clear the False Positives, but False Negatives are sent home and lost to follow-up evaluation. \n",
    "\n",
    "A model that only predicted no recurrence of breast cancer would achieve an accuracy of (201/286) * 100 or 70.28%. You call this  <b>All No Recurrence</b>. This is a high accuracy, but a terrible model. If it was used alone for decision support to inform doctors, it would send home 85 women with incorrectly thinking their breast cancer was not going to reoccur (high False Negatives).\n",
    "\n",
    "A model that only predicted the recurrence of breast cancer would achieve an accuracy of (85/286) * 100 or 29.72%. You’ll call this <b>All Recurrence</b>. This model has terrible accuracy and would send home 201 women thinking that had a recurrence of breast cancer but really didn’t (high False Positives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aligning to the concept of **confusion matrix** a perfect classifier would correctly predict 201 no recurrence and 85 recurrence which would be entered into the top left cell no recurrence/no recurrence (True Negatives) and bottom right cell recurrence/recurrence (True Positives).\n",
    "\n",
    "But most of the time that's not the case. Let's see the two confusion matrices of All No Recurrence and All Recurrence:\n",
    "\n",
    "**All No Recurrence**:\n",
    "<img src = \"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/03/no_recurrence_confusion_matrix.png\">\n",
    "**All Recurrence**:\n",
    "<img src = \"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/03/recurrence_confusion_matrix.png\">\n",
    "\n",
    "[Source](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/03/recurrence_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can calculate the Precision and Recall easily now: \n",
    "\n",
    "**Precision**:\n",
    "- The precision of the All No Recurrence model is 0/(0+0) or not a number, or 0.\n",
    "- The precision of the All Recurrence model is 85/(85+201) or 0.30.\n",
    "\n",
    "**Recall**:\n",
    "- The recall of the All No Recurrence model is 0/(0+85) or 0.\n",
    "- The recall of the All Recurrence model is 85/(85+0) or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well! You have now enough reasons as to wonder why considering only classification accuracy to evaluate your classification  model is not a good choice. \n",
    "\n",
    "Let's proceed to the next approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling your dataset:\n",
    "\n",
    "Dealing with imbalanced datasets entails strategies such as improving classification algorithms or balancing classes in the training data (data preprocessing) before providing the data as input to the machine learning algorithm. The later technique is preferred as it has wider application.\n",
    "\n",
    "The main objective of balancing classes is to either increasing the frequency of the minority class or decreasing the frequency of the majority class. This is done in order to obtain approximately the same number of instances for both the classes. \n",
    "\n",
    "This change is called sampling your dataset and there are two main methods that you can use to even-up the classes:\n",
    "\n",
    "- You can add copies of instances from the under-represented class called **over-sampling** (or more formally sampling with replacement), or\n",
    "- You can delete instances from the over-represented class, called **under-sampling**.\n",
    "These approaches are often very easy to implement and fast to run. They are an excellent starting point.\n",
    "\n",
    "In fact, advisable to always try both approaches on all of your imbalanced datasets, just to see if it gives you a boost in your preferred accuracy measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn about over-sampling and under-sampling in a bit more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Under-Sampling: \n",
    "\n",
    "Random under-sampling aims to balance class distribution by randomly eliminating majority class examples.  This is done until the majority and minority class instances are balanced out.\n",
    "\n",
    "Total Observations = 1000\n",
    "\n",
    "Fraudulent   Observations =20\n",
    "\n",
    "Non Fraudulent Observations = 980\n",
    "\n",
    "Event Rate= 2 %\n",
    "\n",
    "In this case, you are taking 10 % samples without replacement from Non Fraud instances.  And combining them with Fraud instances.\n",
    "\n",
    "Non Fraudulent Observations after random under sampling = 10 % of 980 =98\n",
    "\n",
    "Total Observations after combining them with Fraudulent observations = 20+98=118\n",
    "\n",
    "Event Rate for the new dataset after under sampling = 20/118 = 17%\n",
    "\n",
    " \n",
    "\n",
    "**Advantages of this approach**:\n",
    "\n",
    "- It can help improve run time and storage problems by reducing the number of training data samples when the training data set is huge.\n",
    "\n",
    "**Disadvantages**:\n",
    "- It can discard potentially useful information which could be important for building rule classifiers.\n",
    "- The sample chosen by random under sampling may be a biased sample. And it will not be an accurate representative of the population. Thereby, resulting in inaccurate results with the actual test data set.\n",
    " \n",
    "\n",
    "#### Random Over-Sampling:\n",
    "\n",
    "Over-Sampling increases the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample.\n",
    "\n",
    "Total Observations = 1000\n",
    "\n",
    "Fraudulent   Observations =20\n",
    "\n",
    "Non Fraudulent Observations = 980\n",
    "\n",
    "Event Rate= 2 %\n",
    "\n",
    "In this case we are replicating 20 fraud observations   20 times.\n",
    "\n",
    "Non Fraudulent Observations = 980\n",
    "\n",
    "Fraudulent Observations after replicating the minority class observations= 400\n",
    "\n",
    "Total Observations in the new data set after oversampling=1380\n",
    "\n",
    "Event Rate for the new data set after under sampling= 400/1380 = 29 %\n",
    "\n",
    "**Advantages of random over-sampling**:\n",
    "- Unlike under sampling this method leads to no information loss.\n",
    "- Outperforms under sampling\n",
    "\n",
    "**Disadvantages**:\n",
    "\n",
    "- It increases the likelihood of overfitting since it replicates the minority class events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Rules of Thumb**:\n",
    "- Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more)\n",
    "- Consider testing over-sampling when you don’t have a lot of data (tens of thousands of records or less)\n",
    "- Consider testing random and non-random (e.g. stratified) sampling schemes.\n",
    "- Consider testing different resampled ratios (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will the study the next approach for handling imbalanced data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out different perspectives:\n",
    "\n",
    "There are fields of study dedicated to imbalanced datasets. They have their own algorithms, measures and terminology.\n",
    "\n",
    "Taking a look and thinking about your problem from these point of views can sometimes give you good ideas.\n",
    "\n",
    "Two you might like to consider are **anomaly detection** and **change detection**.\n",
    "\n",
    "- Anomaly detection is the detection of rare events. This might be a machine malfunction indicated through its vibrations or a malicious activity by a program indicated by it’s sequence of system calls. The events are rare and when compared to normal operation.\n",
    "\n",
    "This shift in thinking considers the minor class as the outliers class which might help you think of new ways to separate and classify samples.\n",
    "\n",
    "- Change detection is similar to anomaly detection except rather than looking for an anomaly it is looking for a change or difference. This might be a change in behavior of a user as observed by usage patterns or bank transactions.\n",
    "\n",
    "Both of these shifts take a more real-time stance to the classification problem that might give you some new ways of thinking about your problem and maybe some more techniques to try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach can actually help you in playing between different domains and coming up with something new. It's always recommended to dig deeper in this. Anyway, you will now move onto the next and final approach of handling imbalanced data for this post:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Generate Synthetic Samples:\n",
    "A simple way to generate synthetic samples is to randomly sample the attributes from instances in the minority class.\n",
    "\n",
    "You could sample them empirically within your dataset or you could use a method like _Naive Bayes_ that can sample each attribute independently when run in reverse. You will have more and different data, but the non-linear relationships between the attributes may not be preserved.\n",
    "\n",
    "There are systematic algorithms that you can use to generate synthetic samples. The most popular of such algorithms is called SMOTE or the **Synthetic Minority Over-sampling Technique**. It was proposed in 2002 and you can take a look at the [original SMOTE paper](\"http://www.jair.org/papers/paper953.html\"). Following info-graphic will give you a fair idea about the synthetic samples:\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/max/1600/1*uAiwqUNhqaSZmsXCrl9kVQ.png\"></img>\n",
    "\n",
    "[Source](\"https://cdn-images-1.medium.com/max/1600/1*uAiwqUNhqaSZmsXCrl9kVQ.png\")\n",
    "\n",
    "As its name suggests, SMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some advantages and disadvantages of SMOTE - \n",
    "\n",
    "**Advantages** - \n",
    "- Mitigates the problem of overfitting caused by random oversampling as synthetic examples are generated rather than replication of instances. \n",
    "- No loss of useful information.\n",
    "\n",
    "**Disadvantages** - \n",
    "- While generating synthetic examples SMOTE does not take into consideration neighboring examples from other classes. This can result in increase in overlapping of classes and can introduce additional noise.\n",
    "- SMOTE is not very effective for high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up!\n",
    "\n",
    "So far, you got yourself introduced to the concept of imbalanced data and the kind of problem it creates while designing and developing machine learning models. You also saw several reasons as to why it is important to tackle imbalanced data. After that, you studied four different approaches that can help you to handle imbalanced datasets effectively.\n",
    "\n",
    "A lot of important concepts in one go! Absolutely amazing!\n",
    "\n",
    "That is all for this tutorial. In the next tutorial, you will actually implement some of the approaches in Python with a real-world dataset. \n",
    "\n",
    "Below are some paper links if you are very keen to study even more about the topic of imbalanced data:\n",
    "- [Learning from Imbalanced Data](\"http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5128907\")\n",
    "- [Addressing the Curse of Imbalanced Training Sets: One-Sided Selection](\"http://sci2s.ugr.es/keel/pdf/algorithm/congreso/kubat97addressing.pdf\")\n",
    "- [A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data](\"http://dl.acm.org/citation.cfm?id=1007735\")\n",
    "\n",
    "**References**:\n",
    "- [Towards Data Science article on Imbalanced data](\"https://towardsdatascience.com/dealing-with-imbalanced-classes-in-machine-learning-d43d6fa19d2\")\n",
    "- [Python Machine Learning](\"https://g.co/kgs/a35QhF\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
