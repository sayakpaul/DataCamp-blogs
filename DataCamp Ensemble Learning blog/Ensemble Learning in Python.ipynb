{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Ensemble Learning in Python</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You all know that the field of machine learning keeps getting better and better with time. Predictive models form the core of machine learning. Better the accuracy better the model is and so is the solution to a particular problem. In this post, you are going to learn about something called <b><i>Ensemble learning</i></b> which is a very powerful technique to improve the performance of your machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post you will cover:\n",
    "<ul>\n",
    "<li>What is Ensemble learning?</li>\n",
    "<li>How it improves the performance of a machine learning model?</li>\n",
    "<li>Different Ensemble learning methods</li>\n",
    "<li>Pitfalls of Ensembles</li>\n",
    "<li>A Pythonic implementation of different Ensemble learning methods with a real test dataset</li>\n",
    "<li>Further studies on Ensemble learning</li>\n",
    "</ul>\n",
    "\n",
    "So, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>What is Ensemble learning?</h2><br>\n",
    "In the world of Statistics and Machine Learning, Ensemble learning techniques attempt to make the performance of the predictive models better by improving their accuracy. Ensemble Learning is a process using which multiple machine learning models (such as classifiers) are strategically constructed to solve a particular problem. <br>\n",
    "\n",
    "Let's take a real example to build the intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, you want to invest in a company XYZ. You not sure about its performance though. So, you look for advice on whether the stock price will increase more than 6% per annum or not? You decide to approach various experts having diverse domain experience:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b><u>Employee of Company XYZ:</u></b> This person knows the internal functionality of the company and have the insider information about the functionality of the firm. But he lacks a broader perspective on how are competitors innovating, how is the technology evolving and what will be the impact of this evolution on Company XYZ’s product. <b>In the past, he has been right 70% times.</b></li><br>\n",
    "\n",
    "<li><b><u>Financial Advisor of Company XYZ:</u></b> This person has a broader perspective on how companies strategy will fair of in this competitive environment. However, he lacks a view on how the company’s internal policies are fairing off. <b>In the past, he has been right 75% times.</b></li><br>\n",
    "\n",
    "<li><b><u>Stock Market Trader:</u></b> This person has observed the company’s stock price over past 3 years. He knows the seasonality trends and how the overall market is performing. He also has developed a strong intuition on how stocks might vary over time. <b>In the past, he has been right 70% times.</b></li><br>\n",
    "\n",
    "<li><b><u>Employee of a competitor:</u></b> This person knows the internal functionality of the competitor firms and is aware of certain changes which are yet to be brought. He lacks a sight of company in focus and the external factors which can relate the growth of competitor with the company of subject. <b>In the past, he has been right  60% of times.</b></li><br>\n",
    "\n",
    "<li><b><u>Market Research team in same segment:</u></b> This team analyzes the customer preference of company XYZ’s product over others and how is this changing with time. Because he deals with customer side, he is unaware of the changes company XYZ will bring because of alignment to its own goals. <b>In the past, they have been right 75% of times.</b></li><br>\n",
    "\n",
    "<li><b><u>Social Media Expert:</u></b> This person can help us understand how has company XYZ has positioned its products in the market. And how are the sentiment of customers changing over time towards company. He is unaware of any kind of details beyond digital marketing. <b>In the past, he has been right 65% of times.</b></li>\n",
    "\n",
    "Given the broad spectrum of access you have, you can probably combine all the information and make an informed decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a scenario when all the 6 experts/teams verify that it’s a good decision(assuming all the predictions are independent of each other), you will get a combined accuracy rate of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - (30% . 25% . 30% . 40% . 25% . 35%) = 1 - 0.07875 = <b>99.92125%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption used here that all the predictions are completely independent is slightly extreme as they are expected to be correlated. However, you can see how we can be so sure by combining various predictions together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, Ensemble learning is no different.\n",
    "<br><br>Ensemble is the art of combining diverse set of learners (individual models) together to improvise on the stability and predictive power of the model. In the above example, the way we combine all the predictions together will be termed as Ensemble learning.\n",
    "<br><br>Moreover, Ensemble based models can be incorporated in both of the two scenarios i.e. when data is of large volume and when data is too little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now understand how do you actually get different set of machine learning models. Models can be different from each other for a variety of reasons:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>There can be difference in the population of data</li>\n",
    "<li>There can be different modelling technique useed.</li>\n",
    "<li>There can be dfferent hypothesis.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you are playing trivial pursuit. When you play alone, there might be some topics you are good at, and some that you know next to nothing about. If we want to maximize our trivial pursuit score, we need build a team to cover all topics. This is the basic idea of an ensemble: combining predictions from several models averages out idiosyncratic errors and yield better overall predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following picture shows an example schematics of an ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://www.dataquest.io/blog/content/images/2018/01/network-1.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the picture above, An input array <b>X</b> is fed through two preprocessing pipelines and then to a set of base learners <b>f<sup>(i)</sup></b>. The ensemble combines all base learner predictions into a final prediction array P. <a href = \"http://ml-ensemble.com/\">Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the important question is how to combine predictions. In the trivial pursuit example, it is easy to imagine that team members might make their case and majority voting decides which to pick. Machine learning is remarkably similar in classification problems: taking the most common class label prediction is equivalent to a majority voting rule. But there are many other ways to combine predictions, and more generally you can use a model to learn how to best combine predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram presents a basic Ensemble structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://www.dataquest.io/blog/content/images/2018/01/ensemble_network.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Data is fed to a set of models, and a meta learner combine model predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model error and reducing this error with Ensembles:</h2>\n",
    "<br>\n",
    "The error emerging from any machine model can be broken down into three components mathematically. Following are these component:\n",
    "\n",
    "<center><b>Bias + Vairance + Irreducible error</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this important in the current context? To understand what really goes behind an ensemble model, you need to first know what causes error in the model. You will briefly get introduced to these errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Bias error</b> is useful to quantify how much on an average are the predicted values different from the actual value. A high bias error means we have a under-performing model which keeps on missing important trends.\n",
    "\n",
    "<b>Variance</b> on the other side quantifies how are the prediction made on same observation different from each other. A high variance model will over-fit on your training population and perform badly on any observation beyond training. Following diagram will give you more clarity (Assume that red spot is the real value and blue dots are predictions) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://www.analyticsvidhya.com/wp-content/uploads/2015/07/variance_bias.png\"></img><span><a href = \"https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/\">Source</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, as you increase the complexity of your model, you will see a reduction in error due to lower bias in the model. However, this only happens till a particular point. As you continue to make your model more complex, you end up over-fitting your model and hence your model will start suffering from high variance.\n",
    "\n",
    "A good model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors. Ensemble learning is one way to well manage this trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are familiar with the basics of Ensemble learning let's look at different Ensemble learning techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Different types of Ensemble learning methods:</h2>\n",
    "<br>Although there are several types of Ensemble learning methods but the following three are the most-used ones in the industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Bagging based Ensemble learning:</h4>\n",
    "<br>Bagging is one of the Ensemble construction techniques which is also known as <b><i>Bootstrap Aggregation</i></b>. Bootstrap establishes the foundation of Bagging technique. Bootstrap is a sampling technique in which we select “n” observations out of a population of “n” observations. But the selection is completely random i.e. each observation can be selected from the original population so that each observation is equally likely to selected in each iteration of the bootstrapping process. After the bootstrapped samples are formed, separate models are trained with the bootstrapped samples. In real experiments, the bootstrapped samples are drawn from the training set and the sub-models are tested using the testing set. The final output prediction is combined across the predictions of all of the sub-models. \n",
    "\n",
    "The following infographic gives a brief idea of Bagging:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://www.analyticsvidhya.com/wp-content/uploads/2015/07/bagging.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Boosting based Ensemble learning:</h2>\n",
    "<br>Boosting is a form of <i>sequential learning</i> technique. The algorithm works by training a model with the entire training set and subsequent models are constructed by fitting the residual error values of\n",
    "the initial model. In this way, Boosting attempts to give higher weight to those observations that were poorly estimated by the\n",
    "previous model. Once the sequence of the models is created the predictions made by models are weighted by their accuracy scores and the results are combined to create a final estimation. Models that are typically used in Boosting technique are XGBoost, GBM, ADABoost etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Voting based Ensemble learning:</h2>\n",
    "<br>Voting is one of the simplest Ensemble learning techniques in which predictions from multiple models are combined. The method starts with creating two or more separate models with same dataset. Then a Voting based Ensemble model can be used to wrap the previous models and aggregate the predictions of those models. After the Voting based Ensemble model is constructed, it can be used to make prediction on new data. The predictions made by the sub-models can be assigned weights. <i>Stacked aggregation</i> is a technique which can be used to learn how to weigh these predictions in the best possible way.\n",
    "\n",
    "The following infographic best describes Voting-based Ensembles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://www.analyticsvidhya.com/wp-content/uploads/2015/07/stacking-297x300.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the time has come when you apply these concepts to strengthen your intuition and confidence. Let's do it in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A case study in Python</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you are going to be using for this case study is popularly known as <b><i>Wisconsin Breast Cancer dataset</i></b>. The task related to it is Classification. \n",
    "\n",
    "The dataset contains a total number of 10 features labelled in either <i>benign</i> or <i>malignant</i> classes. The features have 699 instances out of which 16 feature values are missing. The dataset only contains numeric values. \n",
    "\n",
    "The dataset can be downloaded from <a href = \"archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29\">here</a>.\n",
    "\n",
    "You will implement the Ensembles using the might scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import all the Python dependencies you will be needing for this case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset in a DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample code number</th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
       "0             1000025                5                        1   \n",
       "1             1002945                5                        4   \n",
       "2             1015425                3                        1   \n",
       "3             1016277                6                        8   \n",
       "4             1017023                4                        1   \n",
       "\n",
       "   Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
       "0                         1                  1                            2   \n",
       "1                         4                  5                            7   \n",
       "2                         1                  1                            2   \n",
       "3                         8                  1                            3   \n",
       "4                         1                  3                            2   \n",
       "\n",
       "  Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
       "0           1                3                1        1      2  \n",
       "1          10                3                2        1      2  \n",
       "2           2                3                1        1      2  \n",
       "3           4                3                7        1      2  \n",
       "4           1                3                1        1      2  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('cancer.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The column \"Sample code number\" is just an indicator and it's of no use in the modelling. So, let's drop it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.drop(['Sample code number'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape  \\\n",
       "0                5                        1                         1   \n",
       "1                5                        4                         4   \n",
       "2                3                        1                         1   \n",
       "3                6                        8                         8   \n",
       "4                4                        1                         1   \n",
       "\n",
       "   Marginal Adhesion  Single Epithelial Cell Size Bare Nuclei  \\\n",
       "0                  1                            2           1   \n",
       "1                  5                            7          10   \n",
       "2                  1                            2           2   \n",
       "3                  1                            3           4   \n",
       "4                  3                            2           1   \n",
       "\n",
       "   Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
       "0                3                1        1      2  \n",
       "1                3                2        1      2  \n",
       "2                3                1        1      2  \n",
       "3                3                7        1      2  \n",
       "4                3                1        1      2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the column is dropped now. Let's get some statistics about the data using Panda's describe() and info() functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.417740</td>\n",
       "      <td>3.134478</td>\n",
       "      <td>3.207439</td>\n",
       "      <td>2.806867</td>\n",
       "      <td>3.216023</td>\n",
       "      <td>3.437768</td>\n",
       "      <td>2.866953</td>\n",
       "      <td>1.589413</td>\n",
       "      <td>2.689557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.815741</td>\n",
       "      <td>3.051459</td>\n",
       "      <td>2.971913</td>\n",
       "      <td>2.855379</td>\n",
       "      <td>2.214300</td>\n",
       "      <td>2.438364</td>\n",
       "      <td>3.053634</td>\n",
       "      <td>1.715078</td>\n",
       "      <td>0.951273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape  \\\n",
       "count       699.000000               699.000000                699.000000   \n",
       "mean          4.417740                 3.134478                  3.207439   \n",
       "std           2.815741                 3.051459                  2.971913   \n",
       "min           1.000000                 1.000000                  1.000000   \n",
       "25%           2.000000                 1.000000                  1.000000   \n",
       "50%           4.000000                 1.000000                  1.000000   \n",
       "75%           6.000000                 5.000000                  5.000000   \n",
       "max          10.000000                10.000000                 10.000000   \n",
       "\n",
       "       Marginal Adhesion  Single Epithelial Cell Size  Bland Chromatin  \\\n",
       "count         699.000000                   699.000000       699.000000   \n",
       "mean            2.806867                     3.216023         3.437768   \n",
       "std             2.855379                     2.214300         2.438364   \n",
       "min             1.000000                     1.000000         1.000000   \n",
       "25%             1.000000                     2.000000         2.000000   \n",
       "50%             1.000000                     2.000000         3.000000   \n",
       "75%             4.000000                     4.000000         5.000000   \n",
       "max            10.000000                    10.000000        10.000000   \n",
       "\n",
       "       Normal Nucleoli     Mitoses       Class  \n",
       "count       699.000000  699.000000  699.000000  \n",
       "mean          2.866953    1.589413    2.689557  \n",
       "std           3.053634    1.715078    0.951273  \n",
       "min           1.000000    1.000000    2.000000  \n",
       "25%           1.000000    1.000000    2.000000  \n",
       "50%           1.000000    1.000000    2.000000  \n",
       "75%           4.000000    1.000000    4.000000  \n",
       "max          10.000000   10.000000    4.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 699 entries, 0 to 698\n",
      "Data columns (total 10 columns):\n",
      "Clump Thickness                699 non-null int64\n",
      "Uniformity of Cell Size        699 non-null int64\n",
      "Uniformity of Cell Shape       699 non-null int64\n",
      "Marginal Adhesion              699 non-null int64\n",
      "Single Epithelial Cell Size    699 non-null int64\n",
      "Bare Nuclei                    699 non-null object\n",
      "Bland Chromatin                699 non-null int64\n",
      "Normal Nucleoli                699 non-null int64\n",
      "Mitoses                        699 non-null int64\n",
      "Class                          699 non-null int64\n",
      "dtypes: int64(9), object(1)\n",
      "memory usage: 54.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the dataset contains missing values. The column named \"Bare Nuclei\" contains them. Let's verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1      10\n",
       "2       2\n",
       "3       4\n",
       "4       1\n",
       "5      10\n",
       "6      10\n",
       "7       1\n",
       "8       1\n",
       "9       1\n",
       "10      1\n",
       "11      1\n",
       "12      3\n",
       "13      3\n",
       "14      9\n",
       "15      1\n",
       "16      1\n",
       "17      1\n",
       "18     10\n",
       "19      1\n",
       "20     10\n",
       "21      7\n",
       "22      1\n",
       "23      ?\n",
       "24      1\n",
       "25      7\n",
       "26      1\n",
       "27      1\n",
       "28      1\n",
       "29      1\n",
       "       ..\n",
       "669     5\n",
       "670     8\n",
       "671     1\n",
       "672     1\n",
       "673     1\n",
       "674     1\n",
       "675     1\n",
       "676     1\n",
       "677     1\n",
       "678     1\n",
       "679     1\n",
       "680    10\n",
       "681    10\n",
       "682     1\n",
       "683     1\n",
       "684     1\n",
       "685     1\n",
       "686     1\n",
       "687     1\n",
       "688     1\n",
       "689     1\n",
       "690     1\n",
       "691     5\n",
       "692     1\n",
       "693     1\n",
       "694     2\n",
       "695     1\n",
       "696     3\n",
       "697     4\n",
       "698     5\n",
       "Name: Bare Nuclei, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Bare Nuclei']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can spot some \"?\"s in it right? Well, these are your missing values and you will be imputing them with <i>Mean Imputation</i>. But first, you will replace those \"?\"s with 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.replace('?',0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1      10\n",
       "2       2\n",
       "3       4\n",
       "4       1\n",
       "5      10\n",
       "6      10\n",
       "7       1\n",
       "8       1\n",
       "9       1\n",
       "10      1\n",
       "11      1\n",
       "12      3\n",
       "13      3\n",
       "14      9\n",
       "15      1\n",
       "16      1\n",
       "17      1\n",
       "18     10\n",
       "19      1\n",
       "20     10\n",
       "21      7\n",
       "22      1\n",
       "23      0\n",
       "24      1\n",
       "25      7\n",
       "26      1\n",
       "27      1\n",
       "28      1\n",
       "29      1\n",
       "       ..\n",
       "669     5\n",
       "670     8\n",
       "671     1\n",
       "672     1\n",
       "673     1\n",
       "674     1\n",
       "675     1\n",
       "676     1\n",
       "677     1\n",
       "678     1\n",
       "679     1\n",
       "680    10\n",
       "681    10\n",
       "682     1\n",
       "683     1\n",
       "684     1\n",
       "685     1\n",
       "686     1\n",
       "687     1\n",
       "688     1\n",
       "689     1\n",
       "690     1\n",
       "691     5\n",
       "692     1\n",
       "693     1\n",
       "694     2\n",
       "695     1\n",
       "696     3\n",
       "697     4\n",
       "698     5\n",
       "Name: Bare Nuclei, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Bare Nuclei']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"?\"s are replaced with 0's now. Let's do the missing value treatment now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the DataFrame object into NumPy array otherwise you will not be able to impute\n",
    "values = data.values\n",
    "\n",
    "# Now impute it\n",
    "imputer = Imputer()\n",
    "imputedData = imputer.fit_transform(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you take a look at the dataset itself you will see that all the ranges of the features of the dataset is not the same. This may cause a problem. A small in a feature might not affect the other. To address this problem, you will normalize the ranges of the features to a uniform range, in this case 0 - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "normalizedData = scaler.fit_transform(imputedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! \n",
    "\n",
    "You have performed all the preprocessing that was required in order to perform your Ensembling experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will start with Bagging based Ensembling. In this case you will use a Bagged Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bagged Decision Trees for Classification - necessary dependencies\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have imported the depdendencies for the Bagged Decision Trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Segregate the features from the labels\n",
    "X = normalizedData[:,0:9]\n",
    "Y = normalizedData[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9571428571428573\n"
     ]
    }
   ],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=7)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what you did in the above cell.\n",
    "\n",
    "First, you initialized a 10-fold cross-validation fold. After that, you instantiated a Decision Tree Classifier with 100 trees and wrapped it in a Bagging-based Ensemble. Then you evaluated your model.\n",
    "\n",
    "You model performed pretty well. It yielded an accuracy of <b>95.71%</b>.\n",
    "\n",
    "Brilliant! Let's implement the other ones. \n",
    "\n",
    "<i>(If you want a quick refresher on cross-validation then this is the <a href=\"https://www.youtube.com/watch?v=CRqLeHpACVI\">link</a> to go for.)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9557142857142857\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "seed = 7\n",
    "num_trees = 70\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you did a AdaBoost classification (with 70 trees) which is based on Boosting type of Ensembling. The model gave you an accuracy of <b>95.57%</b> for a 10-fold cross validation.\n",
    "\n",
    "Finally it's time for you to implement the Voting-based Ensemble technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Voting Ensemble for Classification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9642857142857142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You implemented a Voting based Ensemble model where you took Logistic Regression, Decision Tree and Support Vector Machine for voting purpose. The model performed the best so far with an accuracy of <b>96.42%</b> for a 10-fold cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get you familiarized with some common pitfalls of Ensemble learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pitfalls of Ensemble learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general it is not true that it will always perform better. There are several ensemble methods, each with its own advantages/weaknesses. Which one to use and then depends on the problem at hand.\n",
    "\n",
    "For example, if you have models with high variance (they over-fit your data), then you are likely to benefit from using bagging. If you have biased models, it is better to combine use them with Boosting. There are also different strategies to form ensembles. The topic is just too wide to cover it in one answer.\n",
    "\n",
    "But the point is: if you use the wrong ensemble method for your setting, you are not going to do better. For example, using Bagging with a biased model is not going to help.\n",
    "\n",
    "Also, if you need to work in a probabilistic setting, ensemble methods may not work either. It is known that Boosting (in its most popular forms like AdaBoost) delivers poor probability estimates. That is, if you would like to have a model that allows you to reason about your data, not only classification, you might be better off with a graphical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this post you got introduced to Ensemble learning technique. You covered its basics, how it improves your model's performance. You covered its three main types.\n",
    "\n",
    "Also, you implemented these three types in Python with the help of scikit-learn and in this course of action you gained a bit knowledge about necessary preprocessing steps.\n",
    "\n",
    "That's quite a feat! Well done! In this final section, I suggest some further undertakings on Ensembles which you might want to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Take it further:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Try other Boosting based Ensemble techniques viz. Gradient Boosting, XGBoost.</li>\n",
    "<li>Play with the different parameter settings that scikit-learn offers in Ensembles and then try to find why a particular setting performed well. This will make your understanding even stronger. <a href = \"scikit-learn.org/stable/modules/ensemble.html\">link</a></li>\n",
    "<li>Try Ensemble learning on a variety of datasets to understand where you should and where you should not apply Ensemble learning. For finding datasets Kaggle, UCI Repository etc. are good places to search.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you enjoyed this tutorial. Let me know your doubts (if you have any) in the comments section."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
