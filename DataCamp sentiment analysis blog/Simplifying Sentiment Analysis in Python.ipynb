{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplifying Sentiment Analysis in Python\n",
    "\n",
    "**Learn the basics of sentiments analysis and how to build a simple sentiment classifier in Python.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The promise of machine learning has shown many absolutely stunning results in a lot of fields. Natural language processing is no exception of it and it is one of those fields where machine learning has been able to show _general artificial intelligence_ (not fully but at least partially) achieving some brilliant results for genuinely complicated tasks. \n",
    "\n",
    "Now, NLP (natural language processing) is not a new field and so is machine learning. But the fusion of both the fields is quite contemporary and only vows to make progress. This is one of those hybrid applications which everyone (with a budget smart-phone) comes across daily. For example, take \"keyboard  word suggestion\" into the account, or intelligent auto-completions; these all are the byproducts of the amalgamation of NLP and Machine Learning and quite naturally these have become the inseparable parts of our lives.\n",
    "\n",
    "**Sentiment analysis** is one such topic from this hybrid of NLP and Machine Learning. It has easily become one of the most hot topics in the field because of its relevance and the amount of business problems it is solving and has been able to solve. In this tutorial, you will cover this _not-so-simple_ topic in a _simple way_. You will break down all the little mathematics behind it and you will study it. You will also build a simple sentiments' classifier at the end of this tutorial. Specifically you will cover:\n",
    " - Understanding sentiment analysis from a practitioner's perspective\n",
    " - Formulating the problem statement of sentiment analysis\n",
    " - Naive Bayes classification for sentiment analysis\n",
    " - A case study in Python\n",
    " - How sentiment analysis is affecting several business grounds\n",
    " - Further reading on the topic\n",
    " \n",
    "Let's get started.\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/max/361/0*ga5rNPmVYBsCm-lz.\"></img>\n",
    "**Source: [Medium](https://medium.com/@tomyuz/a-sentiment-analysis-approach-to-predicting-stock-returns-d5ca8b75a42)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is sentiment analysis - A practitioner's perspective:\n",
    "\n",
    "Essentially, sentiment analysis or sentiment classification fall into the wide category of text classification tasks where you are supplied a phrase or a list phrases and your classifier is supposed to tell if the sentiment behind that is positive, negative or neutral. Sometimes, the third attribute is not taken to keep it a binary classification problem. In recent tasks, sentiments like \"somewhat positive\" and \"somewhat negative\" are also being considered. Let's understand with an example now.\n",
    "\n",
    "Consider the following phrases:\n",
    "1. \"Titanic is a great movie.\"\n",
    "2. \"Titanic is not a great movie.\"\n",
    "3. \"Titanic is a movie.\"\n",
    "\n",
    "The phrases correspond to short movie reviews and each one of them conveys different sentiments. For example, the first phrase denote positive sentiment about the movie Titanic while the second one treats the movie as not so great (negative sentiment). Take a look at the third one more closely. There is no such word in that phrase which can tell you about anything regarding the sentiment conveyed by it. Hence, that is an example of neutral sentiment. \n",
    "\n",
    "Now, from a strictly machine learning point of view, this task is nothing but a _supervised learning_ task. You will supply a bunch of phrases (with the labels of their respective sentiments) to the machine learning model and you will test the model on unlabeled phrases.\n",
    "\n",
    "For mere introducing sentiment analysis this should be good but for being able to build a sentiment classification model, you need something more. Let's proceed. \n",
    "\n",
    "<img src = \"https://image.ibb.co/eefou9/Capture.jpg\"></img>\n",
    "**Source: [SlideShare](https://www.slideshare.net/jchoi7s/cs571-sentiment-analysis)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating the problem statement of sentiment analysis:\n",
    "\n",
    "Before understanding the problem statement of a sentiment classification task you need to have a clear idea of general text classification problem. Let's formally define the problem of a general text classification task.\n",
    "\n",
    "- **Input**:\n",
    "    - A document d\n",
    "    - A fixed set of classes C = {c<sub>1</sub>,c<sub>2</sub>,..,c<sub>n</sub>}\n",
    "<br><br>\n",
    "- **Output**: A predicted class c $\\in$ C\n",
    "\n",
    "The _document_ term here is subjective because in the text classification world. By document it is meant tweets, phrases, parts of news article, the whole news article, a whole article, a product manual, a story etc. The reason behind this terminology is _word_ in an atomic entity and small in this context. So, to denote large sequences of words, this term _document_ is used in general. Tweets mean shorter document whereas an article means a larger document. \n",
    "\n",
    "So, a training set of n labeled documents looks like: _(d<sub>1</sub>,c<sub>1</sub>), (d<sub>2</sub>,c<sub>2</sub>),...,(d<sub>n</sub>,c<sub>n</sub>)_ and the ultimate output is a learned classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are doing good! But one question that you must be having at this point is where are the features of the documents? Genuine question! You will get to that a bit later. \n",
    "\n",
    "Now, let's just move on with the problem formulation and slowly build the intuition behind sentiment classification. \n",
    "\n",
    "One important point you need to keep in mind while working in sentiment analysis is not all the words in a phrase convey the sentiment of the phrase. Words like \"I\", \"Are\", \"Am\" etc. do not contribute in conveying any kind of sentiments and hence, they are not relative in a sentiment classification context. Consider the problem of feature selection here. In feature selection you try to figure out the most relevant feature that relate the most to the class-label. That same idea applies here as well. Therefore, only a handful of words in a phrase take part in this and identifying them and extracting them from the phrases challenging tasks. But don't worry, you will get to that. \n",
    "\n",
    "Consider the following movie review to better understand this: \n",
    "\n",
    "\"_I love this movie! It's sweet, but with satirical humor. The dialogs are great and the adventure scenes are fun. It manages to be romantic and whimsical while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I have seen it several times and I'm always happy to see it again......._\"\n",
    "\n",
    "Yes, this is certainly a review which carries positive sentiments regarding a certain movie. But what are those specific words which define this positivity?\n",
    "\n",
    "Take a look at the review again. \n",
    "\n",
    "\"_I **love** this movie! It's **sweet**, but with **satirical** humor. The dialogs are **great** and the adventure scenes are **fun**. It manages to be **romantic** and **whimsical** while laughing at the conventions of the fairy tale genre. I would **recommend** it to just about anyone. I have seen it several times and I'm always **happy** to see it **again**......._\"\n",
    "\n",
    "You must have got the clear picture now. The bold words in the above piece of text are the most important words which construct the positive nature of the sentiment conveyed by the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do with these word? The next step which seems natural is to create a representation similar to the following: \n",
    "\n",
    "<img src = \"https://image.ibb.co/gOgH49/Capture.jpg\"></img>\n",
    "\n",
    "So what is the above representation doing? You have guessed it now. Each row is containing a word and its frequency of occurrence in the document (let's call it a document from now on). You are also wondering love has appeared for only once but why the frequency is 2? Well, this is a part of the whole review. Consider, the representation is for the whole review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While formulating the problem statement of a sentiment classification task, you understood \"**Bag of words**\" representation  and the above representation is nothing but a `Bag-of-words` representation. This is probably the most fundamental concepts in NLP and is the first step of doing any text classification problem. So, make sure you understand it well. \n",
    "\n",
    "**A `bag-of-words` representation of a document does not only contain specific words but all the `unique` words in a document and their frequencies of occurrences.** A bag is a mathematical `set` here, so by the definition of set, bag does not contain any duplicate words. \n",
    "\n",
    "But for this application, you are only interested in the bold words as mentioned earlier, so the bag-of-words for this document will only contain these words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents are not written in a jumbled way. Are they? The sequence of words in a document is extremely important. But in the context of sentiment classification, this sequence is not very important. What is more important or the most important part here is the presence of these words. \n",
    "\n",
    "The words that you found out in the bag-of-words will now construct the feature set of your document. So, consider you a set of many movie reviews (documents) and you have created bag-of-words representations for each one of them and preserved their labels (i.e. sentiments - +ve or -ve in this case). Your training set should look like:\n",
    "\n",
    "<img src = \"https://image.ibb.co/gOmaP9/Capture.jpg\"></img>\n",
    "\n",
    "**This representation is also known an Corpus.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training set should be easy to interpret -\n",
    "\n",
    "All the rows are independent feature vectors containing information about a specific document (movie reviews), the specific words and its sentiment. Note that, the label _sentiment_ is often denoted as (+, -) or (+ve, -ve). Also, the features w1, w2, w3, 34, ..., wn are generated from bag of words and it is not necessary that all the documents will contain each of these features/words. \n",
    "\n",
    "You will pass these feature vectors to the classifier. So, let's study it next - Naive Bayes classification model for sentiment classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classification for sentiment analysis:\n",
    "\n",
    "Naive Bayes classification is nothing but applying Bayes rules for forming classification probabilities. In this section you study Naive Bayes classifier from the context of sentiment classification. It's highly recommended to get some introduction about Naive Bayes classification and the Bayes rule. Resources for that are as follows: \n",
    "\n",
    "- [Beginning Bayes in R (practice)](https://www.datacamp.com/community/open-courses/beginning-bayes-in-r)\n",
    "- [6 Easy Steps to Learn Naive Bayes Algorithm ](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)\n",
    "\n",
    "But why Naive Bayes in the world k-NN, Decision Trees and so many others? You will get to that later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first build the notion of general terms in Naive Bayes classifier the context of sentiment classification. You will start off by taking a look at the Bayes rule:\n",
    "\n",
    "- For a document **d** and class **c**:\n",
    "\n",
    "<img src = \"https://image.ibb.co/dKms49/Capture.jpg\"></img>\n",
    "\n",
    "**In this case, the class comprises two sentiments. Positive and negative.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's study each term of the above image in details in this context. \n",
    "- The RHS term P(c|d) is read as _probability of class c given a document d_. This term is also known as *Posterior*.\n",
    "- P(d|c) should be similar. \n",
    "\n",
    "Now, what are these Prior and Likelihood? Also, the term P(d) (probability of a document); does it sound absurd? Gems of questions! Let's find the answers now! \n",
    "\n",
    "- The term which is shown as *Prior* is your original belief i.e. original label of document being positive or negative (in terms of sentiments).\n",
    "- The term *Likelihood* is probability of a document d given a class c.\n",
    "- Now think of the term Posterior as your updated rule or updated belief obtained by multiplying Prior and Likelihood. \n",
    "- But what is *Normalization Constant* P(d)? This term is divided with the result produced by the multiplication just to ensure the ultimate result can be presented in a probability distribution.\n",
    "\n",
    "Not the finest of details till now! But just stick to it. You will discover more details. But remember, you are still building your intuition as to relate Bayes rule in the context of sentiment classification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get into more details as to find out what exactly Bayes rule is trying to do here. The following image presents more detailed steps of Bayes rule:\n",
    "\n",
    "<img src = \"https://image.ibb.co/fsjJu9/Untitled.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of unknown terms here. Let's take it slow.\n",
    "\n",
    "Let's start with the RHS term c<sub>MAP</sub>. It denotes the main objective of the Bayes rule here i.e. to find out the maximum posterior probability/estimate of a certain document belonging to a certain class. **MAP** is abbreviation of ``Max A Posteriori`` which is a Greek terminology. \n",
    "\n",
    "What is `argmax`?  You could have used just `max`!\n",
    "- Well, `argmax` denotes the index. Suppose the P(+|d) > P(-|d) where + and - denote positive and negative sentiments respectively.  These terms P(+|d), P(-|d) return probabilities which is a numeric quantity. But, you are not interested in the probability, you are interested in finding out the class for which P(+|d) is greater and `argmax` returns that. For P(+|d) > P(-|d), `argmax` will return **+**. \n",
    "\n",
    "And yes, you can drop the denominator term P(d). It's entirely depends on the implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do I find out $P(d|c)$ and $P(c)$? This is exactly where `bag of words` will come in handy. But how? \n",
    "\n",
    "Read on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know how to convert a given document to a bag of words representation. More importantly, you can represent a document as a set of features with this. So now, essentially the term c<sub>MAP</sub> can be written as (ignoring the denominator term P(d)):\n",
    "\n",
    "<img src = \"https://image.ibb.co/fkpOSU/Capture.jpg\"></img>\n",
    "\n",
    "But, how do you really calculate the probabilities? Let's start with $P(c)$ first. \n",
    "\n",
    "P(c) is basically concerned with this question : \"How often does this class occur?\" Let's say your document dataset contains 60% positive sentiments and 40% negative sentiments. So, $P(+) = 0.6$ and $P(-) = 0.4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how do you interpret this term: P(x1, x2,...,xn | c)?\n",
    "\n",
    "Think it this way - what is the probability of the occurrences of these words (features) given the class c. For example, say you have 1000 documents and you have only two words in the corpus - \"good\" and \"awesome\". Now, out these 1000 documents 500 documents are labeled as positive and the remaining 500 are labeled as negative. Also, you found out that out of 500 positively labeled documents, 200 documents contain \"good\" and \"awesome\" both (note P(x1,x2) means P(x1 and x2)). So, the probability P(good,awesome | +) = 200 / 1000 = 1/5.\n",
    "\n",
    "One important point you would like to make here is if your size of vocabulary is $X$ then you can formulate **X<sup>n</sup>** likelihood (like P(good,awesome | +)) probabilities provided your document contains n words. \n",
    "\n",
    "Remember you have to compute the likelihood probabilities for both the classes here. So, the total number of combinations a scenario where you have 2000 total words and each document contains 20 words on an average will be (2000)<sup>20</sup>. This number is insanely large! And what if the corpus size is in millions (which really happens in practical cases)? \n",
    "\n",
    "This is called the **Bayes Classifier**. But it just does not work as the computations are way too many. Now, you will study some assumptions for making Bayes classifier a **Naive Bayes** classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumptions that you are going to study are called **Naive Bayes Independence Assumptions**. They are as follows:\n",
    "<center>\n",
    "   P(x1, x2,...,xn | c)\n",
    "</center>\n",
    "- **Bag of words assumption**: Assume position does not matter. Assume that a particular word occurs at 10th and 20th position, but with this assumption it is meant that you only care about frequency for that word has occurred which is 2. 10 and 12 these two numbers are irrelevant here. \n",
    "<br><br>\n",
    "- **Conditional independence assumption**: This is an extremely important assumption which make Bayes classifier Naive Bayes. It states that \"assume the feature probabilities P(x<sub>i</sub>|c<sub>j</sub>)\". \n",
    "Take a closer look at the statement. It means that P(x<sub>1</sub>|c<sub>j</sub>), P(x<sub>2</sub>|c<sub>j</sub>) and so on are independent of each other. (It does not anyway mean that P(x<sub>1</sub>), P(x<sub>2</sub>) and so on are independent of each other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the term P(x1, x2,...,xn | c) can be expressed as following:\n",
    "\n",
    "<img src = \"https://image.ibb.co/fGwHOe/Capture.jpg\"></img>\n",
    "\n",
    "So, naturally **X<sup>n</sup>** combinations get reduced to **Xn** which is exponentially less (if your size of vocabulary is $X$ and your document contains n words). Defined mathematically, Bayes classifier when reduced to Naive Bayes classifier looks like:\n",
    "\n",
    "<img src = \"https://image.ibb.co/k80Qbz/Capture.jpg\"></img>\n",
    "\n",
    "Naive Bayes has two advantages:\n",
    "- Reduced number of parameters.\n",
    "- Linear time complexity as opposed to exponential time complexity. \n",
    "\n",
    "Naive Bayes classification mechanism when applied to a text classification problem, it is referred to as \"**Multinomial Naive Bayes**\" classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are quite apt in understanding the mechanics of a Naive Bayes classifier especially for a sentiment classification problem. Now, it's high-time that you implement a sentiment classifier. \n",
    "\n",
    "You will do that it Python! Let's get started with the case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple sentiment classifier in Python:\n",
    "\n",
    "For this case case study, you'll use an off-line movie review corpus as covered in the [NLTK book](http://www.nltk.org/book/ch06.html#document-classification) and can be downloaded from [here](http://www.nltk.org/nltk_data/). `nltk` provides a version of the dataset. The dataset categorizes each review as positive or negative. You need to download that first as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not recommended to run it from Jupyter Notebook. try to run it from the command prompt (if using Windows). It will take sometime. So, be patient. \n",
    "\n",
    "For more information about NLTK datasets, make sure you visit [this link](https://www.nltk.org/data.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be implementing Naive Bayes or let's say Multinomial Naive Bayes classifier using `NLTK` which stands for Natural Language Toolkit. It is a library dedicated to NLP and NLU related tasks and the documentation is very good. It covers many techniques in a great and provides free datasets as well for experiments. \n",
    "\n",
    "[This](http://www.nltk.org) is NLTK's official website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the data is downloaded, you will start off by importing the movie reviews dataset by `from nltk.corpus import movie_reviews`. Then, you will construct a list of documents, labeled with the appropriate categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will define a feature extractor for documents, so the classifier will know which aspects of the data it should pay attention to. In this case, you can define a feature for each word, indicating whether the document contains that word. To limit the number of features that the classifier needs to process, you start by constructing a list of the 2000 most frequent words in the overall corpus. You can then define a feature extractor that simply checks if each of these words is present in a given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the feature extractor\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that you computed the set of all words in a document `document_words = set(document)`, rather than just checking if word in document, is that checking whether a word occurs in a set is much faster than checking whether it occurs in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have defined the feature extractor. Now, you can use it to train a Naive Bayes classifier to predict the sentiments of new movie reviews. To check your classifier's performance, you will compute its accuracy on the test set. NLTK provides `show_most_informative_features()` to see which features the classifier found to be most informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Naive Bayes classifier\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71\n"
     ]
    }
   ],
   "source": [
    "# Test the classifier\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The classifier was able to achieve an accuracy of 71% without even tweaking any parameters or fine tuning. This is great for the first go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "       contains(winslet) = True              pos : neg    =      8.4 : 1.0\n",
      "     contains(illogical) = True              neg : pos    =      7.6 : 1.0\n",
      "      contains(captures) = True              pos : neg    =      7.0 : 1.0\n",
      "        contains(turkey) = True              neg : pos    =      6.5 : 1.0\n",
      "        contains(doubts) = True              pos : neg    =      5.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Show the most important features as interpreted by Naive Bayes\n",
    "classifier.show_most_informative_features(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, a review that mentions \"Illogical\" is almost 8 times more likely to be negative than positive, while a review that mentions \"Captures\" is about 6 times more likely to be positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now the question - why Naive Bayes?**\n",
    "\n",
    "- You chose to study Naive Bayes because of the way it is designed and developed. Text data has some sensible and sophisticated features which are best mapped to Naive Bayes provided you are not considering Neural Nets. Besides, it's easy to interpret and does not create the notion of a blackbox model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes suffers from a genuine disadvantage as well:\n",
    "\n",
    "The main limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that you get a set of predictors which are completely independent.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why sentiment analysis is so important?\n",
    "\n",
    "Sentiment analysis solves a number of  genuine business problems:\n",
    "- It helps to predict customer behavior for a particular product.\n",
    "- It can help to test the adaptability of a product.\n",
    "- Automates the task of customer preference reports.\n",
    "- it can easily the automate the process of determining how well did a movie run by analyzing the sentiments behind the movie's reviews from a number of platforms. \n",
    "- And many more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps:\n",
    "\n",
    "Congratulations! You have made it till the end. NLP is a very vast and interesting topic solving  some really challenging problems. Specifically the intersection of NLP and Deep Learning has given birth to some amazing products. It has completely revolutionized the way chatbots interact. The list is never ending.  \n",
    "\n",
    "This tutorial hopefully gave you a head-start in one of the prime sub-fields of NLP i.e. Sentiment Analysis. You covered one of the most fundamental topic of NLP - Bag of Words and then studied Naive Bayes Classifier in a great detailed manner. You studied its shortcomings as well. You got yourself familiarized with `nltk`, one of the most popular libraries for NLP and NLU tasks. You implemented a simple Naive Bayes classifier with the movie corpus that `nltk` provides. Give yourself a round of applause. You deserve it!\n",
    "\n",
    "Next are some links to some amazing resources if you want to take things further from this humble beginning:\n",
    "- [Study more about NLP and its different tool/techniques](nltk.org)\n",
    "- [Study about text data in general](https://www.manning.com/books/deep-learning-with-python)\n",
    "- [Study Recurrent Neural Nets](https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/)\n",
    "- [See some amazing applications of NLP with Deep Learning](https://machinelearningmastery.com/applications-of-deep-learning-for-natural-language-processing/)\n",
    "\n",
    "Following references were used in order to create this tutorial:\n",
    "- [Text Classification and Naïve Bayes](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)\n",
    "- [Predict Sentiment From Movie Reviews Using Deep Learning](https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/)\n",
    "- [NLTK Book](nltk.org)\n",
    "- [NLP Basics](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/)\n",
    "\n",
    "If you are interested in learning the basics of NLP and applying it to real world datasets, you can take DataCamp's [\"Natural Language Processing Fundamentals in Python\"](https://www.datacamp.com/courses/natural-language-processing-fundamentals-in-python) course. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
