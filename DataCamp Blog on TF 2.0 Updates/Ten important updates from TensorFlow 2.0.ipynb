{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ten Important updates from TensorFlow 2.0 \n",
    "\n",
    "In this article, you will be going through the ten most important updates introduced in the newly released `TensorFlow 2.0` and you will get to implement some of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is been quite a few days since `TensorFlow 2.0 alpha` released. The framework does have a huge impact on the deep learning community. Practitioners, researchers, developers have loved the framework and have adapted it like never before. It is easily one of the main reasons behind the jump-start of all the super cool deep learning enabled applications that we get to see today. But `TensorFlow 1.x` [has its cons](https://www.tensorflow.org/alpha/guide/effective_tf2#a_brief_summary_of_major_changes) too (like many other frameworks). As [Martin Wicke](https://twitter.com/martin_wicke) (Software Engineer from the `TensorFlow` team) said during **TF Dev Summit '19** - \n",
    "\n",
    "> We've learned a lot since 1.0. \n",
    "\n",
    "With all the lessons learned from the wide user-base, GitHub issues, the `TensorFlow` team released the `TensorFlow 2.0 alpha` which comes with a significant number of important changes for the betterment of performance, user experience and so on. It enables you with rapid prototyping and includes many modern deep learning practices. In this article, you will get to study some of these changes through precise implementations. \n",
    "\n",
    "**Note** that the updates discussed here are the most significant ones according to the author. You will be needing some previous `TensorFlow` and `Keras` experience in order to follow along with this article. Following are some resources which you may find handy in case you are looking for refreshers on `TensorFlow` and `Keras` - \n",
    "\n",
    "* [TensorFlow Tutorial For Beginners](https://www.datacamp.com/community/tutorials/tensorflow-tutorial)\n",
    "* [Keras Tutorial: Deep Learning in Python](https://www.datacamp.com/community/tutorials/deep-learning-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and a demo dataset\n",
    "\n",
    "Updating to `TensorFlow 2.0` is running the following line of code from a Jupyter Notebook:\n",
    "\n",
    "`!pip install tensorflow==2.0.0-alpha0`\n",
    "\n",
    "The GPU variant can also be installed in the same way (requires CUDA before):\n",
    "\n",
    "`!pip install tensorflow-gpu==2.0.0-alpha0`\n",
    "\n",
    "You can find more about the installation process [here](https://github.com/tensorflow/community/blob/master/sigs/testing/faq.md). \n",
    "\n",
    "Some of the updates that you will be studying will include code implementations. In those cases, you will need a dataset. For this article, you will be using the [Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult) from the UCI Archive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>WorkClass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationNum</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Gender</th>\n",
       "      <th>CapitalGain</th>\n",
       "      <th>CapitalLoss</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>NativeCountry</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age          WorkClass  fnlwgt   Education  EducationNum  \\\n",
       "0   39          State-gov   77516   Bachelors            13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors            13   \n",
       "2   38            Private  215646     HS-grad             9   \n",
       "3   53            Private  234721        11th             7   \n",
       "4   28            Private  338409   Bachelors            13   \n",
       "\n",
       "         MaritalStatus          Occupation    Relationship    Race   Gender  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   CapitalGain  CapitalLoss  HoursPerWeek   NativeCountry  Income  \n",
       "0         2174            0            40   United-States   <=50K  \n",
       "1            0            0            13   United-States   <=50K  \n",
       "2            0            0            40   United-States   <=50K  \n",
       "3            0            0            40   United-States   <=50K  \n",
       "4            0            0            40            Cuba   <=50K  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [\"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n",
    "        \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n",
    "        \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"]\n",
    "\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "                    header=None,\n",
    "                    names=columns)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset represents a binary classification task which is to predict if an individual would earn more than $50k per year or not given a set of personal details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some basic data preprocessing and then set up the data splits in an 80:20 ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Label Encode\n",
    "le = LabelEncoder()\n",
    "data = data.apply(le.fit_transform)\n",
    "\n",
    "# Segregate data features & convert into NumPy arrays\n",
    "X = data.iloc[:, 0:-1].values\n",
    "y = data['Income'].values\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you should have a working environment with `TensorFlow 2.0` installed and a dataset loaded into your workspace. You can straightly proceed towards the updates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Eager execution by default\n",
    "\n",
    "In `TensorFlow 2.0`, you no longer need to create a session and run the computational graph within that. [Eager execution](https://www.tensorflow.org/alpha/guide/eager) is enabled by default in the 2.0 release so that you can build your models and run them instantly. You can choose to disable the eager execution like so :\n",
    "\n",
    "`tf.compat.v1.disable_eager_execution()` (provided `tensorflow` is imported with `tf` alias.) \n",
    "\n",
    "Here's a little code-based comparison that shows this difference - \n",
    "\n",
    "![](https://i.ibb.co/R0pn24K/Untitled-Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tf.function and AutoGraph\n",
    "\n",
    "While eager execution enables you with imperative programming, when it comes to distributed training, full-scale optimization, production environments `TensorFlow 1.x` style graph execution has its advantages over eager execution. In `TensorFlow 2.0`, you retain graph based executions but in a more flexible way. It is achieved with [`tf.function`](https://www.tensorflow.org/alpha/tutorials/eager/tf_function) and [`AutoGraph`](https://www.tensorflow.org/alpha/guide/autograph). \n",
    "\n",
    "`tf.function` allows you to define `TensorFlow` graphs with Python style syntax via its `AutoGraph` feature. `AutoGraph` supports a good range of Python compatibility including `if-statement`, `for-loop`, `while-loop`, `Iterators` etc. However, there are limitations. [Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/LIMITATIONS.md#python-language-support-status) you can find the complete list of supports that are currently available. Below is an example that shows you how easy it is to define a `TensorFlow` graph with just a `decorator`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=73, shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0.5779363 , 0.11255255, 0.26296678, 0.12809312, 0.23484911],\n",
       "       [0.5932371 , 0.1793559 , 0.2845083 , 0.23249313, 0.21367362]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# Define the forward pass\n",
    "@tf.function\n",
    "def single_layer(x, y):\n",
    "    return tf.nn.relu(tf.matmul(x, y))\n",
    "\n",
    "# Generate random data drawn from a uniform distribution\n",
    "x = tf.random.uniform((2, 3))\n",
    "y = tf.random.uniform((3, 5))\n",
    "\n",
    "single_layer(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you did not have to create any sessions or placeholders to run the function `single_layer()`. This is one of the nifty features of `tf.function`. Behind the hood, it does all the necessary optimizations so that your code runs faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `tf.variable_scope` no longer needed\n",
    "\n",
    "In `TensorFlow 1.x`, to be able to use `tf.layers` as variables and to reuse them, you had to use the `tf.variable` block. But this is no longer needed in `TensorFlow 2.0`. Because of the presence of `keras` as the center high-level API in `TensorFlow 2.0`, all the layers created using `tf.layers` can easily be put into a `tf.keras.Sequential` definition. This makes the code much easier to read and you get to keep track of the variables and losses as well. \n",
    "\n",
    "Here's an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.        ]\n",
      " [0.12573627]\n",
      " [1.        ]\n",
      " ...\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]], shape=(26048, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dropout(rate=0.2, input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Get the output probabilities\n",
    "out_probs = model(X_train.astype(np.float32), training=True)\n",
    "print(out_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, you passed the training data through the `model` just to get the raw output probabilities. Notice that it is just a forward pass. You can, of course, go ahead and train your model - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26048 samples, validate on 6513 samples\n",
      "Epoch 1/5\n",
      "26048/26048 [==============================] - 2s 62us/sample - loss: 79.5270 - val_loss: 0.7142\n",
      "Epoch 2/5\n",
      "26048/26048 [==============================] - 1s 48us/sample - loss: 2.0096 - val_loss: 0.5894\n",
      "Epoch 3/5\n",
      "26048/26048 [==============================] - 1s 47us/sample - loss: 0.8750 - val_loss: 0.5761\n",
      "Epoch 4/5\n",
      "26048/26048 [==============================] - 1s 49us/sample - loss: 0.6650 - val_loss: 0.5629\n",
      "Epoch 5/5\n",
      "26048/26048 [==============================] - 1s 47us/sample - loss: 0.6885 - val_loss: 0.5539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc2b1944780>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "              validation_data=(X_test, y_test),\n",
    "              epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of the model's trainable parameters in a layer by layer manner like so - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_12/kernel:0' shape=(14, 64) dtype=float32, numpy=\n",
       " array([[-1.48688853e-02,  2.74527162e-01,  2.58149177e-01,\n",
       "         -2.35980123e-01,  7.92130232e-02, -1.19770452e-01,\n",
       "          1.83823228e-01,  2.26748139e-01, -1.31252930e-01,\n",
       "         -1.67176753e-01,  1.43430918e-01,  2.32805759e-01,\n",
       "          2.47395486e-01,  8.89694989e-02,  1.75705254e-02,\n",
       "         -2.01672405e-01,  2.01087326e-01, -1.67460442e-01,\n",
       "         -1.03051037e-01, -2.56078333e-01, -6.07236922e-02,\n",
       "          4.76933420e-02, -4.65645194e-02,  2.20712095e-01,\n",
       "          1.98741913e-01,  9.32294428e-02,  1.51318759e-01,\n",
       "         -3.96257639e-03, -1.51869521e-01,  8.89182389e-02,\n",
       "         -4.22340333e-02,  1.55168772e-03, -7.01716542e-03,\n",
       "         -8.23616534e-02, -1.85766399e-01, -1.97881564e-01,\n",
       "          1.94241285e-01,  2.11566478e-01, -1.68947518e-01,\n",
       "         -2.34904587e-01, -8.28040987e-02, -1.37671828e-02,\n",
       "          3.46715450e-02,  9.42899585e-02,  9.07505751e-02,\n",
       "          2.64314085e-01,  4.13734019e-02, -1.75569654e-02,\n",
       "          2.49794573e-01,  2.40060896e-01,  1.24608070e-01,\n",
       "         -2.27075279e-01, -1.13472998e-01, -1.09154880e-01,\n",
       "         -2.51923293e-01,  2.43190974e-01,  2.63507813e-01,\n",
       "          1.83881164e-01,  5.65617085e-02, -2.68286765e-01,\n",
       "          1.78039759e-01,  6.91905916e-02, -2.60141104e-01,\n",
       "         -2.56884694e-02],\n",
       "        [-1.60553172e-01,  1.84462130e-01, -1.64327353e-01,\n",
       "         -2.02879310e-03, -1.35839581e-02, -2.11382195e-01,\n",
       "         -1.51656792e-01, -1.50204003e-02,  1.61570847e-01,\n",
       "         -1.29508615e-01, -1.70697004e-01, -2.11556107e-01,\n",
       "          2.15181440e-01,  2.67737001e-01, -1.19572535e-01,\n",
       "          1.15734965e-01, -5.27024269e-02,  4.56553698e-02,\n",
       "         -1.80567816e-01, -1.51056111e-01, -2.31304854e-01,\n",
       "         -1.31544277e-01,  1.42878979e-01, -8.88223648e-02,\n",
       "         -2.77194977e-01,  1.98713481e-01,  1.64229482e-01,\n",
       "         -8.50015134e-02,  1.04941219e-01,  2.73275048e-01,\n",
       "          2.01503932e-02,  2.22145498e-01,  1.61160469e-01,\n",
       "          5.18816710e-02, -1.18925110e-01,  2.20809698e-01,\n",
       "          9.16796625e-02, -1.24019340e-01, -1.42927185e-01,\n",
       "         -1.58376783e-01,  8.95256698e-02, -1.36581853e-01,\n",
       "         -9.74076241e-02, -2.06318110e-01,  4.34296429e-02,\n",
       "          1.48526222e-01, -2.64008492e-01,  2.33468860e-01,\n",
       "         -1.74503058e-01, -2.60894388e-01,  1.12190038e-01,\n",
       "         -1.72933638e-01,  1.87754840e-01,  5.69777489e-02,\n",
       "          9.31494832e-02,  9.37287509e-02, -2.24829912e-01,\n",
       "         -5.65375686e-02, -2.31988132e-01, -5.92674166e-02,\n",
       "         -2.54451334e-01, -1.28820181e-01,  1.57452404e-01,\n",
       "          2.53181010e-01],\n",
       "        [-8.94532055e-02, -7.04574287e-02, -2.74045289e-01,\n",
       "         -2.29278371e-01, -1.12556815e-02, -4.37867343e-02,\n",
       "          6.96483850e-02, -2.20679641e-02, -8.04719925e-02,\n",
       "         -4.27710414e-02, -6.98548555e-03,  5.35116494e-02,\n",
       "         -1.54523849e-02, -1.36115998e-01,  1.38038993e-01,\n",
       "         -1.85180068e-01,  2.15847164e-01,  2.55365819e-01,\n",
       "          1.37135267e-01,  1.90906912e-01, -2.23682523e-02,\n",
       "          1.52650058e-01,  2.04477787e-01, -4.36266363e-02,\n",
       "          1.78499818e-01,  1.90241158e-01, -2.02745885e-01,\n",
       "          1.43350720e-01, -1.13368660e-01, -2.01326758e-01,\n",
       "         -1.61648542e-01,  2.25443751e-01, -2.68535197e-01,\n",
       "          2.37828940e-01,  2.71143168e-01,  1.59860253e-02,\n",
       "          1.41094506e-01, -1.76632628e-01,  1.88476801e-01,\n",
       "          2.02816904e-01, -1.03268191e-01, -2.36591846e-01,\n",
       "          1.79396987e-01,  1.70014054e-01, -2.30597705e-01,\n",
       "          2.61288881e-03, -4.42424417e-03, -3.84955704e-02,\n",
       "          2.72334903e-01, -4.91250306e-02,  1.07610583e-01,\n",
       "         -2.72850186e-01, -2.71188200e-01, -1.15645885e-01,\n",
       "          2.53611356e-01, -1.48682937e-01, -4.46224958e-02,\n",
       "         -6.12093955e-02, -2.67423481e-01, -1.97976261e-01,\n",
       "          4.02505398e-02,  8.28173161e-02,  1.94115847e-01,\n",
       "          6.79514706e-02],\n",
       "        [ 1.02568567e-02, -2.73051471e-01,  1.93972498e-01,\n",
       "          1.67789280e-01, -7.65820295e-02,  1.69053733e-01,\n",
       "         -1.67652726e-01, -1.12306148e-01,  1.29045337e-01,\n",
       "          5.20431995e-03,  1.22617424e-01,  2.59980887e-01,\n",
       "          2.37120360e-01,  2.59193987e-01,  1.71425581e-01,\n",
       "          2.73495167e-01, -3.11368108e-02,  2.11496860e-01,\n",
       "         -2.26072937e-01, -9.43622887e-02,  2.56022662e-01,\n",
       "          1.86894894e-01, -2.35674426e-01, -9.95516777e-03,\n",
       "          1.84704363e-01,  2.27636904e-01, -1.74311996e-02,\n",
       "         -1.57380402e-02, -1.43433169e-01, -1.87973380e-02,\n",
       "          1.76340997e-01, -1.85148180e-01,  1.91334367e-01,\n",
       "          1.00137413e-01, -2.62901902e-01, -8.22693110e-03,\n",
       "         -1.17425114e-01, -2.61702567e-01, -2.40183711e-01,\n",
       "         -7.42957443e-02, -2.43198499e-01,  1.00527972e-01,\n",
       "         -1.11117616e-01, -9.74197388e-02, -1.09167382e-01,\n",
       "         -7.14137256e-02,  2.48018056e-01, -3.86851579e-02,\n",
       "          4.26724553e-02, -2.99333185e-02,  2.41537303e-01,\n",
       "         -2.68284887e-01,  8.95127654e-03, -3.74048352e-02,\n",
       "          4.77899015e-02,  2.41122097e-01,  1.11537516e-01,\n",
       "         -3.37415487e-02, -1.43319309e-01, -1.34244651e-01,\n",
       "          1.61695689e-01, -1.83817685e-01,  5.05107641e-02,\n",
       "          2.74721473e-01],\n",
       "        [ 3.05238366e-02,  4.31960225e-02,  1.15660310e-01,\n",
       "          2.01156676e-01,  8.93190503e-03, -1.82507738e-01,\n",
       "         -1.66644901e-01,  2.53293186e-01,  9.39259827e-02,\n",
       "          2.66437620e-01,  1.03438407e-01,  6.01558089e-02,\n",
       "         -5.76229393e-02,  1.00222319e-01, -8.71886164e-02,\n",
       "          2.47991115e-01,  2.03391343e-01, -5.64218462e-02,\n",
       "         -1.81319863e-01, -1.78091347e-01,  1.94970667e-02,\n",
       "          2.73696750e-01,  2.22271591e-01, -1.62375182e-01,\n",
       "         -1.20849550e-01, -5.32025993e-02, -7.60249197e-02,\n",
       "         -3.30891609e-02, -1.34273469e-01, -7.55624324e-02,\n",
       "          1.07143939e-01,  2.12463081e-01,  7.97367096e-03,\n",
       "         -6.87274337e-03, -8.43367577e-02,  2.55893081e-01,\n",
       "          1.24732047e-01,  3.09056938e-02,  8.86841714e-02,\n",
       "         -2.23312736e-01,  1.97805136e-01,  2.18041629e-01,\n",
       "          3.45717669e-02, -4.20909375e-02,  5.96292019e-02,\n",
       "          1.79306090e-01,  2.72990197e-01,  3.02815437e-02,\n",
       "          2.37860054e-01,  2.76284903e-01,  3.77161503e-02,\n",
       "          2.26478606e-01,  8.85216296e-02, -1.82998061e-01,\n",
       "         -1.41343147e-01, -3.46849561e-02, -2.34851494e-01,\n",
       "          1.46038651e-01, -1.52093291e-01, -8.06826651e-02,\n",
       "          8.09380412e-03,  2.53538191e-02, -1.27880573e-02,\n",
       "          1.55383885e-01],\n",
       "        [-1.07118145e-01,  2.71667391e-01, -1.35462150e-01,\n",
       "          8.78523886e-02,  8.47310722e-02, -3.18741649e-02,\n",
       "         -1.72285080e-01,  9.50790346e-02, -7.42185712e-02,\n",
       "         -1.69902325e-01, -8.20439905e-02, -3.02564055e-02,\n",
       "          1.61808312e-01,  6.13009930e-03,  4.78896201e-02,\n",
       "         -1.39527738e-01, -1.96388185e-01, -9.79056209e-02,\n",
       "          8.11750889e-02, -8.75651240e-02, -3.17215472e-02,\n",
       "          2.24185854e-01,  1.03506386e-01,  2.46435404e-03,\n",
       "         -1.83918521e-01, -1.77772760e-01, -1.59666687e-01,\n",
       "         -5.00660688e-02, -1.95413038e-01,  2.49774963e-01,\n",
       "          2.11800635e-01,  7.34189749e-02, -1.63613647e-01,\n",
       "          1.28584713e-01, -2.04943165e-01,  4.48526740e-02,\n",
       "         -9.40444320e-02, -2.36514211e-01,  4.40850854e-02,\n",
       "         -7.21262991e-02,  5.26860356e-03,  2.54257828e-01,\n",
       "         -1.71898901e-02, -1.66287631e-01, -4.29128110e-02,\n",
       "          3.84885073e-02,  1.63391858e-01, -1.09616295e-01,\n",
       "          2.26927966e-01, -2.67344981e-01,  1.98232234e-01,\n",
       "          1.29737794e-01,  2.69295484e-01, -2.23180622e-01,\n",
       "         -1.87438726e-03, -5.20526767e-02,  9.74531174e-02,\n",
       "         -1.05390891e-01,  1.23165011e-01,  2.33101934e-01,\n",
       "         -2.56039590e-01,  2.46387571e-01,  1.33860320e-01,\n",
       "          1.71753883e-01],\n",
       "        [ 2.46957332e-01, -4.92525846e-02, -2.22080618e-01,\n",
       "          4.05346751e-02, -5.00992537e-02, -2.60361612e-01,\n",
       "          1.50414556e-01,  2.01799482e-01, -2.87890434e-03,\n",
       "          9.51286852e-02, -5.86918592e-02,  2.12740213e-01,\n",
       "         -1.76745623e-01, -2.74649799e-01,  2.05127060e-01,\n",
       "         -4.51588929e-02, -1.18441284e-02,  1.17566496e-01,\n",
       "          2.14967847e-01,  2.30442315e-01, -2.03341544e-02,\n",
       "          7.21938014e-02,  1.91002727e-01, -2.73522615e-01,\n",
       "         -1.07315734e-01,  1.57117695e-01, -7.27429241e-02,\n",
       "          1.98784769e-01,  1.34299874e-01, -2.60534406e-01,\n",
       "          8.44456553e-02,  5.92016876e-02, -8.88088793e-02,\n",
       "          9.40183103e-02,  8.87127221e-02, -9.60084200e-02,\n",
       "          2.42618769e-01,  9.65010524e-02,  6.18630648e-03,\n",
       "          1.61135674e-01, -3.82966697e-02,  1.02110088e-01,\n",
       "         -1.88043356e-01,  6.97199404e-02,  2.39620298e-01,\n",
       "          5.69199026e-02, -1.25965476e-01, -8.32125545e-02,\n",
       "         -8.48805904e-03,  1.70814633e-01,  2.38609940e-01,\n",
       "          9.24529135e-02,  9.29380953e-02, -1.60003811e-01,\n",
       "         -2.04197079e-01,  2.51140565e-01,  2.41884738e-01,\n",
       "         -2.46104851e-01,  6.61611557e-03, -2.67855734e-01,\n",
       "         -7.67029077e-02, -2.74775296e-01,  2.36378461e-01,\n",
       "         -2.72717297e-01],\n",
       "        [ 1.63002580e-01, -1.04987592e-01, -1.11121044e-01,\n",
       "         -2.73849100e-01,  1.99946165e-02,  2.11521506e-01,\n",
       "          2.06256032e-01,  2.54784852e-01,  2.57405788e-01,\n",
       "          1.75982475e-01, -1.57612175e-01, -1.88202858e-02,\n",
       "         -1.82799488e-01, -6.26320094e-02, -9.18765068e-02,\n",
       "         -1.66230381e-01,  2.42929131e-01, -3.45604420e-02,\n",
       "          3.02044451e-02, -1.67087615e-02, -9.18568671e-02,\n",
       "         -1.18204534e-01,  2.26822466e-01, -8.45120549e-02,\n",
       "          1.58829272e-01, -2.22656310e-01, -1.80833176e-01,\n",
       "         -1.51249528e-01,  2.30215102e-01, -2.01435268e-01,\n",
       "          2.50793129e-01,  1.61696225e-01,  1.12378091e-01,\n",
       "         -8.44676197e-02, -1.86490998e-01,  2.16112882e-01,\n",
       "         -1.67694584e-01,  8.36035609e-02,  1.36310160e-02,\n",
       "         -2.36266181e-01,  2.16432512e-02,  2.17068702e-01,\n",
       "          1.48556292e-01, -6.13741130e-02,  1.84532225e-01,\n",
       "         -1.20505244e-01,  5.50346076e-02,  1.04375720e-01,\n",
       "          1.96388662e-01,  2.04656780e-01,  8.99768472e-02,\n",
       "          1.04485691e-01,  1.16647959e-01, -9.09715742e-02,\n",
       "          2.40128249e-01,  7.08191991e-02, -1.35386303e-01,\n",
       "          1.52992904e-02,  2.04906076e-01,  2.08586067e-01,\n",
       "          2.65424818e-01,  1.74420804e-01,  1.45571589e-01,\n",
       "         -1.06450215e-01],\n",
       "        [-1.22071415e-01,  6.90596700e-02, -9.81627107e-02,\n",
       "         -1.82385862e-01,  3.71887982e-02,  1.33560777e-01,\n",
       "          6.62094355e-03, -2.25594267e-01, -8.94398540e-02,\n",
       "         -2.11033255e-01,  2.53058523e-01,  5.08429706e-02,\n",
       "         -1.27695456e-01, -7.27435797e-02, -1.51305407e-01,\n",
       "          3.16268504e-02,  2.58970231e-01,  8.51702690e-02,\n",
       "          2.73242801e-01, -1.25677899e-01, -2.71640301e-01,\n",
       "         -1.60824418e-01, -2.76342273e-01,  2.24858135e-01,\n",
       "         -8.03019106e-02, -4.79616970e-02,  4.94971275e-02,\n",
       "          2.46035010e-01, -1.74869299e-02,  1.85437828e-01,\n",
       "         -2.01017499e-01, -2.23311543e-01,  2.70765752e-01,\n",
       "         -2.11389661e-01, -2.26453170e-01,  2.06002831e-01,\n",
       "          2.16605961e-01,  1.56077802e-01, -2.76331574e-01,\n",
       "         -7.14364648e-03, -1.25960454e-01,  1.02812976e-01,\n",
       "          5.37744164e-03, -9.14498568e-02, -2.16731012e-01,\n",
       "         -4.22561914e-02, -1.18804276e-02, -4.11395282e-02,\n",
       "         -2.58837283e-01, -9.24162269e-02,  2.24286765e-01,\n",
       "          1.97664350e-01, -2.04566836e-01,  1.49493903e-01,\n",
       "          1.82809919e-01,  2.18066871e-01,  2.27073222e-01,\n",
       "          1.76770508e-01,  1.28788888e-01,  7.43162632e-03,\n",
       "         -2.44799465e-01,  2.06821591e-01, -9.25005376e-02,\n",
       "          1.84141576e-01],\n",
       "        [ 1.05317682e-01,  1.83150172e-02, -6.71321154e-02,\n",
       "          1.00300103e-01, -2.54237145e-01, -3.71084660e-02,\n",
       "         -1.02833554e-01, -5.97543716e-02, -2.18547538e-01,\n",
       "         -8.90600234e-02, -2.40394264e-01, -2.57878542e-01,\n",
       "         -1.38011947e-01,  2.36597955e-02, -2.27259427e-01,\n",
       "         -1.65269971e-02,  2.32348710e-01, -1.00096032e-01,\n",
       "         -2.13123351e-01, -1.40784979e-02, -2.66731352e-01,\n",
       "         -2.15898558e-01, -5.78602701e-02,  1.08396888e-01,\n",
       "         -2.02795267e-01, -1.52687684e-01,  2.78952122e-02,\n",
       "          4.09219265e-02, -5.15770912e-02, -1.81588203e-01,\n",
       "          2.73707718e-01,  1.09840721e-01, -1.40243679e-01,\n",
       "         -2.13766873e-01, -1.94679320e-01, -9.15652514e-03,\n",
       "         -1.61587566e-01,  2.27655083e-01, -1.11349046e-01,\n",
       "         -1.05967700e-01,  8.99270475e-02,  2.07172066e-01,\n",
       "          5.06473184e-02,  2.01718628e-01, -1.03773981e-01,\n",
       "          2.73704678e-01,  4.07311916e-02,  9.41670239e-02,\n",
       "         -7.51210451e-02,  2.25694746e-01,  4.44093049e-02,\n",
       "          2.77287036e-01,  2.25879252e-02, -6.58842623e-02,\n",
       "         -2.06691712e-01, -1.68207854e-01,  1.10538006e-02,\n",
       "         -1.19143382e-01,  1.65247411e-01, -1.02170840e-01,\n",
       "          7.17070699e-02, -7.43492991e-02, -7.37106651e-02,\n",
       "         -1.29226327e-01],\n",
       "        [ 2.08517313e-02,  8.65581036e-02, -2.01248676e-01,\n",
       "         -1.06920242e-01,  2.04556465e-01, -5.12601584e-02,\n",
       "          1.17174774e-01, -1.21960059e-01, -1.31039545e-01,\n",
       "          1.45936877e-01,  9.38895345e-03, -1.14137828e-02,\n",
       "          1.54711992e-01,  2.67244726e-01, -7.15402961e-02,\n",
       "         -2.23028928e-01, -2.71299481e-01, -1.36449203e-01,\n",
       "         -1.25627816e-02,  3.13916504e-02,  1.73118323e-01,\n",
       "         -2.17780888e-01, -1.95076853e-01,  1.28784478e-02,\n",
       "          1.73919499e-01, -2.42948875e-01, -2.14346394e-01,\n",
       "          5.35857081e-02,  2.67256826e-01, -1.71346068e-02,\n",
       "         -2.76432812e-01, -1.73468918e-01,  1.22662723e-01,\n",
       "         -9.96078849e-02, -1.15638345e-01, -2.65158296e-01,\n",
       "          2.12729961e-01, -2.70184338e-01,  1.08982086e-01,\n",
       "         -1.14385784e-02,  2.67733067e-01,  2.64605552e-01,\n",
       "          7.57011771e-02, -8.78878832e-02, -9.69131440e-02,\n",
       "         -6.81236386e-03,  6.40029907e-02, -1.91579491e-01,\n",
       "          1.71635926e-01, -2.19610840e-01, -1.01383820e-01,\n",
       "          1.74940199e-01, -1.23514935e-01, -4.02086824e-02,\n",
       "          2.65191942e-01, -2.47828737e-01, -5.83019853e-03,\n",
       "         -1.24326095e-01, -2.10787788e-01, -2.57244408e-02,\n",
       "         -9.65181738e-02, -1.34586707e-01, -2.63660282e-01,\n",
       "         -2.33780265e-01],\n",
       "        [-2.09537894e-01,  1.81803823e-01, -2.23274127e-01,\n",
       "          2.68277794e-01, -2.12194473e-01,  2.69619197e-01,\n",
       "         -1.91460058e-01,  1.50443584e-01, -6.01146221e-02,\n",
       "          1.15322739e-01,  5.74926138e-02, -2.09335685e-01,\n",
       "          2.66064018e-01, -2.50099152e-01,  2.27989703e-01,\n",
       "          1.48722529e-03, -2.75823861e-01, -2.74460733e-01,\n",
       "         -2.54678339e-01,  2.07069367e-01,  2.42757052e-01,\n",
       "         -8.09566826e-02, -2.22230926e-01,  3.88453007e-02,\n",
       "         -7.51499534e-02, -1.13763615e-01,  1.86943352e-01,\n",
       "          1.81314886e-01, -1.03227988e-01,  1.27721041e-01,\n",
       "          1.00327253e-01, -1.25737816e-01, -9.31653380e-03,\n",
       "         -1.79606676e-02, -1.99202478e-01,  1.40470475e-01,\n",
       "         -1.78151071e-01,  3.56182456e-02,  2.09965855e-01,\n",
       "          9.80757773e-02,  9.55764055e-02,  2.42440253e-01,\n",
       "          2.26146430e-01, -8.72465968e-03, -2.06995502e-01,\n",
       "          1.26261711e-01,  1.92399114e-01,  2.21498907e-02,\n",
       "          2.40556687e-01, -1.17468238e-01, -8.96153450e-02,\n",
       "          3.64099145e-02,  5.64157963e-05, -9.97322649e-02,\n",
       "          1.81693852e-01, -1.95398301e-01,  2.67696530e-01,\n",
       "          2.18172163e-01,  1.50565267e-01, -2.76668876e-01,\n",
       "         -2.90721059e-02,  6.15487993e-02,  5.47989309e-02,\n",
       "         -2.45864540e-01],\n",
       "        [ 1.13498271e-01, -1.24701887e-01, -1.19635433e-01,\n",
       "          6.81682229e-02,  1.42366707e-01, -5.18653989e-02,\n",
       "          1.70933545e-01,  4.18927073e-02, -8.23812187e-02,\n",
       "         -1.72122866e-01,  3.46628726e-02,  2.39999801e-01,\n",
       "         -4.86224890e-04,  8.29051435e-02, -6.71084374e-02,\n",
       "         -1.72895417e-01, -2.63225108e-01, -1.55994743e-01,\n",
       "          8.19830298e-02,  2.49279350e-01, -1.41113624e-01,\n",
       "          1.25947356e-01, -9.30310488e-02,  2.40998656e-01,\n",
       "          2.44344383e-01, -1.36330962e-01, -1.14291891e-01,\n",
       "         -2.29074568e-01,  1.76846683e-01, -7.63051659e-02,\n",
       "         -6.28410280e-02, -1.43780455e-01, -7.99130350e-02,\n",
       "         -2.32542127e-01, -3.03542614e-03,  7.96765089e-03,\n",
       "          2.05407441e-02, -3.18776071e-02, -1.66951925e-01,\n",
       "         -2.53402591e-01,  1.85931325e-02, -2.08924711e-02,\n",
       "         -2.02480197e-01, -1.78624660e-01, -9.39854980e-03,\n",
       "          2.22942740e-01, -7.72327036e-02,  8.92090797e-03,\n",
       "          5.94776869e-03, -1.45615578e-01, -1.00357220e-01,\n",
       "         -6.98443055e-02, -1.69289708e-02,  1.10462517e-01,\n",
       "         -2.50632793e-01,  1.05173588e-01, -1.03613839e-01,\n",
       "         -1.78682446e-01, -4.74603325e-02,  2.64549822e-01,\n",
       "          2.41646737e-01, -9.74451900e-02, -1.91499934e-01,\n",
       "         -2.03671366e-01],\n",
       "        [ 3.43604088e-02, -4.77244258e-02, -2.74687082e-01,\n",
       "          1.44897908e-01,  1.87038392e-01, -2.73052067e-01,\n",
       "         -1.34714529e-01, -1.96854770e-02,  1.78879768e-01,\n",
       "         -4.30725813e-02, -1.44803524e-02, -4.08369452e-02,\n",
       "          1.24610901e-01,  1.33537620e-01, -5.67995459e-02,\n",
       "          1.66517943e-01,  1.21737421e-02, -2.28156358e-01,\n",
       "          2.42469996e-01, -8.04692805e-02,  2.54256994e-01,\n",
       "          1.89271569e-02,  1.06245875e-01,  2.76879996e-01,\n",
       "          1.47841871e-01, -9.83145386e-02,  1.41099930e-01,\n",
       "         -9.15518403e-03,  2.22966105e-01,  1.95244431e-01,\n",
       "          2.46362776e-01,  1.43388927e-01,  2.12212205e-01,\n",
       "         -2.39929557e-02,  2.23469466e-01,  2.43519396e-01,\n",
       "          2.35615760e-01, -7.24931657e-02, -9.37553197e-02,\n",
       "          2.35618442e-01,  1.09928012e-01, -2.83769220e-02,\n",
       "         -1.05210841e-02, -2.18923137e-01, -1.58438280e-01,\n",
       "         -1.87489986e-02,  1.51137710e-02,  1.77096963e-01,\n",
       "          7.83360600e-02,  2.20489174e-01, -3.45443189e-02,\n",
       "          6.89106286e-02,  2.31777161e-01, -1.25984594e-01,\n",
       "          1.43728256e-02,  2.55063027e-01, -2.42056713e-01,\n",
       "          8.74229670e-02,  2.20979035e-01, -2.00921297e-03,\n",
       "          1.69425875e-01, -8.34510028e-02, -1.03761226e-01,\n",
       "          8.88096690e-02]], dtype=float32)>,\n",
       " <tf.Variable 'dense_12/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_13/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[ 0.20200957,  0.03036232,  0.11040972, ..., -0.21020778,\n",
       "          0.17196609, -0.03736575],\n",
       "        [-0.2064129 ,  0.13786067,  0.09109865, ..., -0.15494904,\n",
       "          0.09000905, -0.18967415],\n",
       "        [-0.0387924 , -0.02436857,  0.16121905, ..., -0.1803377 ,\n",
       "         -0.00170219,  0.15630807],\n",
       "        ...,\n",
       "        [ 0.19548352,  0.10514452, -0.03767221, ...,  0.03404056,\n",
       "          0.02135798,  0.00550348],\n",
       "        [-0.16041529, -0.07542154, -0.1700579 , ...,  0.00083075,\n",
       "          0.11576484,  0.08763643],\n",
       "        [-0.09544714,  0.08534966, -0.06500863, ...,  0.04508607,\n",
       "         -0.17440501,  0.1134396 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_13/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_14/kernel:0' shape=(64, 1) dtype=float32, numpy=\n",
       " array([[ 0.17874134],\n",
       "        [ 0.06660989],\n",
       "        [ 0.2120269 ],\n",
       "        [ 0.1908356 ],\n",
       "        [-0.05980097],\n",
       "        [ 0.2545969 ],\n",
       "        [ 0.16937432],\n",
       "        [ 0.28103924],\n",
       "        [-0.301428  ],\n",
       "        [-0.1401844 ],\n",
       "        [-0.02959338],\n",
       "        [ 0.10712665],\n",
       "        [ 0.09891567],\n",
       "        [-0.28661886],\n",
       "        [ 0.28736794],\n",
       "        [ 0.03912222],\n",
       "        [-0.03885537],\n",
       "        [-0.25707358],\n",
       "        [-0.24519518],\n",
       "        [ 0.11147693],\n",
       "        [ 0.02554649],\n",
       "        [-0.20881867],\n",
       "        [ 0.00373942],\n",
       "        [ 0.02928248],\n",
       "        [ 0.09055263],\n",
       "        [ 0.15126869],\n",
       "        [-0.11197442],\n",
       "        [ 0.23908103],\n",
       "        [ 0.07320437],\n",
       "        [-0.05635457],\n",
       "        [ 0.14777556],\n",
       "        [-0.17251213],\n",
       "        [-0.02642217],\n",
       "        [ 0.25192064],\n",
       "        [-0.15656634],\n",
       "        [-0.0924283 ],\n",
       "        [-0.20901027],\n",
       "        [-0.17767514],\n",
       "        [-0.15508023],\n",
       "        [ 0.06313407],\n",
       "        [ 0.2708218 ],\n",
       "        [-0.14065444],\n",
       "        [ 0.12714231],\n",
       "        [-0.05807959],\n",
       "        [ 0.17975545],\n",
       "        [ 0.19628727],\n",
       "        [-0.24905266],\n",
       "        [-0.12731928],\n",
       "        [-0.15389986],\n",
       "        [-0.15024558],\n",
       "        [-0.08432762],\n",
       "        [-0.28963754],\n",
       "        [-0.07519016],\n",
       "        [-0.04082993],\n",
       "        [ 0.13681188],\n",
       "        [ 0.18757123],\n",
       "        [ 0.09581241],\n",
       "        [ 0.09615937],\n",
       "        [ 0.22277021],\n",
       "        [ 0.2865938 ],\n",
       "        [ 0.00316831],\n",
       "        [-0.27389333],\n",
       "        [-0.09506477],\n",
       "        [ 0.01873708]], dtype=float32)>,\n",
       " <tf.Variable 'dense_14/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model's trainable parameters in a layer by layer fashion\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom layers made very easy\n",
    "\n",
    "In machine learning research or even in industrial applications, there is often a need for writing custom layers to cater to specific use cases. `TensorFlow 2.0` makes it super easy to write a custom layer and use it along with the existing layers. You can also customize the forward pass of your model in any way you want.\n",
    "\n",
    "In order to create a custom layer, the easiest option is to extend the `Layer` class from `tf.keras.layers` and then define it accordingly. You will create a custom layer and you will then define its forward computations. Following is the output of executing `help(tf.keras.layers.Layer)`. It tells you what are the things you need to specify in order to get this done:\n",
    "\n",
    "![](https://i.ibb.co/5xC30pk/Screenshot-from-2019-05-24-17-43-26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking help from the above snippet, you will - \n",
    "* Define the constructor with the number of the outputs\n",
    "* In the `build()` method you will add the weights for your layer\n",
    "* Finally in the `call()` method you will define the forward pass by chaining matrix multiplication and `relu()` together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'my_dense_layer_7/kernel:0' shape=(3, 10) dtype=float32, numpy=\n",
      "array([[ 0.43613756,  0.21344548,  0.37803996,  0.65583944,  0.11884308,\n",
      "         0.13909656,  0.30802298,  0.5313586 ,  0.04967308,  0.32889426],\n",
      "       [ 0.1680265 , -0.59944266, -0.4014195 ,  0.14887196,  0.07071263,\n",
      "         0.37862527, -0.5822403 , -0.5963166 ,  0.3106798 ,  0.05353856],\n",
      "       [-0.44345278, -0.23122305, -0.62959856, -0.43062705,  0.13194847,\n",
      "        -0.60124606, -0.62745696,  0.12254918, -0.09806103, -0.45324165]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    # Define the constructor\n",
    "    def __init__(self, num_outputs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "    # Define the build function to add the weights\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_variable(\"kernel\",\n",
    "                                    shape=[input_shape[-1],\n",
    "                                           self.num_outputs])\n",
    "    # Define the forward pass\n",
    "    def call(self, input):\n",
    "        matmul = tf.matmul(input, self.kernel)\n",
    "        return tf.nn.relu(matmul)\n",
    "\n",
    "# Initialize the layer with 10 output units\n",
    "layer = MyDenseLayer(10)\n",
    "# Supply the input shape\n",
    "layer(tf.random.uniform((10,3)))\n",
    "# Display the trainable parameters of the layer\n",
    "print(layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compose multiple layers by extending `Model` class from `tf.keras`. You can find more about composing models [here](https://www.tensorflow.org/alpha/tutorials/eager/custom_layers#models_composing_layers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Flexibility in model training\n",
    "\n",
    "`TensorFlow` can use [automatic differentiation](https://www.tensorflow.org/alpha/tutorials/eager/automatic_differentiation) to compute the gradients of the loss function with respect to model parameters. `tf.GradientTape` creates a tape within a context which is used by `TensorFlow` to keep track of the gradients recorded from each computation in that tape. To understand this, let's define a model in a more low-level by extending the `tf.keras.Model` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "class CustomModel(Model):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.do1 = tf.keras.layers.Dropout(rate=0.2, input_shape=(14,))\n",
    "        self.fc1 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "        self.do2 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        self.fc2 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "        self.do3 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        self.out = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.do1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.do2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.do3(x)\n",
    "        return self.out(x)\n",
    "\n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the topology of this model is exactly the same as the one you defined earlier. \n",
    "To be able to train this model using automatic differentiation, you need to define the loss function and the optimizer in a different way - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now define the metrics which will be used to measure the performance of the network turning its training. By performance, model's loss and accuracy are meant here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the loss across the batch size within an epoch\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_acc = tf.keras.metrics.BinaryAccuracy(name='train_acc')\n",
    "\n",
    "valid_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "valid_acc = tf.keras.metrics.BinaryAccuracy(name='valid_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data` provides utility methods to define input data pipelines. This is particularly very useful when you are dealing with a large volume of data. \n",
    "\n",
    "You will now define the data generator which will generate batches of data during the model's training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.astype(np.float32), X_test.astype(np.float32)\n",
    "y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)\n",
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)\n",
    "\n",
    "# Batches of 64\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to train the model using `tf.GradientTape`. Firstly, you will define a method which will train the model with the data you just defined using `tf.data.DataSet`. You will also wrap the model training steps with the `tf.function` decorator to take the advantage of the speedup it offers in the computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "@tf.function\n",
    "def model_train(features, labels):\n",
    "    # Define the GradientTape context\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the probabilities\n",
    "        predictions = model(features)\n",
    "        # Calculate the loss\n",
    "        loss = loss_func(labels, predictions)\n",
    "    # Get the gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Update the weights\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_acc(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the model\n",
    "@tf.function\n",
    "def model_validate(features, labels):\n",
    "    predictions = model(features)\n",
    "    t_loss = loss_func(labels, predictions)\n",
    "\n",
    "    valid_loss(t_loss)\n",
    "    valid_acc(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above two methods to train and validate the model for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 9.8155517578125, train_acc: 66.32754516601562, train_loss: 2.8762073516845703, test_acc: 78.96514892578125\n",
      "Epoch 2, train_loss: 10.235926628112793, train_acc: 67.04353332519531, train_loss: 3.508544921875, test_acc: 79.0572738647461\n",
      "Epoch 3, train_loss: 8.876679420471191, train_acc: 67.97962951660156, train_loss: 4.440890789031982, test_acc: 78.7348403930664\n",
      "Epoch 4, train_loss: 8.136384963989258, train_acc: 68.46015167236328, train_loss: 3.812603235244751, test_acc: 73.58360290527344\n",
      "Epoch 5, train_loss: 7.779866695404053, train_acc: 68.70469665527344, train_loss: 3.80180025100708, test_acc: 74.73975372314453\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for features, labels in train_ds:\n",
    "        model_train(features, labels)\n",
    "\n",
    "    for test_features, test_labels in test_ds:\n",
    "        model_validate(test_features, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, train_loss: {}, train_acc: {}, train_loss: {}, test_acc: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_acc.result()*100,\n",
    "                         valid_loss.result(),\n",
    "                         valid_acc.result()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is inspired by [this example](https://www.tensorflow.org/alpha/tutorials/quickstart/advanced) by `TensorFlow 2.0`'s authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TensorFlow datasets\n",
    "\n",
    "A separate module named `DataSets` to operate with the network model in an elegant way. You already saw this in the earlier example. In this section, you will see how you can load in the MNIST dataset just in the way you want.\n",
    "\n",
    "You can install the `tensorflow_datasets` library with `pip`. Once it is installed you are ready to go. It provides a number of utility functions to help you prepare your dataset construction pipeline in a flexible way. You can check more about these functions [here](https://github.com/tensorflow/datasets) and [here](https://www.youtube.com/watch?v=-nTe44WT0ZI). You will now see how you can build a data input pipeline to load in the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# You can fetch the DatasetBuilder class by string\n",
    "mnist_builder = tfds.builder(\"mnist\")\n",
    "\n",
    "# Download the dataset\n",
    "mnist_builder.download_and_prepare()\n",
    "\n",
    "# Construct a tf.data.Dataset: train and test\n",
    "ds_train, ds_test = mnist_builder.as_dataset(split=[tfds.Split.TRAIN, tfds.Split.TEST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ignore the warning. Notice how elegantly `tensorflow_datasets` handled the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batches of 128 from the training set\n",
    "ds_train = ds_train.batch(128)\n",
    "\n",
    "# Load in the dataset in the simplest way possible\n",
    "for features in ds_train:\n",
    "    image, label = features[\"image\"], features[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now display the first image from the collection of images you loaded in. **Note** that `tensorflow_datasets` works in eager mode and in a graph based setting as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADi1JREFUeJzt3X+sVPWZx/HPs2xJjBR/5A70huJebIxZYxQ2E7IJZnXTWOmmBkosAoFgbKCGaraBP1ZJTPlng26Urokb9Ha9KUZqW9O6YkQFzSZK3DTMNTfVlu5i8EqvIFz8EcQ/ROXZP+65zS3e+Z5h5sycuTzvV2LuzHnOmfPkxA9nZr7nzNfcXQDi+auyGwBQDsIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCov+7kznp6eryvr6+TuwRCGR4e1okTJ6yRdVsKv5ktlvSQpGmS/tPd70ut39fXp1qt1souASRUq9WG1236bb+ZTZP0H5K+LekqSSvN7KpmXw9AZ7XymX+hpLfc/ZC7n5b0C0lLimkLQLu1Ev45kv404flItuwvmNl6M6uZWW10dLSF3QEoUivhn+xLhS/dH+zu/e5edfdqpVJpYXcAitRK+EckzZ3w/OuSjrTWDoBOaSX8+yVdYWbzzGy6pBWSdhXTFoB2a3qoz90/N7M7Jb2osaG+AXf/fWGdAWirlsb53X23pN0F9QKgg7i8FwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaBamqXXzIYlfSzpC0mfu3u1iKYAtF9L4c/8o7ufKOB1AHQQb/uBoFoNv0vaY2aDZra+iIYAdEarb/sXufsRM5slaa+Z/dHdX5m4QvaPwnpJuuyyy1rcHYCitHTmd/cj2d/jkp6WtHCSdfrdveru1Uql0sruABSo6fCb2YVm9tXxx5K+JenNohoD0F6tvO2fLelpMxt/nZ+7+wuFdAWg7ZoOv7sfknRtgb2gBJ988kmyfssttyTrL7yQ/vf+1VdfrVu77rrrktuivRjqA4Ii/EBQhB8IivADQRF+ICjCDwRVxF196GKnTp1K1lesWJGsv/jii8l6dp1HXcPDw3VrDPWVizM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFOP95btWqVcn67t27W3r91atXJ+vXXHNNS69flkOHDiXrebdC9/b2Jus9PT3n3FPROPMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM83eBM2fOJOsffvhhsv7AAw/UreXdj9+qGTNmJOtXXnllW/efcvLkybq1rVu3Jrfdtm1bsj5t2rRk/YknnkjWly1blqx3Amd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwgqd5zfzAYkfUfScXe/Olt2qaRfSuqTNCxpubunB6NRV944fqVSSdbdvW4t73f1W5V3HcH+/fvr1lr93f5PP/00Wb/++uvr1oaGhpLb5h23Cy64IFnPu9+/GzRy5v+ZpMVnLbtb0svufoWkl7PnAKaQ3PC7+yuSPjhr8RJJO7LHOyQtLbgvAG3W7Gf+2e5+VJKyv7OKawlAJ7T9Cz8zW29mNTOrjY6Otnt3ABrUbPiPmVmvJGV/j9db0d373b3q7tW8L64AdE6z4d8laW32eK2kZ4ppB0Cn5IbfzJ6U9D+SrjSzETP7vqT7JN1oZgcl3Zg9BzCF5I7zu/vKOqVvFtzLeevdd99N1u+6665kPTWOnydv2/7+/mR93bp1Te+73fbs2ZOs543lp1x00UXJ+qZNm5L1NWvWNL3vTuEKPyAowg8ERfiBoAg/EBThB4Ii/EBQ/HR3B+QN9T333HPJet7tpanhvHvvvTe57e23356sl+n5559P1vOGSFPHLW8o79lnn03WW70duRtw5geCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnL8DIyEiyfuuttybrn332WUv7v+222+rW7rnnnuS2eVNNt9OBAweS9Q0bNiTrhw8fTtZnzpxZt5Z3S+75MI6fhzM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFOH8BHnnkkWT9nXfeSdbzxto3btyYrN9///3JeplOnz5dt7Z0aXp+11aPW+pnyZcvX57cNgLO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVO44v5kNSPqOpOPufnW2bIukdZJGs9U2u/vudjXZ7R599NGWtr/44ouT9W4ex8+zevXqurWDBw+29Np51z8wlp/WyJn/Z5IWT7L8J+4+P/svbPCBqSo3/O7+iqQPOtALgA5q5TP/nWb2OzMbMLNLCusIQEc0G/7tkr4hab6ko5IerLeima03s5qZ1UZHR+utBqDDmgq/ux9z9y/c/Yykn0pamFi3392r7l6tVCrN9gmgYE2F38x6Jzz9rqQ3i2kHQKc0MtT3pKQbJPWY2YikH0u6wczmS3JJw5J+0MYeAbRBbvjdfeUkix9rQy9TVm9vb7J+4sSJluqXX355sr5o0aJkPeXmm29O1qdPn56sP/hg3a97JEn79u07557GpeYjkKb29Q/dgCv8gKAIPxAU4QeCIvxAUIQfCIrwA0Hx090F2LFjR7K+ZMmSZD1viu/h4eFk/e23365bM7Pktjt37kzW22nevHnJeitDmMjHmR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmKcvwALFixI1oeGhpL11157LVkfHBw8554a9fDDDyfr77//fkuvv2zZsrq17du3J7edNWtWS/tGGmd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwjK3L1jO6tWq16r1Tq2P0jvvfdesn7ttdcm63lTrOVtv3fv3rq1np6e5LY4d9VqVbVaLf0jDhnO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVO79/GY2V9Ljkr4m6Yykfnd/yMwulfRLSX2ShiUtd/cP29cqmrFly5ZkPW8cP8/WrVuTdcbyu1cjZ/7PJW1y97+V9PeSfmhmV0m6W9LL7n6FpJez5wCmiNzwu/tRd389e/yxpAOS5khaIml8qpodkpa2q0kAxTunz/xm1idpgaTfSprt7kelsX8gJPGbS8AU0nD4zWyGpF9L+pG7nzyH7dabWc3Maq1+vgRQnIbCb2Zf0Vjwd7r7b7LFx8ysN6v3Sjo+2bbu3u/uVXevViqVInoGUIDc8NvYNK+PSTrg7tsmlHZJWps9XivpmeLbA9Aujfx09yJJayS9YWbjv0G9WdJ9kn5lZt+XdFjS99rTIlrx1FNPJet5t3TfcccdyfrixYvPuSd0h9zwu/s+SfXuD/5mse0A6BSu8AOCIvxAUIQfCIrwA0ERfiAowg8ExRTd54GXXnqpbu2jjz5Kbjt79uxkPe+WYExdnPmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjG+c8Dg4ODdWt59+tv2LAhWc+7DgBTF2d+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiKcf7zwJw5c5redtWqVQV2gqmEMz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBJU7zm9mcyU9Lulrks5I6nf3h8xsi6R1kkazVTe7++52NYr6brrpprq1mTNnJrfduHFjsj4wMJCs9/T0JOvoXo1c5PO5pE3u/rqZfVXSoJntzWo/cfcH2tcegHbJDb+7H5V0NHv8sZkdkNT8JWUAusI5feY3sz5JCyT9Nlt0p5n9zswGzOySOtusN7OamdVGR0cnWwVACRoOv5nNkPRrST9y95OStkv6hqT5Gntn8OBk27l7v7tX3b1aqVQKaBlAERoKv5l9RWPB3+nuv5Ekdz/m7l+4+xlJP5W0sH1tAihabvjNzCQ9JumAu2+bsLx3wmrflfRm8e0BaJdGvu1fJGmNpDfMbChbtlnSSjObL8klDUv6QVs6RK7Ux6m8KboRVyPf9u+TZJOUGNMHpjCu8AOCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRl7t65nZmNSnpnwqIeSSc61sC56dbeurUvid6aVWRvf+PuDf1eXkfD/6Wdm9XcvVpaAwnd2lu39iXRW7PK6o23/UBQhB8Iquzw95e8/5Ru7a1b+5LorVml9FbqZ34A5Sn7zA+gJKWE38wWm9n/mtlbZnZ3GT3UY2bDZvaGmQ2ZWa3kXgbM7LiZvTlh2aVmttfMDmZ/J50mraTetpjZu9mxGzKzfyqpt7lm9t9mdsDMfm9m/5wtL/XYJfoq5bh1/G2/mU2T9H+SbpQ0Imm/pJXu/oeONlKHmQ1Lqrp76WPCZvYPkk5Jetzdr86W/ZukD9z9vuwfzkvc/V+6pLctkk6VPXNzNqFM78SZpSUtlXSbSjx2ib6Wq4TjVsaZf6Gkt9z9kLuflvQLSUtK6KPrufsrkj44a/ESSTuyxzs09j9Px9XprSu4+1F3fz17/LGk8ZmlSz12ib5KUUb450j604TnI+quKb9d0h4zGzSz9WU3M4nZ2bTp49Onzyq5n7PlztzcSWfNLN01x66ZGa+LVkb4J5v9p5uGHBa5+99J+rakH2Zvb9GYhmZu7pRJZpbuCs3OeF20MsI/ImnuhOdfl3SkhD4m5e5Hsr/HJT2t7pt9+Nj4JKnZ3+Ml9/Nn3TRz82QzS6sLjl03zXhdRvj3S7rCzOaZ2XRJKyTtKqGPLzGzC7MvYmRmF0r6lrpv9uFdktZmj9dKeqbEXv5Ct8zcXG9maZV87LptxutSLvLJhjL+XdI0SQPu/q8db2ISZna5xs720tgkpj8vszcze1LSDRq76+uYpB9L+i9Jv5J0maTDkr7n7h3/4q1Obzdo7K3rn2duHv+M3eHerpP0qqQ3JJ3JFm/W2Ofr0o5doq+VKuG4cYUfEBRX+AFBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCOr/Abh8EXxBn2YTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# You can convert a TensorFlow tensor just by using\n",
    "# .numpy()\n",
    "plt.imshow(image[0].numpy().reshape(28, 28), cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Automatic mixed precision policy\n",
    "\n",
    "Mixed precision policy was proposed by NVIDIA last year. You can find the original paper [here](https://arxiv.org/abs/1710.03740). The brief idea behind mixed precision policy is to use a mixture of half (FP16) and full precision (FP32) and take advantages of both the worlds. It has shown amazing results in the training of very deep neural networks (both in terms of time and score). \n",
    "\n",
    "If you are on a CUDA enabled GPU environment (Volta Generation, Tesla T4 for example) and you installed the GPU variant of `TensorFlow 2.0`, you can instruct `TensorFlow` to train in mixed precision like so - \n",
    "\n",
    "`os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will automatically cast the operations of a TensorFlow graph accordingly. You will be able to see a good amount of boost in your model's performance. You can also optimize TensorFlow core operations with mixed precision policy. Check [this article](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html) to know more on this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Distributed training\n",
    "\n",
    "`TensorFlow 2.0` makes it super easy to distribute the training process across multiple GPUs. This is particularly useful for production purpose when you have to meet super heavy loads. This is as easy as putting your model training block inside a `with` block. \n",
    "\n",
    "First, you specify a distribution strategy like so:\n",
    "\n",
    "`mirrored_strategy = tf.distribute.MirroredStrategy()`\n",
    "\n",
    "A mirrored strategy creates one replica per GPU and the model variables are equally mirrored across GPUs. You can now use the defined strategy like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with mirrored_strategy.scope():\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "    model.fit(X_train, y_train,\n",
    "             validation_data=(X_test, y_test),\n",
    "             batch_size=128,\n",
    "             epochs=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that the above piece of code will only be useful if you have multiple GPUs configured on a single system. There are a number of distribution strategies you can configure. You can find more about it [here](https://www.tensorflow.org/alpha/guide/distribute_strategy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. TensorBoard within Jupyter Notebook\n",
    "\n",
    "This is probably the most exciting part of this update. You can visualize the model training directly within your Jupyter Notebook via `TensorBoard`. The new `TensorBoard` is loaded with a lot of exciting features like memory profiling, viewing image data including confusion matrix, conceptual model graph and so on. You can find more about this [here](https://www.tensorflow.org/tensorboard/r2/get_started). \n",
    "\n",
    "In this section, you will configure your environment such that the `TensorBoard` is displayed within Jupyter Notebook. You will first have load the `tensorboard.notebook` notebook extension - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard.notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now define the `TensorBoard` callback using the `tf.keras.callbacks` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Make a directory to keep the training logs\n",
    "os.mkdir(\"logs\")\n",
    "\n",
    "# Set the callback\n",
    "logdir = \"logs\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild the model using the `Sequential` API of `tf.keras` - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dropout(rate=0.2, input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test sets were modified for different uses. So, it will be a good idea to split them once again - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are all ready to train the model - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc215b69710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc215b69400>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The TensorBoard extension\n",
    "%tensorboard --logdir logs/\n",
    "# Pass the TensorBoard callback you defined\n",
    "model.fit(X_train, y_train,\n",
    "         validation_data=(X_test, y_test),\n",
    "         batch_size=64,\n",
    "         epochs=10,\n",
    "         callbacks=[tensorboard_callback],\n",
    "         verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TensorBoard` dashboard should be loaded in your Jupyter Notebook and you should be able to trace the training and validation metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorFlow for Swift\n",
    "\n",
    "Despite all the incredible success, one thing very saddening about Python is that it is **slow**. To help researchers and practitioners and even beginners the `TensorFlow` team has developed a version for Swift. Although it is not as production ready as the Python variant it certainly has the potential. Swift allows for more low-level interactions and advanced compilation modules. This is [where](https://github.com/tensorflow/swift) you will be able to find everything related to TensorFlow's Swift variant. You are also encouraged to see [this interview](https://www.youtube.com/watch?v=drSpCwDFwnM) of [Jeremy Howard](https://twitter.com/jeremyphoward) conducted by [Josh Gordon](https://www.linkedin.com/in/joshuabendergordon) where Jeremy shares his views on this direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "You have come to an end for this article. The beauty of deep learning lies in its application. Your immediate next step should be try the ideas discussed in this article and incorporate your own ones too. This `TensorFlow` update is easily one of the most comprehensive and developer friendly ones. You are encouraged to check this YouTube watchlist of this year's `TensorFlow` Dev Summit which will provide you with the additional updates on `TensorFlow lattice`, `TensorFlow probability`, `TensorFlow lite` and so on. \n",
    "\n",
    "If you are interested in strengthening your deep learning knowledge, you may check out the following courses: \n",
    "- [Introduction to TensorFlow in Python](https://www.datacamp.com/courses/introduction-to-tensorflow-in-python#!)\n",
    "- [Advanced Deep Learning with Keras in Python](https://www.datacamp.com/courses/advanced-deep-learning-with-keras-in-python)\n",
    "\n",
    "Happy `eager_execut`ing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
