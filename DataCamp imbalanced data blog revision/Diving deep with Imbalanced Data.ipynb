{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Diving deep with Imbalanced Data</center>\n",
    "\n",
    "Learn the techniques to deal with an imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following situation - \n",
    "\n",
    "You are working on your dataset. You create a classification model and get 90% accuracy immediately. The results seem fantastic to you. You dive a little deeper and discover that almost entirety of the data belongs to one class. Damn! Imbalanced data can cause you a lot of frustration.\n",
    "\n",
    "You feel very frustrated when you discovered that your data has imbalanced classes and that all of the great results you thought you were getting turn out to be a lie. What is even more frustrating is the good books don't even cover this topic in a holistic manner.\n",
    "\n",
    "This is an example of a situation cases by an **imbalanced** dataset and the frustrating results it can cause.\n",
    "\n",
    "In this tutorial, you will discover the techniques that you can use to deliver good results on datasets with imbalanced data. Specifically, you will cover:\n",
    "\n",
    "- What is imbalanced data?\n",
    "- Challenges faced with imbalanced datasets\n",
    "- various approaches for handling imbalanced data\n",
    "- Further reading on the topic\n",
    "\n",
    "Let's first see what is imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is imbalanced data?\n",
    "\n",
    "Imbalanced data typically refers to classification tasks where the classes are not represented equally.\n",
    "\n",
    "For example, you may have a binary classification problem with 100 instances out of which 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.\n",
    "\n",
    "This is an example of an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 4:1.\n",
    "\n",
    "You can have a class imbalance problem on binary classification problems as well as multi-class classification problems. You will study the techniques to combat the both. Basically, the techniques that are used to deal with class-imbalance problem in a binary classification problem can easily be extended to that of a multi-class classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image is also a representative of imbalanced data - \n",
    "\n",
    "<img src = \"https://www.kdnuggets.com/wp-content/uploads/imbalanced-data-1.png\"></img>\n",
    "\n",
    "[Source](\"https://www.kdnuggets.com/wp-content/uploads/imbalanced-data-1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be it a Kaggle competition or real test dataset, class imbalance problem is one of the most common ones. \n",
    "\n",
    "Most of the real-world classification problems display some level of class imbalance, which happens when there are not sufficient instances of the data that correspond to either of the class labels. Therefore, it is extremely important to properly choose the evaluation metric of your model. If it is not done then you might end up adjusting/optimizing an useless metric. In a real business-first scenario, this may lead to a complete waste. \n",
    "\n",
    "There are problems where a class imbalance is not just common, it is bound to happen. For example, in datasets that characterize fraudulent transactions are imbalanced. Because almost all of transactions in those datasets will be in the “Not-Fraud” class and a very small minority will be in the “Fraud” class.\n",
    "\n",
    "Another example is customer churn datasets, where the vast majority of customers stay with the service (the “No-Churn” class) and a small minority cancel their subscription (the “Churn” class).\n",
    "\n",
    "Generally, it is considered to be a class imbalance problem when the ratio between distributions of the classes is 4:1 and so on.  \n",
    "\n",
    "Let's now take a look at the challenges faced with imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges faced with imbalanced datasets:\n",
    "\n",
    "One of the main challenges faced by the utility industry today is _electricity theft_. Electricity theft is the third largest form of theft worldwide. The concerned companies are rapidly employing advanced analytics and machine learning algorithms to identify consumption patterns that indicate theft.\n",
    "\n",
    "However, one of the biggest rollicking blocks in the imbalance problem is the enormous data and its varied amounts of distributions. Fraudulent transactions are significantly lower than normal healthy transactions i.e. accounting it to around 1-2 % of the total number of observations. Therefore your goal should be to improve the identification of the rare minority class as opposed to achieving higher overall accuracy.\n",
    "\n",
    "Some examples of imbalanced datasets: \n",
    "- Datasets to identify customer churn where a vast majority of customers will continue using the service. Specifically, Telecommunication companies where Churn Rate is lower than 2 %.\n",
    "- Datasets to identify rare diseases in medical diagnostics etc.\n",
    "- Natural Disaster like Earthquakes\n",
    "\n",
    "Most of the classifiers will fail to detect the imbalance between the classes if they are not adjusted properly or if you do not choose the right metric to evaluate their performance. There are some specific approaches for dealing with these situations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before studying the approaches, let's first learn about **Accuracy Paradox** which is very relevant for this topic.\n",
    "\n",
    "### Accuracy paradox:\n",
    "\n",
    "The accuracy paradox is the name for the exact situation in the introduction to this post.\n",
    "\n",
    "\"_It is the case where your accuracy measures tell the story that you have excellent accuracy (such as 90%), but the accuracy is only reflecting the underlying class distribution._\" - [Machine Learning Mastery](\"http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\")\n",
    "\n",
    "So one obvious question that you will ask to yourself is - \n",
    "\n",
    "**What is going on in your model when you train on an imbalanced dataset?**\n",
    "\n",
    "As you might have guessed, the reason you get 90% accuracy on an imbalanced data (with 90% of the instances in Class-1) is because when your model is fed with this data and trained with conventional approaches it fails to see through this problem and its hypothesis learns to always predict “Class-1” and thereby achieves high _classification accuracy_.\n",
    "\n",
    "This is best seen when using a simple rule based algorithm like _k-NN_. If you print out the rule in the final model you will see that it is very likely predicting one class regardless of the data it is asked to predict.\n",
    "\n",
    "Instead, a fine tuned and more sophisticated method may achieve a lower accuracy, but it would have a substantially higher true positive rate (or recall), which is really the metric you should have chosen for dealing with the dataset.\n",
    "\n",
    "Following sections will now cover some of the approaches for tackling imbalanced datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches for handling imbalanced data:\n",
    "\n",
    "### Evaluation metric: \n",
    "\n",
    "Accuracy is not the only metric to use when you are working with an imbalanced dataset. You already have seen that it is misleading.\n",
    "\n",
    "There are metrics that have been designed to present you a more uncovered story of your model when you trained it on an imbalanced dataset. Let's study them one by one. \n",
    "\n",
    "**Confusion matrix**: It is one of the standard evaluation metrics of a classifier's performance. It contains information about the actual and the predicted class.\n",
    "\n",
    "<img src = \"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/03/16142827/ICP1.png\"></img>\n",
    "\n",
    "[Source](\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/03/16142827/ICP1.png\")\n",
    "\n",
    "The table should be pretty self-explanatory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**: Precision is the number of True Positives divided by the number of True Positives and False Positives. Put another way, it is the number of positive predictions divided by the total number of positive class values predicted. It is also called the Positive Predictive Value (PPV).\n",
    "\n",
    "Precision can be thought of as a measure of a classifier's *exactness*. A low precision can also indicate a large number of False Positives.\n",
    "\n",
    "**Recall**: Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Sensitivity or the True Positive Rate.\n",
    "\n",
    "Recall can be thought of as a measure of a classifier's *completeness*. A low recall indicates many False Negatives.\n",
    "\n",
    "Now, you will go in a bit-more details about these two terms with an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An example illustrating Precision and Recall**:\n",
    "\n",
    "The [breast cancer dataset](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer) is a standard machine learning dataset. It contains 9 attributes describing 286 women that have suffered and survived breast cancer and whether or not breast cancer recurred within 5 years. Let's investigate this dataset to get you a real feel of the problem. \n",
    "\n",
    "The dataset concerns a binary classification problem. Of the 286 women, 201 did not suffer a recurrence of breast cancer, leaving the remaining 85 that did. \n",
    "\n",
    "Let's explore about the dataset more visually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0      1        2      3    4   5  6      7          8  \\\n",
      "0  no-recurrence-events  30-39  premeno  30-34  0-2  no  3   left   left_low   \n",
      "1  no-recurrence-events  40-49  premeno  20-24  0-2  no  2  right   right_up   \n",
      "2  no-recurrence-events  40-49  premeno  20-24  0-2  no  2   left   left_low   \n",
      "3  no-recurrence-events  60-69     ge40  15-19  0-2  no  2  right    left_up   \n",
      "4  no-recurrence-events  40-49  premeno    0-4  0-2  no  2  right  right_low   \n",
      "5  no-recurrence-events  60-69     ge40  15-19  0-2  no  2   left   left_low   \n",
      "6  no-recurrence-events  50-59  premeno  25-29  0-2  no  2   left   left_low   \n",
      "7  no-recurrence-events  60-69     ge40  20-24  0-2  no  1   left   left_low   \n",
      "8  no-recurrence-events  40-49  premeno  50-54  0-2  no  2   left   left_low   \n",
      "9  no-recurrence-events  40-49  premeno  20-24  0-2  no  2  right    left_up   \n",
      "\n",
      "    9  \n",
      "0  no  \n",
      "1  no  \n",
      "2  no  \n",
      "3  no  \n",
      "4  no  \n",
      "5  no  \n",
      "6  no  \n",
      "7  no  \n",
      "8  no  \n",
      "9  no  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "data = pd.read_csv(\"breast-cancer.data\",header=None)\n",
    "\n",
    "# See the data\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names are numeric because you are using a partially preprocessed dataset. But if you are interested then you may refer to the following image:\n",
    "\n",
    "<img src=\"https://image.ibb.co/gxW5Z9/Capture.jpg\"></img>\n",
    "\n",
    "[Source](\"http://archive.ics.uci.edu/ml/datasets/Breast+Cancer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a bar graph of the class distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = data[9].values\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "\n",
    "plt.bar(unique,counts)\n",
    "plt.title('Class Frequency')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see the class imbalance here. `yes` denotes the instances which have the cancer and as obvious the number of these instances is very small as compared to the instances corresponding the other class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Negatives are probably worse than False Positives for this sensitive problem. More detailed screening can clear the False Positives, but False Negatives are sent home and lost to follow-up evaluation. \n",
    "\n",
    "Let's define \"No Recurrences\" and \"Recurrences\" event that are there in the dataset which will make things even more clear. \n",
    "\n",
    "- **All No Recurrence**: A model that only predicted no recurrence of breast cancer would achieve an accuracy of (201/286) * 100 or 70.28%. This is called <b>All No Recurrence</b>. This is a high accuracy, but a terrible model. If this model was misinterpreted, it would send home 85 women incorrectly thinking their breast cancer was not going to reoccur (High False Negatives).\n",
    "\n",
    "- **All recurrence**: A model that only predicted the recurrence of breast cancer would achieve an accuracy of (85/286) * 100 or 29.72%. This is known <b>All Recurrence</b>. This model fails at producing good accuracy and would send home 201 women thinking that had a recurrence of breast cancer but this is really not the case (High False Positives).\n",
    "\n",
    "This concept should spark an ignition inside you by now. Let's move forward with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With respect to the concept of **confusion matrix** a perfect classifier would correctly predict 201 no recurrence and 85 recurrence which would be entered into the top left cell no recurrence/no recurrence (True Negatives) and bottom right cell recurrence/recurrence (True Positives).\n",
    "\n",
    "But most of the time that's not the case. Let's see the two confusion matrices of All No Recurrence and All Recurrence:\n",
    "\n",
    "**All No Recurrence**:\n",
    "<img src = \"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/03/no_recurrence_confusion_matrix.png\">\n",
    "**All Recurrence**:\n",
    "<img src = \"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/03/recurrence_confusion_matrix.png\">\n",
    "\n",
    "[Source](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can calculate the Precision and Recall easily now: \n",
    "\n",
    "**Precision**:\n",
    "- The precision of the All No Recurrence model is 0/(0+0) or not a number, or 0.\n",
    "- The precision of the All Recurrence model is 85/(85+201) or 0.30.\n",
    "\n",
    "**Recall**:\n",
    "- The recall of the All No Recurrence model is 0/(0+85) or 0.\n",
    "- The recall of the All Recurrence model is 85/(85+0) or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well! You have now enough reasons as to wonder why considering only classification accuracy to evaluate your classification  model is not a good choice. \n",
    "\n",
    "Let's proceed to the next approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling the dataset:\n",
    "\n",
    "Dealing with imbalanced datasets includes various strategies such as improving classification algorithms or balancing classes in the training data (essentially a data preprocessing step) before providing the data as input to the machine learning algorithm. The later technique is preferred as it has wider application and adaptation. Moreover, the time taken to enhance an algorithm is often higher than to generate required samples. But for research purposes, both are preferred. \n",
    "\n",
    "The main idea of balancing classes is to either increasing the samples of the minority class or decreasing the samples of the majority class. This is done in order to obtain a fair balance in the number of instances for both the classes. \n",
    "\n",
    "This change is called _sampling_ and there are two main methods that you can use to even-up the classes:\n",
    "\n",
    "- You can add copies of instances from the under-represented class called **over-sampling** (or more formally sampling with replacement), or\n",
    "- You can delete instances from the over-represented class, called **under-sampling**.\n",
    "These approaches are often very easy to implement and fast to run which makes these an excellent starting point.\n",
    "\n",
    "In fact, it is advisable to always try both approaches on all of your imbalanced datasets, just to see if it gives you a boost in your preferred evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn about over-sampling and under-sampling in a bit more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Under-Sampling: \n",
    "\n",
    "Under-sampling aims to balance class distribution by randomly eliminating majority class examples.  This is done until the majority and minority class instances are balanced out. Let's consider the following example: \n",
    "\n",
    "Total Observations = 1000\n",
    "\n",
    "Minority   Observations =20\n",
    "\n",
    "Majority Observations = 980\n",
    "\n",
    "Event Rate= 2% (which means out of a population of 1000 observations, only 20 observations will be minority)\n",
    "\n",
    "In this case, you are taking 10 % samples without replacement from majority and combining them with minority instances.\n",
    "\n",
    "Minority observations after random under-sampling = 10 % of 980 =98\n",
    "\n",
    "Total Observations after combining them with minority observations = 20+98=118\n",
    "\n",
    "Event Rate for the new dataset after under-sampling = 20/118 = 17%\n",
    "\n",
    "\n",
    "\n",
    "**Advantages of this approach**:\n",
    "\n",
    "- It can help improve runtime of the model and solve the memory problems by reducing the number of training data samples when the training data set is huge.\n",
    "\n",
    "**Disadvantages**:\n",
    "- It can discard useful information about the data itself which could be important for building rule-based classifiers such as Random Forests.\n",
    "- The sample chosen by random under sampling may be a biased sample. And it will not be an accurate representative of the population. Thereby, resulting in inaccurate results with the actual test data set.\n",
    " \n",
    "\n",
    "#### Random Over-Sampling:\n",
    "\n",
    "Over-Sampling increases the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample. Let's consider the following example:\n",
    "\n",
    "Total Observations = 1000\n",
    "\n",
    "Minority   Observations =20\n",
    "\n",
    "Majority Observations = 980\n",
    "\n",
    "Event Rate= 2%\n",
    "\n",
    "In this case, you are replicating 20 minority observations   20 times.\n",
    "\n",
    "Majority Fraudulent Observations = 980\n",
    "\n",
    "Minority Observations after replicating the minority class observations= 400\n",
    "\n",
    "Total Observations in the new dataset after over-sampling = 1380\n",
    "\n",
    "New Event rate for the new data set after over-sampling= 400/1380 = 29%\n",
    "\n",
    "**Advantages of random over-sampling**:\n",
    "- Unlike under sampling this method leads to no information loss.\n",
    "\n",
    "**Disadvantages**:\n",
    "- It increases the likelihood of overfitting since it replicates the minority class events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You consider the following factors while thinking of applying under-sampling and over-sampling:\n",
    "- Consider applying under-sampling when you have a lot data\n",
    "- Consider applying over-sampling when you don’t have a lot of data\n",
    "- Consider applying random and non-random (e.g. stratified) sampling schemes.\n",
    "- Consider applying different ratios of the class-labels (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios)\n",
    "\n",
    "If you want to implement under-sampling and over-sampling in Python you should definitely check [`scikit-learn-contrib`](https://github.com/scikit-learn-contrib/imbalanced-learn). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will study the next approach for handling imbalanced data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out different perspectives:\n",
    "\n",
    "There are fields of study dedicated to handling imbalanced datasets. They have their own algorithms, measures and terminology.\n",
    "\n",
    "Often creativity and an innovative bent of mind can give you new perspectives to consider when you are dealing with imbalanced data. Therefore, following points are worth considering:\n",
    ".\n",
    "\n",
    "- **Anomaly detection** is the detection of rare events. In a simpler sense it is the process of identifying the samples that abnormally deviates from the standard deviation of the data. \n",
    "\n",
    "This shift in thinking considers the minor class as the outliers class which might help you think of new ways to separate and classify samples.\n",
    "\n",
    "- **Change detection** is similar to anomaly detection except rather than looking for an anomaly it is looking for a change or difference for example a change in behavior of a user as observed by usage patterns or bank transactions.\n",
    "\n",
    "Both of these approaches add a more real-time sauce to the classification problem that might give you some new ways of thinking about your problem and maybe some more techniques to try out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach can actually help you in playing between different domains and coming up with something new. It's always recommended to dig deeper in this. Anyway, you will now move onto the next and final approach of handling imbalanced data for this post:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try generating synthetic samples:\n",
    "A simple way to generate synthetic samples is to randomly sample the attributes from instances in the minority class.\n",
    "\n",
    "You could sample them empirically within your dataset or you could use a method like _Naive Bayes_ that can sample each attribute independently when run in reverse. You will have more and different data, but the non-linear relationships between the attributes may not be preserved.\n",
    "\n",
    "There are systematic algorithms that you can use to generate synthetic samples. The most popular of such algorithms is called SMOTE or the **Synthetic Minority Over-sampling Technique**. It was proposed in 2002 and you can take a look at the [original SMOTE paper](\"http://www.jair.org/papers/paper953.html\"). Following info-graphic will give you a fair idea about the synthetic samples:\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/max/1600/1*uAiwqUNhqaSZmsXCrl9kVQ.png\"></img>\n",
    "\n",
    "[Source](\"https://cdn-images-1.medium.com/max/1600/1*uAiwqUNhqaSZmsXCrl9kVQ.png\")\n",
    "\n",
    "SMOTE is an oversampling method which create “synthetic” example rather than\n",
    "oversampling by replacements. The minority class is over-sampled by taking each minority class sample and\n",
    "introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbors.\n",
    "Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly\n",
    "chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some advantages and disadvantages of SMOTE - \n",
    "\n",
    "**Advantages** - \n",
    "- Alleviates overfitting caused by random oversampling as synthetic examples are generated rather than replication of instances. \n",
    "- No loss of information.\n",
    "\n",
    "**Disadvantages** - \n",
    "- While generating synthetic examples SMOTE does not take into consideration neighboring examples from other classes. This can result in increase in overlapping of classes and can introduce additional noise.\n",
    "- SMOTE is not very effective for high dimensional data.\n",
    "\n",
    "There are some variants of SMOTE such as safe level SMOTE, border line SMOTE, OSSLDDD-SMOTE etc. If you want to use SMOTE and its other variants you can check the `scikit-learn-contrib` module as mentioned before. If you want to learn more about SMOTE you check [this](sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf) and [this](https://rd.springer.com/chapter/10.1007/978-3-642-01307-2_43) papers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up!\n",
    "\n",
    "So far, you got yourself introduced to the concept of imbalanced data and the kind of problem it creates while designing and developing machine learning models. You also saw several reasons as to why it is important to tackle imbalanced data. After that, you studied four different approaches that can help you to handle imbalanced datasets effectively. Handling imbalanced data is an active area of research and it can open new horizons for you to consider new research problems. \n",
    "\n",
    "A lot of important concepts in one go! Absolutely amazing!\n",
    "\n",
    "That is all for this tutorial. In the next tutorial, you will actually implement some of the approaches in Python with a real-world dataset. \n",
    "\n",
    "Below are some paper links if you are very keen to study even more about the topic of imbalanced data:\n",
    "- [Learning from Imbalanced Data](\"http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5128907\")\n",
    "- [Addressing the Curse of Imbalanced Training Sets: One-Sided Selection](\"http://sci2s.ugr.es/keel/pdf/algorithm/congreso/kubat97addressing.pdf\")\n",
    "- [A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data](\"http://dl.acm.org/citation.cfm?id=1007735\")\n",
    "\n",
    "**References**:\n",
    "- [Analytics Vidhya article on imbalanced data](\"https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/\")\n",
    "- [Towards Data Science article on Imbalanced data](\"https://towardsdatascience.com/dealing-with-imbalanced-classes-in-machine-learning-d43d6fa19d2\")\n",
    "- [Python Machine Learning](\"https://g.co/kgs/a35QhF\")\n",
    "\n",
    "If you want to learn more about data visualization, take DataCamp's [\"Interactive Data Visualization with Bokeh\"](https://www.datacamp.com/courses/interactive-data-visualization-with-bokeh) taught by [Bryan Van de Ven](https://www.datacamp.com/instructors/bryanv) who is one of the developers of Bokeh. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
