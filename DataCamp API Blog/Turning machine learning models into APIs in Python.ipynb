{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Turning machine learning models into APIs in Python</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn to how to create a simple machine learning model API in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a the following situation:\n",
    "\n",
    "You have built a super cool machine learning model that can predict if a particular transaction is fraudulent or not. Now, a friend of yours is building an android application for general banking activities and wants to integrate your machine learning model in his application for its super objective. \n",
    "\n",
    "But he found out that, you have coded your model in Python while he is building his application in Java. So? Won't it be possible to integrate your machine learning model into your friend's application?\n",
    "\n",
    "Fortunately enough, you have the power of *APIs*. And the above situation is one of the many one where the need of turning your machine learning models into APIs is extremely important. Many of the industries are now looking for Data Scientists who can do this. Now, wrapping a machine learning model into an API is not very difficult and that is exactly what you will be doing in this tutorial - **Turn your machine learning model into an API**. \n",
    "\n",
    "Specifically, you will be covering the following: \n",
    "- Options to implement machine learning models\n",
    "- What are APIs?\n",
    "- Flask basics\n",
    "- Creating a machine learning model\n",
    "- Saving the machine learning model: Serialization & Deserialization\n",
    "- Creating an API from a machine learning model using Flask\n",
    "- Test your API in Postman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options to implement Machine Learning models\n",
    "\n",
    "Most of the times, the real use of your machine learning model lies at the heart of a product – that maybe a small component of a recommender system or a chat-bot. These are the times when the barriers seem very difficult to overcome.\n",
    "\n",
    "For example, majority of the ML practitioner use R/Python for their experiments. But consumer of those ML models would be software engineers who use a completely different stack. There are two ways via which this problem can be solved:\n",
    "\n",
    "- Option 1: Rewriting the whole code in the language that the software engineering folks work. The above seems like a good idea, but the time & energy required to get those intricate models replicated would be utterly waste. Majority of languages like JavaScript, do not have great libraries to perform ML. One would be wise to stay away from it.\n",
    "- Option 2: API-first approach – Web APIs have made it easy for cross-language applications to work well. If a frontend developer needs to use your ML Model to create a ML powered web application, they would just need to get the URL Endpoint from where the API is being served.\n",
    "\n",
    "Now, before going any further let's study what really is an API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are APIs?\n",
    "\n",
    "In simple words, an API is a (hypothetical) contract between 2 softwares saying if the user software provides input in a pre-defined format, the later with extend its functionality and provide the outcome to the user software.\n",
    "\n",
    "You can read this article to understand why APIs are a popular choice amongst developers:\n",
    "- [History of APIs](http://apievangelist.com/2012/12/20/history-of-apis/)\n",
    "- [Introduction to APIs](https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-apis-application-programming-interfaces-5-apis-a-data-scientist-must-know/)\n",
    "\n",
    "In a simpler sense, APIs are very much like web applications but instead of giving you a nicely styled HTML page, APIs tend to return data in a standard data-exchange format such as JSON, XML etc. There are many popular ML APIs as well for example - *IBM Watson's* ML API which is capable of the following: \n",
    "- Machine Translation - Helps translate text in different language pairs.\n",
    "- Message Resonance – To find out the popularity of a phrase or word with predetermined audience.\n",
    "- Question and Answers - This service provides direct answers to the queries that are triggered by primary document sources.\n",
    "- User Modelling – To make predictions about social characteristics of someone from given text.\n",
    "\n",
    "[Google Vision API](\"https://cloud.google.com/vision/\") is also a very good example which provides dedicated services for Computer Vision tasks. [Click here](\"https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/python\") to get an idea of what can be done using Google Vision API.\n",
    "\n",
    "Basically what happens is majority of the cloud providers and smaller machine learning focused companies provide ready-to-use APIs. They cater to the needs of developers / businesses that do not have expertise in ML, who want to implement ML in their processes or product suites.\n",
    "\n",
    "Now that you have a fair idea of APIs are, let's see how you can wrap a machine learning model (developed in Python) into an API in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask - A web-services' framework in Python:\n",
    "\n",
    "Now, you might think what is a web-service. Web-service is a form of API only that assumes that an API is hosted over a server and can be consumed. Web API, Web Service  - these terms are generally used interchangeably. \n",
    "\n",
    "Coming to Flask, it is web-service development framework in Python. Now, it is not the only one in Python, there couple others as well such as Django, Falcon, Hug etc. But you will use Flask for this tutorial. For learning about Flask, you can refer [these tutorials](\"https://www.tutorialspoint.com/flask\"). \n",
    "\n",
    "If you downloaded the Anaconda distribution, you already have Flask installed, otherwise, you will have to install it yourself with – ```pip install flask```.\n",
    "\n",
    "Flask is very minimal since you only bring in the parts as you need them. To demonstrate this, here’s a small code snippet that uses Flask to create a very simple web server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Welcome to machine learning model APIs!\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once executed, you can navigate to the web address, which is shown the terminal, and observe the expected result.\n",
    "\n",
    "![alt](https://image.ibb.co/jxG4Re/Capture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some points: \n",
    "\n",
    "- **Jupyter Notebooks are great for anything related to markdowns, R and Python. But when it comes to building a web server, it may behave inconsistent sometimes. So, it is a good idea to write the Flask codes in a text editor like *Sublime* and run the Python code from terminal**. \n",
    "\n",
    "- Make sure you don't name the file as _flask.py_.\n",
    "\n",
    "- Flask runs on port number 5000 by default. Sometimes, the Flask server starts on this port number successfully but when you hit the URL (that the servers returns on the terminal) in a web-browser or any API-client like *Postman*, you may not get the output. Consider the following situation: \n",
    "\n",
    "<img src = \"https://image.ibb.co/iF5hBe/Capture.jpg\"></img>\n",
    "\n",
    "- According to Flask, its server has started successfully on port 5000, but when the URL was fired in the browser, it didn't return anything. So, this can be a possible case of port number conflict. In this case, changing the default port 5000 to your desired port number would be a good choice. You can do that just by doing the following:\n",
    "\n",
    "   ```app.run(debug=True,port=12345)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In that case, the Flask server would look something like the following: \n",
    "\n",
    "<img src = \"https://image.ibb.co/cpkhBe/Capture.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go through step by step of the code that you wrote:\n",
    "\n",
    "After importing, you created an instance of the Flask class and pass in the \"__name__\" variable that Python fills in for us. This variable will be \"__main__\", if this file is being directly run through Python as a script. If you import the file instead, the value of \"__name__\" will be the name of the file where you did the import. For instance, if you had test.py and run.py, and you imported test.py into run.py the \"__name__\" value of test.py will be test.\n",
    "\n",
    "Above you `hello()` method definition, there’s the `@app.route(\"/\")` line. The @ denotes a [decorator](\"https://jeffknupp.com/blog/2013/11/29/improve-your-python-decorators-explained/\"), which allows the function, property, or class it’s precedes to be dynamically altered.\n",
    "\n",
    "The `hello()` method is where you put the code to run whenever the route of your app or API hits the top level route: /.\n",
    "\n",
    "If your \"__name__\" variable is \"__main__\", indicating that you ran the file directly instead of importing it, then it will start the Flask app which will run and wait for web requests until the process ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now study some of the factors that you will need to keep in mind if you are turning your machine learning models (built using scikit-learn) into a Flask API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn models with Flask\n",
    "\n",
    "Creating very simple to very complex machine learning models have never been this easy in Python with scikit-learn. But there are some points you will have to remember about scikit-learn:\n",
    "\n",
    "- Scikit-learn is an intuitive and powerful Python machine learning library that makes training and validating many models fairly easy. Scikit-learn models can be persisted (pickled) to avoid retraining the model every time they are used. You can use Flask to create an API that can provide predictions based on a set of input variables using a pickled model.\n",
    "<br><br>\n",
    "- Before you get into merging scikit-learn models into Flask it’s important to point out that scikit-learn does not handle categorical variables and missing values. Categorical variables need to be encoded as numeric values. Typically categorical variables are transformed using OneHotEncoder (OHE) or LabelEncoder. LabelEncoder assigns an integer to each categorical value and transforms the original variable to a new variable with corresponding integers replaced for categorical variables. The problem with this approach is that a nominal variable is effectively transformed to an ordinal variable which may fool a model into thinking that the order is meaningful. OHE, on the other hand, does not suffer from this issue, however it tends to explode the number of transformed variables since a new variable is created for every value of a categorical variables.\n",
    "<br><br>\n",
    "- One thing to know about LabelEncoder is that the transformation will change based on the number of categorical values in a variable. Let’s say you have a “subscription” variable with “gold” and “platinum” values. LabelEncoder will map these to 0 and 1 respectively. Now if you add the value “free” to the mix the assignment is changed (free is encoded as 0, gold to 1, and platinum to 2). For this reason it’s important to keep your original LabelEncoder around for transformation at the prediction time.\n",
    "<br><br>\n",
    "- For handling missing values, scikit-learn provides ```sklearn.preprocessing```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding and missing values are important data preprocessing steps which are very essential for building a good machine learning model. If you want to learn more on this, be sure to check the following course offered by DataCamp:\n",
    "\n",
    "- [Preprocessing for Machine Learning in Python](\"https://www.datacamp.com/courses/preprocessing-for-machine-learning-in-python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, you will keep things simple. You will use the Titanic dataset which is one of most popular datasets for many reasons such as - the dataset contain a well mixture different types of variables, the dataset contains missing values etc. This [DataCamp tutorial](\"https://www.datacamp.com/community/tutorials/k-means-clustering-python\") covers a good analysis of the dataset and the dataset can be downloaded from [here](\"https://www.kaggle.com/c/titanic/data\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify things even further, you will only use four variables: ```age```, ```sex```, ```embarked```, and ```survived``` where `survived` is the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset in a dataframe object and include only four features as mentioned\n",
    "url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "df = pd.read_csv(url)\n",
    "include = ['Age', 'Sex', 'Embarked', 'Survived'] # Only four features\n",
    "df_ = df[include]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Sex\" and \"Embarked\" are categorical variables and need to be transformed. “Age” has missing values which is typically imputed, meaning it’s replaced by a summary statistic such as median or mean. Missing values can be quite meaningful and it’s worth investigating what they represent in real-world applications. \n",
    "\n",
    "Here, you will simply to replace NaNs with 0 and you will write a helper function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = []\n",
    "for col, col_type in df_.dtypes.iteritems():\n",
    "     if col_type == 'O':\n",
    "          categoricals.append(col)\n",
    "     else:\n",
    "          df_[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function will iterate over all columns in the dataframe and append categorical variables (with data type “O”) to the categoricals list. For non-categorical variables (integers and floats), which is only \"Age\" in this case, you are replacing NaNs with zeros. **Filling NaNs with a single value may have unintended consequences, especially if the value that you’re replacing NaNs with is within the observed range for the numeric variable. Since zero is not an observed and legitimate age value you are not introducing bias, you would have if you used say 36!**\n",
    "\n",
    "Now you’re ready to OHE our categorical variables. Pandas provides a simple method ```get_dummies()``` for creating OHE variables for a given dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ohe = pd.get_dummies(df_, columns=categoricals, dummy_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about OHE is that it’s deterministic. A new column is created for every column/value combination, in the following column_value format. For example for the “Embarked” variable you’re going to get “Embarked_C”, “Embarked_Q”, “Embarked_S”, and “Embarked_nan”.\n",
    "\n",
    "Now that you’ve successfully transformed your dataset you’re ready to train the machine learning model. You will use a _Logistic Regression classifier_ for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "dependent_variable = 'Survived'\n",
    "x = df_ohe[df_ohe.columns.difference([dependent_variable])]\n",
    "y = df_ohe[dependent_variable]\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have built your machine learning model. You will now save this model. Technically speaking, you will serialize this model. In Python, you call this \"Pickling\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model: Serialization and Deserialization\n",
    "\n",
    "You will use sklearn’s ```joblib``` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(lr, 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s it! You have persisted our model. You can load this model into memory in a single line. Loading the model back into your workspace, is known as Deserialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re now ready to use Flask to serve your persisted model. You have already seen how minimalistic Flask is to get started with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an API from a machine learning model using Flask\n",
    "\n",
    "For serving your model with Flask, you briefly need two things: \n",
    "- Load your persisted model into memory when the application starts, \n",
    "- Create an endpoint that takes input variables, transforms them into the appropriate format, and returns predictions.\n",
    "\n",
    "More specifically, your sample input to the API will look like the following:\n",
    "\n",
    "`[\n",
    "    {\"Age\": 85, \"Sex\": \"male\", \"Embarked\": \"S\"},\n",
    "    {\"Age\": 24, \"Sex\": '\"female\"', \"Embarked\": \"C\"},\n",
    "    {\"Age\": 3, \"Sex\": \"male\", \"Embarked\": \"C\"},\n",
    "    {\"Age\": 21, \"Sex\": \"male\", \"Embarked\": \"S\"}\n",
    "]`\n",
    "\n",
    "and your API will output the following:\n",
    "\n",
    "`{\"prediction\": [0, 1, 1, 0]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify\n",
    "app = Flask(__name__)\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "     json_ = request.json\n",
    "     query_df = pd.DataFrame(json_)\n",
    "     query = pd.get_dummies(query_df)\n",
    "     prediction = lr.predict(query)\n",
    "     return jsonify({'prediction': list(prediction)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would only work under ideal circumstances where the incoming request contains all possible values for the categorical variables. If that’s not the case, ```get_dummies()``` would generate a dataframe that has less columns than the classifier excepts, which would result in a runtime error. Also numerical variables need to be replaced using the same methodology that you trained the model with.\n",
    "\n",
    "A solution to the less than expected number of columns is to persist the list of columns from training. Remember that Python objects (including lists and dictionaries) can be pickled. To do this you will use joblib, as you did previously, to dump the list of columns into a pkl file.\n",
    "\n",
    "Keep that in mind, as discussed earlier it is always better to do all the server level coding in a text editor and then run it from a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_columns.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_columns = list(x.columns)\n",
    "joblib.dump(model_columns, 'model_columns.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you have this list persisted you can just replace the missing values with zeros at the time of prediction. Also you have to load model columns when the application starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if lr:\n",
    "        try:\n",
    "            json_ = request.json\n",
    "            query = pd.get_dummies(pd.DataFrame(json_))\n",
    "            query = query.reindex(columns=model_columns, fill_value=0)\n",
    "\n",
    "            prediction = list(lr.predict(query))\n",
    "\n",
    "            return jsonify({'prediction': prediction})\n",
    "\n",
    "        except:\n",
    "\n",
    "            return jsonify({'trace': traceback.format_exc()})\n",
    "    else:\n",
    "        print ('Train the model first')\n",
    "        return ('No model here to use')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You included all the required elements in the \"/predict\" API and now, you just need to write the main class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        port = int(sys.argv[1]) # This is for a command-line argument\n",
    "    except:\n",
    "        port = 12345 # If you don't provide any port then the port will be set to 12345\n",
    "    lr = joblib.load(model_file_name) # Load \"model.pkl\"\n",
    "    print ('Model loaded')\n",
    "    model_columns = joblib.load(model_columns_file_name) # Load \"model_columns.pkl\"\n",
    "    print ('Model columns loaded')\n",
    "    app.run(port=port, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your API now ready to be hosted. But before going any further, let's recap all that you did till this point: \n",
    "\n",
    "### Putting it altogether:\n",
    "\n",
    "- You loaded Titanic dataset and selected the four features. \n",
    "- You did the necessary data preprocessing.\n",
    "- You built a Logistic Regression classifier and serialized it. \n",
    "- You also serialized all the columns from training as a solution to the less than expected number of columns is to persist the list of columns from training.\n",
    "- You then wrote a simple API using Flask that would predict if a person was survived in the shipwreck given his age, sex and embarked information. \n",
    "\n",
    "Let's put all thew code in one place so that you don't miss out on anything. Also it is a good programming practice if you separate your Logistic Regression model code and your Flask API code into separate `.py` files. \n",
    "\n",
    "So your `model.py` should look like following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset in a dataframe object and include only four features as mentioned\n",
    "url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "df = pd.read_csv(url)\n",
    "include = ['Age', 'Sex', 'Embarked', 'Survived'] # Only four features\n",
    "df_ = df[include]\n",
    "\n",
    "# Data Preprocessing \n",
    "categoricals = []\n",
    "for col, col_type in df_.dtypes.iteritems():\n",
    "     if col_type == 'O':\n",
    "          categoricals.append(col)\n",
    "     else:\n",
    "          df_[col].fillna(0, inplace=True)\n",
    "\n",
    "df_ohe = pd.get_dummies(df_, columns=categoricals, dummy_na=True)\n",
    "\n",
    "# Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "dependent_variable = 'Survived'\n",
    "x = df_ohe[df_ohe.columns.difference([dependent_variable])]\n",
    "y = df_ohe[dependent_variable]\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x, y)\n",
    "\n",
    "# Save your model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lr, 'model.pkl')\n",
    "print(\"Model dumped!\")\n",
    "\n",
    "# Load the model that you just saved\n",
    "lr = joblib.load('model.pkl')\n",
    "\n",
    "# Saving the data columns from training\n",
    "model_columns = list(x.columns)\n",
    "joblib.dump(model_columns, 'model_columns.pkl')\n",
    "print(\"Models columns dumped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `api.py` should look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from flask import Flask, request, jsonify\n",
    "from sklearn.externals import joblib\n",
    "import traceback \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Your API definition\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if lr:\n",
    "        try:\n",
    "            json_ = request.json\n",
    "            print(json_)\n",
    "            query = pd.get_dummies(pd.DataFrame(json_))\n",
    "            query = query.reindex(columns=model_columns, fill_value=0)\n",
    "\n",
    "            prediction = list(lr.predict(query))\n",
    "\n",
    "            return jsonify({'prediction': str(prediction)})\n",
    "\n",
    "        except:\n",
    "\n",
    "            return jsonify({'trace': traceback.format_exc()})\n",
    "    else:\n",
    "        print ('Train the model first')\n",
    "        return ('No model here to use')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        port = int(sys.argv[1]) # This is for a command-line input\n",
    "    except:\n",
    "        port = 12345 # If you don't provide any port the port will be set to 12345\n",
    "\n",
    "    lr = joblib.load(\"model.pkl\") # Load \"model.pkl\"\n",
    "    print ('Model loaded')\n",
    "    model_columns = joblib.load(\"model_columns.pkl\") # Load \"model_columns.pkl\"\n",
    "    print ('Model columns loaded')\n",
    "\n",
    "    app.run(port=port, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty neat! Now you will test this API in an API client called [Postman](\"https://www.getpostman.com/\"). Just make sure that `model.py` and `api.py` are in the same directory and also make sure that you have compiled them both before testing. Refer to the following snapshot of the terminal which is taken after both the `.py` files were compiled successfully.\n",
    "\n",
    "<img src = \"https://image.ibb.co/jgGo2K/Capture.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your API in Postman\n",
    "\n",
    "In order to test your API, you will some kind of API client. Postman is undoubtedly one of the best ones out there. You can easily download Postman from the above link. \n",
    "\n",
    "The Postman interface looks like the following if you downloaded the latest one:\n",
    "<br><img src = \"https://image.ibb.co/hy566e/Capture.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have started the Flask server successfully, just enter the right URL with the right port number in Postman. It should look similar to the following: \n",
    "\n",
    "<img src = \"https://image.ibb.co/eCBKbe/Capture.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You just built your first ever machine learning API. \n",
    "\n",
    "Your API can predict if a passenger survived the titanic shipwreck given his `age`, `sex` and `embarked` information. Now, your friend may call it from his front-end code and process the output of the API into something very exciting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking it further:\n",
    "\n",
    "In this tutorial, you covered one of the most vital industry demanding skills of a full-stack Data Scientist i.e. building an API from a machine learning model. Although the API is very simple, but it is always better to start with simplest of the things so that you know the know-hows in details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a lot more in order to improve this. Possible options you might want to consider: \n",
    "\n",
    "- Write a \"/train\" API which would train a Logistic Regression classifier with the data. \n",
    "- Code a Neural Network model using `keras` and build an API out of it. \n",
    "- Host your API on Cliud so that it can be consumed.\n",
    "- For taking things to more advanced levels, you might refer to [this Machine Learning Mastery blog](\"https://machinelearningmastery.com/deploy-machine-learning-model-to-production/\") which discusses several industry graded approaches. \n",
    "\n",
    "The possibilities and opportunities are huge here. You just need to carefully select the ones which are the most suitable for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: \n",
    "\n",
    "Following are some references that were taken while writing this blog: \n",
    "\n",
    "- [Flask: Building Python Web Services](\"Flask: Building Python Web Services\")\n",
    "- [A Quora thread on the topic ](\"https://www.quora.com/How-do-I-deploy-Machine-Learning-Models-as-an-API\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
