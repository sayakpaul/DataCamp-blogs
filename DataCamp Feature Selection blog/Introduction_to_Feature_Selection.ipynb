{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Beginner's introduction to Feature Selection in Python</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You all have seen datasets. Sometimes they are small but often at times they are tremendously large in size. It becomes very difficult to process the datasets which are very large, at least large to cause a processing bottleneck. \n",
    "\n",
    "So, what makes these datasets this large? Well, its features. The more the number of features the larger the datasets will be. Well, not always. You will find datasets where the number of features is very much but they do not contain that much instances. But that is not the point of discussion here. So, you might wonder with a commodity computer in hand how to process these type of datasets without beating the bush. \n",
    "\n",
    "Often, in a high dimensional dataset, there remain some absolutely irrelevant, insignificant and unimportant features. It has been seen that, the contribution of these types of features is often very very less towards predictive modeling as compared to the important features. They may have zero contribution as well. These features cause a number of problems which in turn prevents the process of efficient predictive modeling - \n",
    "\n",
    "- Unnecessary resource allocation for these features\n",
    "- These features act as a noise for which the machine learning model can perform terribly bad\n",
    "- The machine model takes more time to get trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what's the solution here? The most economic solution is <b>Feature Selection</b>. \n",
    "\n",
    "Feature Selection is the process of selecting out the most significant features from a given dataset. In many of the cases, Feature Selection can enhance the performance of a machine learning model as well. \n",
    "\n",
    "Sounds interesting right? \n",
    "\n",
    "You got an informal introduction to Feature Selection and its importance in the world of Data Science and Machine Learning. This post will cover: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduction to feature selection and understanding its importance\n",
    "- Difference between feature selection and dimensionality reduction\n",
    "- Different types of feature selection methods\n",
    "- Implementation of different feature selection methods with <b>scikit-learn</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is also known as <b>Variable selection</b> or <b>Attribute selection</b>.\n",
    "\n",
    "Essentially, it is the process of selecting the most important/relevant. features of a dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the importance of feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of feature selection can best be understood when you are dealing a dataset that contains a huge number of features. This type of dataset is often referred to as a _high dimensional_ dataset. Now, with this high dimensionality, comes a lot of problems such as - this high dimensionality will significantly increase the training time of your machine learning model, it can make your model very complex which in turn may lead to Overfitting. \n",
    "\n",
    "Often in a high dimensional feature set, there remains several features which are redundant meaning these features are nothing but extensions of the other important features. These redundant features do not effectively contribute to the model training as well. So, clearly there is need to extract the most important and the most relevant features for a dataset in order to get the most effective predictive modeling performance. \n",
    "\n",
    "<i>\"The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data.\"</i>\n",
    "\n",
    "-<a href = \"http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf\">An Introduction to Variable and Feature Selection</a>\n",
    "\n",
    "Now let's understand the difference between <b><i>dimensionality reduction</i></b> and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, feature selection mistaken with dimensionality reduction. But they are different. Feature selection is different from dimensionality reduction. Both methods tend to reduce the number of attributes in the dataset, but a dimensionality reduction method do so by creating new combinations of attributes (sometimes known as feature transformation), where as feature selection methods include and exclude attributes present in the data without changing them.\n",
    "\n",
    "Some examples of dimensionality reduction methods are Principal Component Analysis, Singular Value Decomposition, Linear Discriminant Analysis etc.\n",
    "\n",
    "Let me summarize the importance of feature selection for you:\n",
    "- It enables the machine learning algorithm to train faster.\n",
    "- It reduces the complexity of a model and makes it easier to interpret.\n",
    "- It improves the accuracy of a model if the right subset is chosen.\n",
    "- It reduces Overfitting.\n",
    "\n",
    "In the next section, you will study about the different types of general feature selection methods - Filter methods, Wrapper methods and Embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image best describes filter-based feature selection methods:\n",
    "\n",
    "\n",
    "<img src = \"https://www.analyticsvidhya.com/wp-content/uploads/2016/11/Filter_1.png\"></img>\n",
    "\n",
    "\n",
    "<b>Image Source: Analytics Vidhya</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter method relies on general uniqueness of the data to be evaluated and pick feature subset not including any mining\n",
    "algorithm. Filter method uses the exact assessment criterion which includes distance, information, dependency and consistency. The filter method uses the principal criteria of ranking technique and uses rank ordering method for variable selection. The reason for using ranking method is simplicity, produce good and relevant features. Ranking method will filter out irrelevant features before classification process starts. \n",
    "\n",
    "Filter methods are generally used as a data preprocessing step. The selection of features is independent of any machine learning algorithm. Features given rank on the basis of statistical scores which tend to determine the features' correlation with the outcome variable. Correlation is a heavily contextual term and it varies from work to work. You can refer to the following table for defining correlation co-efficients for different types of data (in this case continuous and categorical).\n",
    "\n",
    "<img src = \"https://www.analyticsvidhya.com/wp-content/uploads/2016/11/FS1.png\"></img>\n",
    "\n",
    "<b>Image Source: Analytics Vidhya</b>\n",
    "\n",
    "Some examples of some filter methods include the <b><i>Chi squared test</i></b>, <b><i>information gain</i></b> and <b><i>correlation coefficient scores</i></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will see Wrapper methods.\n",
    "\n",
    "## Wrapper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like filter methods, let me give you a same kind of info-graphic which will help you to understand wrapper methods better:\n",
    "\n",
    "<img src = \"https://www.analyticsvidhya.com/wp-content/uploads/2016/11/Wrapper_1.png\"></img>\n",
    "\n",
    "<b>Image Source: Analytics Vidhya</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above image, a wrapper method needs one machine learning algorithm and uses its performance as evaluation criteria. This method searches for feature which is best-suited for the machine learning algorithm and aims to improve the mining performance. To evaluate the features, the predictive accuracy used for classification tasks and goodness of cluster is evaluated using clustering. \n",
    "\n",
    "Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n",
    "\n",
    "- <b>Forward Selection</b>: The procedure starts with an empty set of features [reduced set]. The best of the original features is determined and added to the reduced set. At each subsequent iteration, the best of the remaining original\n",
    "attributes is added to the set. <br><br>\n",
    "- <b>Backward Elimination</b>: The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set. <br><br>\n",
    "- **Combination of forward selection and backward elimination**: The stepwise forward selection and backward elimination methods can be combined so that, at each step, the procedure selects the best attribute and removes the worst from among the\n",
    "remaining attributes. <br><br>\n",
    "- <b>Recursive Feature elimination</b>: Recursive feature elimination performs a greedy search to find the best performing feature subset. It iteratively creates models and determines the best or the worst performing feature at each iteration. It constructs the subsequent models with the left features until all the features are explored. It then ranks the features based on the order of their elimination. In worst case, if a dataset contains N number of features RFE will do greedy search for 2<sup>N</sup> combinations of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good enough!\n",
    "\n",
    "Now let's study embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods are iterative in a sense that take care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold. \n",
    "\n",
    "This is why Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).\n",
    "\n",
    "Examples of regularization algorithms are the <b>LASSO</b>, <b>Elastic Net</b>, <b>Ridge Regression</b> etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between filter and wrapper methods\n",
    "\n",
    "Well it might get confusing at times to differentiate between filter methods and wrapper methods in terms their functionalities. Let's take a look at what points they differ from each other. \n",
    "\n",
    "- Filter methods do not incorporate a machine learning model in order to determine if a feature is good or bad where as wrapper methods actually use a machine learning model and train it the feature to decide if it is important or not. \n",
    "- Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive and in case of very large datasets wrapper methods are not the most effective feature selection method to consider.\n",
    "- Filter methods may fail to find the best subset of features in situations when there is not enough data to model the statistical correlation of the features but wrapper methods can always provide the best subset of features because of their exhaustive nature.\n",
    "- Using features from wrapper methods in your final machine learning model can lead to overfitting as wrapper methods already train machine learning models with the features and it affects the true power of _learning_. But the features from filter methods will not lead to overfitting in most of the cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far you have studied the importance of feature selection, understood its difference with dimensionality reduction. You also covered various types of feature selection methods. So far, so good!\n",
    "\n",
    "Now, let's see some traps that you may get into while  performing feature selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important consideration\n",
    "\n",
    "You may have already understood the worth of feature selection in a machine learning pipeline and the kind of services it provides if integrated. But it is very important to understand at exactly where you should integrate feature selection in your machine learning pipeline. \n",
    "\n",
    "Simply speaking, you should include the feature selection step before feeding the data to the model for training especially when you are using accuracy estimation methods such as _cross-validation_. This ensures that feature selection is performed on the data fold right before the model is trained. But if you perform feature selection first to prepare your data, then perform model selection and training on the selected features then it would be a blunder.\n",
    "\n",
    "If you perform feature selection on all of the data and then cross-validate, then the test data in each fold of the cross-validation procedure was also used to choose the features and this tends to bias the performance of your machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough of theories! let's get straight to some coding now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study in Python\n",
    "\n",
    "For this case study, you will use Pima Indians Diabetes dataset. The description of the dataset can be found <a href = \"https://www.kaggle.com/uciml/pima-indians-diabetes-database\">here</a>.\n",
    "\n",
    "The dataset corresponds to a classification tasks on which you need to predict if a person has diabetes based on 8 features.\n",
    "\n",
    "There are a total of 768 observations in the dataset. Your first task is to load the dataset so that you can proceed. But before that let's import the basic dependencies, you are going to need. You can import the other ones as you go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dependencies are imported let's load Pima Indians dataset into a Dataframe object with the help of Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is successfully loaded into the Dataframe object <i>data</i>. Now, let's take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can 8 different features labeled into the outcomes of 1 and 0 where 1 stands for the observation has diabetes, and 0 denotes the observation does not have diabetes. The dataset is known to have missing values. Specifically, there are missing observations for some columns that are marked as a zero value. You can deduce this by the definition of those columns, and it is impractical to have a zero value is invalid for those measures, e.g., zero for body mass index or blood pressure is invalid.\n",
    "\n",
    "But for this tutorial, you will directly use the preprocessed version of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You loaded the data in a DataFrame object called <i>dataframe</i> now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the DataFrame object to a NumPy array to achieve faster computation. Also, let's segregate the data into separate variables so that the features and the labels are separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! You have prepared your data. \n",
    "\n",
    "First you will implement <b><i>Chi-Squared</i></b> statistical test for non-negative features to select 4 of the best features from the dataset. You have already seen Chi-Squared test belongs the class of filter methods. If anyone's curious about knowing the internals of Chi-Squared, <a href = \"https://www.youtube.com/watch?v=VskmMgXmkMQ\">this video</a> does an excellent job. \n",
    "\n",
    "The scikit-learn library provides the `SelectKBest` class that can be used with a suite of different statistical tests to select a specific number of features, in this case it is Chi-Squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries first\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You imported the libraries to run the experiments. Now, let's see it in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# Summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "\n",
    "features = fit.transform(X)\n",
    "# Summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: \n",
    "You can see the scores for each attribute and the 4 attributes chosen (those with the highest scores): plas, test, mass and age. This scores will help you further in choosing the best features for training your model.\n",
    "\n",
    "<b>P.S.: The first row denotes the names of the features. For preprocessing of the dataset, the names have been numerically encoded.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will implement <b><i>Recursive Feature Elimination</i></b> which is a type of wrapper feature selection method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n",
    "\n",
    "It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n",
    "\n",
    "You can learn more about the RFE class in the <a href = \"http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\">scikit-learn documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import your necessary dependencies\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use RFE with the logistic regression algorithm to select the top 3 features. The choice of algorithm does not matter too much as long as it is skillful and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 3 5 6 1 1 4]\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %s\" % (fit.n_features_))\n",
    "print(\"Selected Features: %s\" % (fit.support_))\n",
    "print(\"Feature Ranking: %s\" % (fit.ranking_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that RFE chose the the top 3 features as `preg`, `mass` and `pedi`.\n",
    "\n",
    "These are marked True in the <i>support_</i> array and marked with a choice “1” in the <i>ranking_</i> array. This in turns indicates the strength of these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up you will use <b><i>Ridge regression</i></b> which is basically a regularization technique and an embedded feature selection techniques as well. \n",
    "\n",
    "<a href=\"https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/#three\">This article</a> gives you an excellent explanation on Ridge regression. Be sure to check it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First things first\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will use Ridge regression to determine the coefficient R<sup>2</sup>.\n",
    "\n",
    "Also, <a href = \"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">check scikit-learn's official documentation on Ridge regression</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better understand the results of Ridge regression you will implement a little helper function that will help you to print the results in a better so that you can interpret them easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A helper method for pretty-printing the coefficients\n",
    "def pretty_print_coefs(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will pass Ridge model's coefficient terms to this little function and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge model: 0.021 * X0 + 0.006 * X1 + -0.002 * X2 + 0.0 * X3 + -0.0 * X4 + 0.013 * X5 + 0.145 * X6 + 0.003 * X7\n"
     ]
    }
   ],
   "source": [
    "print (\"Ridge model:\", pretty_print_coefs(ridge.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can spot all the coefficient terms appended with the feature variables. It will again help you to choose the most important features. Below are some points that you should keep in mind while applying Ridge regression:\n",
    "\n",
    "- It is also known as <b>L2-Regularization</b>.\n",
    "- For correlated features, it means that they tend to get similar coefficients. \n",
    "- Feature having negative coefficients don't contribute that much. But in a more complex scenario where you are dealing with lots of features, then this score will definitely help you in the ultimate feature selection decision making process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that concludes the case study section. The methods that you implemented in the above section will definitely help you to understand the features of a particular dataset in a comprehensive manner. Let me give you some important points on these techniques: \n",
    "\n",
    "- Feature selection is essentially a part of data preprocessing which is considered to be the most time-consuming part of any machine learning pipeline. \n",
    "- These techniques will help you to approach it in a more systematic way and machine learning friendly way. You will be able to interpret the features more accurately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, you covered one of the most well studied and well researched statistical topic i.e. feature selection. You also got familiar with its different variants and used them to see which features in a dataset are important. \n",
    "\n",
    "Feature selection is very well-studied and well-researched area. You can take this tutorial further by merging a correlation measure into wrapper method and see how it performs. In the course of action, you might end up creating your own feature selection mechanism. Researchers are also using _soft computing_ principles in order to perform selection. This is itself a whole field of study and research.  \n",
    "\n",
    "Following are some resources if you would like to dig more on this topic:\n",
    "\n",
    "- <a href = \"http://machinelearningmastery.com/feature-selection-to-improve-accuracy-and-decrease-training-time/\">\n",
    "Feature Selection to Improve Accuracy and Decrease Training Time</a>\n",
    "- <a href = \"http://www.amazon.com/dp/079238198X?tag=inspiredalgor-20\">Feature Selection for Knowledge Discovery and Data Mining</a>\n",
    "- <a href = \"http://www.amazon.com/dp/3540341374?tag=inspiredalgor-20\">Subspace, Latent Structure and Feature Selection: Statistical and Optimization Perspectives Workshop</a>\n",
    "- <a href = \"https://www.youtube.com/watch?v=y2Jsa4sgD5w&t=1073s\">Feature Selection : Problem statement and Uses</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the references that were used in order to write this tutorial.\n",
    "\n",
    "- <a href = \"https://www.elsevier.com/books/data-mining-concepts-and-techniques/han/978-0-12-381479-1\">Data Mining: Concepts and Techniques; Jiawei Han Micheline Kamber Jian Pei</a>.\n",
    "- <a href = \"http://machinelearningmastery.com/an-introduction-to-feature-selection/\">An introduction to feature selection</a>\n",
    "- [Analytics Vidhya article on feature selection](\"https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/\")\n",
    "- <a href = \"https://www.datacamp.com/courses/hierarchical-and-mixed-effects-models\n",
    "\">Hierarchical and Mixed Model - DataCamp course</a>\n",
    "- S. Visalakshi and V. Radha, \"A literature review of feature selection techniques and applications: Review of feature selection in data mining,\" 2014 IEEE International Conference on Computational Intelligence and Computing Research, Coimbatore, 2014, pp. 1-6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to post your doubts in the comments section if you have any!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
