{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Introduction to Feature Selection</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You all have seen datasets. Sometimes they are small but often at times they are tremendously large in size. It becomes very difficult to process the datasets which are very large, at least large to cause a processing bottleneck. \n",
    "\n",
    "So, what makes these datasets this large? Well, its features. The more the number of features the larger the datasets will be. Well, not always. You will find datasets where the number of features is very much but they do not contain that much instances. But that is not the point of discussion here. So, you might wonder with a commodity computer in hand how to process these type of datasets without beating the bush. \n",
    "\n",
    "Often, in a high dimensional dataset, there remain some absolutely irrelevant, insignificant and unimportant features. It has been seen that, the contribution of these types of features is often very very less towards predictive modeling as compared to the important features. They may have zero contribution as well. These features cause a number of problems which in turn prevents the process of efficient predictive modeling - \n",
    "\n",
    "- Unnecessary resource allocation for these features\n",
    "- These features act as a noise for which the machine learning model can perform terribly bad\n",
    "- The machine model takes more time to get trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what's the solution here? The most economic solution is <b>Feature Selection</b>. \n",
    "\n",
    "Feature Selection is the process of selecting out the most significant features from a given dataset. In many of the cases, Feature Selection can enhance the performance of a machine learning model as well. \n",
    "\n",
    "Sounds interesting right? \n",
    "\n",
    "You got an informal introduction to Feature Selection and its importance in the world of Data Science and Machine Learning. This post will cover: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduction to feature selection and understanding its importance\n",
    "- Difference between feature selection and dimensionality reduction\n",
    "- Different types of feature selection methods\n",
    "- Implementation of different feature selection methods with <b>scikit-learn</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is also known as <b>Variable selection</b> or <b>Attribute selection</b>.\n",
    "\n",
    "Essentially, it is the automatic selection of attributes in your data (such as columns in tabular data) that are most relevant to the predictive modeling problem you are working on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the importance of feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection methods aid you in your mission to create an accurate predictive model. They help you by choosing features that will give you as good or better accuracy whilst requiring less data.\n",
    "\n",
    "Feature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model.\n",
    "\n",
    "Fewer attributes is desirable because it reduces the complexity of the model, and a simpler model is simpler to understand and explain.\n",
    "\n",
    "<i>\"The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data.\"</i>\n",
    "\n",
    "-<a href = \"http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf\">An Introduction to Variable and Feature Selection</a>\n",
    "\n",
    "Now let's understand the difference between <b><i>dimensionality reduction</i></b> and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is different from dimensionality reduction. Both methods seek to reduce the number of attributes in the dataset, but a dimensionality reduction method do so by creating new combinations of attributes, where as feature selection methods include and exclude attributes present in the data without changing them.\n",
    "\n",
    "Examples of dimensionality reduction methods include Principal Component Analysis, Singular Value Decomposition and Sammon’s Mapping.\n",
    "\n",
    "Let me summarize the importance of feature selection for you:\n",
    "- It enables the machine learning algorithm to train faster.\n",
    "- It reduces the complexity of a model and makes it easier to interpret.\n",
    "- It improves the accuracy of a model if the right subset is chosen.\n",
    "- It reduces Overfitting.\n",
    "\n",
    "In the next section, you will study about the different types of general feature selection methods - Filter methods, Wrapper methods and Embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image best describes filter-based feature selection methods:\n",
    "\n",
    "\n",
    "![FilterMethods](\"https://www.analyticsvidhya.com/wp-content/uploads/2016/11/Filter_1.png\")\n",
    "\n",
    "\n",
    "<b>Image Source: Analytics Vidhya</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter feature selection methods apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset. The methods are often univariate and consider the feature independently, or with regard to the dependent variable.\n",
    "\n",
    "Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here. You can refer to the following table for defining correlation co-efficients for different types of data (in this case continuous and categorical).\n",
    "\n",
    "![DifferentCorrelationCoefficient](\"https://www.analyticsvidhya.com/wp-content/uploads/2016/11/FS1.png\")\n",
    "\n",
    "<b>Image Source: Analytics Vidhya</b>\n",
    "\n",
    "Some examples of some filter methods include the <b><i>Chi squared test</i></b>, <b><i>information gain</i></b> and <b><i>correlation coefficient scores</i></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will see Wrapper methods.\n",
    "\n",
    "## Wrapper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like filter methods, let me give you a same kind of info-graphic which will help you to understand wrapper methods better:\n",
    "\n",
    "![WrapperMethod](\"https://www.analyticsvidhya.com/wp-content/uploads/2016/11/Wrapper_1.png\")\n",
    "\n",
    "<b>Image Source: Analytics Vidhya</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper methods consider the selection of a set of features as a search problem, where different combinations are prepared, evaluated and compared to other combinations. A predictive model us used to evaluate a combination of features and assign a score based on model accuracy.\n",
    "\n",
    "The search process may be methodical such as a best-first search, it may stochastic such as a random hill-climbing algorithm, or it may use heuristics, like forward and backward passes to add and remove features.\n",
    "\n",
    "Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n",
    "\n",
    "- <b>Forward Selection</b>: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "- <b>Backward Elimination</b>: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "- <b>Recursive Feature elimination</b>: It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best ways for implementing feature selection with wrapper methods is to use <a href =\"http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/\"><b>Boruta</b></a> package that finds the importance of a feature by creating shadow features.\n",
    "\n",
    "It works in the following steps:\n",
    "\n",
    "- Firstly, it adds randomness to the given data set by creating shuffled copies of all features (which are called shadow features).\n",
    "- Then, it trains a random forest classifier on the extended data set and applies a feature importance measure (the default is Mean Decrease Accuracy) to evaluate the importance of each feature where higher means more important.\n",
    "- At every iteration, it checks whether a real feature has a higher importance than the best of its shadow features (i.e. whether the feature has a higher Z-score than the maximum Z-score of its shadow features) and constantly removes features which are deemed highly unimportant.\n",
    "- Finally, the algorithm stops either when all features get confirmed or rejected or it reaches a specified limit of random forest runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good enough!\n",
    "\n",
    "Now let's study embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods learn which features best contribute to the accuracy of the model while the model is being created. The most common type of embedded feature selection methods are regularization methods.\n",
    "\n",
    "Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).\n",
    "\n",
    "Examples of regularization algorithms are the <b>LASSO</b>, <b>Elastic Net</b>, <b>Ridge Regression</b> etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between filter and wrapper methods\n",
    "\n",
    "Allow me to give some more points on filter and wrapper methods in a comparing manner, so that you don't miss out on anything.\n",
    "\n",
    "\n",
    "- Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "- Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive as well.\n",
    "- Filter methods use statistical methods for evaluation of a subset of features while wrapper methods use <b>cross validation</b>.\n",
    "- Filter methods might fail to find the best subset of features in many occasions but wrapper methods can always provide the best subset of features.\n",
    "- Using the subset of features from the wrapper methods make the model more prone to Overfitting as compared to using subset of features from the filter methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far you have studied the importance of feature selection, understood its difference with dimensionality reduction. You also covered various types of feature selection methods. So far, so good!\n",
    "\n",
    "Now, let's see some traps that you may get into while selecting features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important consideration\n",
    "\n",
    "Feature selection is another key part of the applied machine learning process, like model selection. You cannot fire and forget.\n",
    "\n",
    "It is important to consider feature selection a part of the model selection process. If you do not, you may inadvertently introduce bias into your models which can result in Overfitting.\n",
    "\n",
    "For example, you must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features.\n",
    "\n",
    "If you perform feature selection on all of the data and then cross-validate, then the test data in each fold of the cross-validation procedure was also used to choose the features and this is what biases the performance analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough of theories! let's get straight to some coding now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study in Python\n",
    "\n",
    "For this case study, you will use Pima Indians Diabetes dataset. The description of the dataset can be found <a href = \"https://www.kaggle.com/uciml/pima-indians-diabetes-database\">here</a>.\n",
    "\n",
    "The dataset corresponds to a classification problem on which you need to make predictions on the basis of whether a person is to suffer diabetes given the 8 features in the dataset.\n",
    "\n",
    "There are a total of 768 observations in the dataset. Your first task is to load the dataset so that you can proceed. But before that let's import the basic dependencies, you are going to need. You import the other ones as you go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dependencies are imported let's load Pima Indians dataset into a Dataframe object with the help of Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is successfully loaded into the Dataframe object <i>data</i>. Now, let's take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can 8 different features labeled into the outcomes of 1 and 0 where 1 stands for the observation has diabetes, and 0 denotes the observation does not have diabetes. The dataset is known to have missing values. Specifically, there are missing observations for some columns that are marked as a zero value. We can corroborate this by the definition of those columns, and the domain knowledge that a zero value is invalid for those measures, e.g., zero for body mass index or blood pressure is invalid.\n",
    "\n",
    "This <a href = \"https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models\">DataCamp article</a> discusses about handling the dataset's missing values in detail. You should definitely refer to it. But for this tutorial, you will directly use the preprocessed version of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You loaded the data in a DataFrame object called <i>dataframe</i> now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the DataFrame object to a NumPy array to ease the computations. Also, let's split the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have prepared the data. \n",
    "\n",
    "First you will implement <b><i>Chi-Squared</i></b> statistical test for non-negative features to select 4 of the best features from the dataset. As you saw earlier Chi-Squared test belongs the class of filter methods. If anyone's curious about knowing the internals of Chi-Squared, <a href = \"https://www.youtube.com/watch?v=VskmMgXmkMQ\">this video</a> does an excellent job. \n",
    "\n",
    "The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features, in this case it is Chi-Squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries first\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You imported the libraries to run the experiments. Now, let's see it in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# Summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "\n",
    "features = fit.transform(X)\n",
    "# Summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the scores for each attribute and the 4 attributes chosen (those with the highest scores): plas, test, mass and age. This scores will help you further in choosing the best features for training your model.\n",
    "\n",
    "<b>P.S.: The first row denotes the names of the features. For preprocessing of the dataset, the names have been numerically encoded.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will implement <b><i>Recursive Feature Elimination</i></b> which is a type of wrapper feature selection method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n",
    "\n",
    "It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n",
    "\n",
    "You can learn more about the RFE class in the <a href = \"http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\">scikit-learn documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import your necessary dependencies\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use RFE with the logistic regression algorithm to select the top 3 features. The choice of algorithm does not matter too much as long as it is skillful and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 3 5 6 1 1 4]\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %s\" % (fit.n_features_))\n",
    "print(\"Selected Features: %s\" % (fit.support_))\n",
    "print(\"Feature Ranking: %s\" % (fit.ranking_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that RFE chose the the top 3 features as preg, mass and pedi.\n",
    "\n",
    "These are marked True in the <i>support_</i> array and marked with a choice “1” in the <i>ranking_</i> array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up you will use <b><i>Ridge regression</i></b> which is basically a regularization technique and an embedded feature selection techniques as well. \n",
    "\n",
    "<a href=\"https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/#three\">This article</a> gives you an excellent explanation on Ridge regression. Be sure to check it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First things first\n",
    "\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will use Ridge regression to determine the coefficient R<sup>2</sup>.\n",
    "\n",
    "Also, <a href = \"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">check scikit-learn's official documentation on Ridge regression</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better understand the results of Ridge regression you will implement a little helper function that will help you to print the results in a better so that you can interpret them easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A helper method for pretty-printing the coefficients\n",
    "\n",
    "def pretty_print_coefs(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will pass Ridge model's coefficient terms to this little function and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge model: 0.021 * X0 + 0.006 * X1 + -0.002 * X2 + 0.0 * X3 + -0.0 * X4 + 0.013 * X5 + 0.145 * X6 + 0.003 * X7\n"
     ]
    }
   ],
   "source": [
    "print (\"Ridge model:\", pretty_print_coefs(ridge.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can spot all the coefficient terms appended with the feature variables. It will again help you to choose the most important features. Below are some points that you should keep in mind while applying Ridge regression:\n",
    "\n",
    "- It is also known as <b>L2-Regularization</b>.\n",
    "- For correlated features, it means that they tend to get similar coefficients. \n",
    "- Feature having negative coefficients don't contribute that much. But in a more complex scenario where you are dealing with lots of features, then this score will definitely help you in the ultimate feature selection decision making process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that concludes the case study section. The methods that you implemented in the above section will definitely hep you to understand the features of a particular dataset in a comprehensive manner. Let me give you some important points on these techniques: \n",
    "\n",
    "- Feature selection is essentially a part of data preprocessing which is considered to be the most time-consuming part of any machine learning pipeline. \n",
    "- These techniques will help you to approach it in a more systematic way and machine learning friendly way. You will be able to interpret the features more accurately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, you covered one of the most well studied and well researched statistical topic i.e. feature selection. You also got familiar with its different variant and used them to see which features in a dataset are important. \n",
    "\n",
    "Brilliant! \n",
    "\n",
    "Following are some resources if you would like to dig more on this topic:\n",
    "\n",
    "- <a href = \"http://machinelearningmastery.com/an-introduction-to-feature-selection/\">An introduction to feature selection</a>\n",
    "- <a href = \"http://machinelearningmastery.com/feature-selection-to-improve-accuracy-and-decrease-training-time/\">\n",
    "Feature Selection to Improve Accuracy and Decrease Training Time</a>\n",
    "- <a href = \"http://www.amazon.com/dp/079238198X?tag=inspiredalgor-20\">Feature Selection for Knowledge Discovery and Data Mining</a>\n",
    "- <a href = \"http://www.amazon.com/dp/3540341374?tag=inspiredalgor-20\">Subspace, Latent Structure and Feature Selection: Statistical and Optimization Perspectives Workshop</a>\n",
    "- <a href = \"https://www.youtube.com/watch?v=y2Jsa4sgD5w&t=1073s\">Feature Selection : Problem statement and Uses</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the references that I used in order to write this tutorial.\n",
    "\n",
    "- <a href = \"https://www.elsevier.com/books/data-mining-concepts-and-techniques/han/978-0-12-381479-1\">Data Mining: Concepts and Techniques; Jiawei Han Micheline Kamber Jian Pei</a>.\n",
    "- <a href = \"https://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/\">Selecting good features – Part II</a>\n",
    "- <a href = \"https://www.datacamp.com/courses/hierarchical-and-mixed-effects-models\n",
    "\">Hierarchical and Mixed Model - DataCamp course</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to post your doubts in the comments section if you have any!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
