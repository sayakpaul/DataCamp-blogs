{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Demystifying crucial statistics in Python </center>\n",
    "\n",
    "### - Know about the basic statistics required for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need any prerequisite knowledge of statistics before you can start learning and applying machine learning. \n",
    "\n",
    "However, knowing some statistics can be very helpful to understand machine learning technically as well intuitively. Knowing some statistics will eventually be required when you want to start validating your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics is a field of mathematics with lots of theories and findings. However, there are various nuts and bolts, tools, techniques and notations are taken from this field in order to make machine learning what it is today. You can use descriptive statistical methods to transform raw observations into information that you can understand and share. You can use inferential statistical methods to reason from small samples of data to whole domains. Later in this post, you will study about descriptive and inferential statistics. So, don't worry.  \n",
    "\n",
    "Before getting started, let's walk through ten examples where statistical methods are used in an applied machine learning project:\n",
    "\n",
    "- **Problem Framing**: Requires the use of exploratory data analysis and data mining.\n",
    "- __Data Understanding__: Requires the use of summary statistics and data visualization.\n",
    "- **Data Cleaning**: Requires the use of outlier detection, imputation and more.\n",
    "- **Data Selection**: Requires the use of data sampling and feature selection methods.\n",
    "- **Data Preparation**: Requires the use of data transforms, scaling, encoding and much more.\n",
    "- **Model Evaluation**: Requires experimental design and resampling methods.\n",
    "- **Model Configuration**: Requires the use of statistical hypothesis tests and estimation statistics.\n",
    "- **Model Selection**: Requires the use of statistical hypothesis tests and estimation statistics.\n",
    "- **Model Presentation**: Requires the use of estimation statistics such as confidence intervals.\n",
    "- **Model Predictions**: Requires the use of estimation statistics such as prediction intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't that fascinating? \n",
    "\n",
    "This post will give you a solid background in the essential but basic statistics required for becoming a good machine learning practitioner. \n",
    "\n",
    "In this post, you will study:\n",
    "\n",
    "- Introduction to Statistics and its types\n",
    "- Statistics for data preparation\n",
    "- Statistics for model evaluation\n",
    "- Introduction to Gaussian and Descriptive stats\n",
    "- Variable correlation\n",
    "- Non-parametric Statistics\n",
    "\n",
    "You have a lot to cover and all of the topics are equally important. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Statistics and its types:\n",
    "\n",
    "Let's briefly study how to define statistics in simple terms. \n",
    "\n",
    "Statistics is a subfield of mathematics. It refers to a collection of methods for working with data and using data to answer questions.\n",
    "\n",
    "When it comes to the statistical tools that are used in practice, it can be helpful to divide the field of statistics into two large groups of methods: descriptive statistics for summarizing data, and inferential statistics for drawing conclusions from samples of data.\n",
    "\n",
    "- __Descriptive Statistics__: Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures. Together with simple graphics analysis, they form the basis of virtually every quantitative analysis of data. The below infographic provides a good summary of descriptive statistics:\n",
    "\n",
    "<img src = \"https://i2.wp.com/intellspot.com/wp-content/uploads/2017/11/descriptive-statistic-spreadsheet-and-pie-chart.png?resize=720%2C437\"></img>\n",
    "\n",
    "_**Source: IntellSpot**_\n",
    "\n",
    "- __Inferential Statistics__: Inferential statistics is a fancy name for methods that aid in quantifying properties of the domain or population from a smaller set of obtained observations called a sample. Below is an inforgraphic which beautifully describes inferential statistics: \n",
    "\n",
    "    <img src = \"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/20150849/what-is-inferential-statistics.jpg\">\n",
    "    \n",
    "_**Source: Analytics Vidhya**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, you will study about the use of statistics for data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics for data preparation:\n",
    "\n",
    "Statistical methods are required in the preparation of train and test data for your machine learning model.\n",
    "\n",
    "This includes techniques for:\n",
    "\n",
    "- Outlier detection\n",
    "- Missing value imputation\n",
    "- Data sampling\n",
    "- Data scaling\n",
    "- Variable encoding\n",
    "\n",
    "A basic understanding of data distributions, descriptive statistics, and data visualization is required to help you identify the methods to choose when performing these tasks.\n",
    "\n",
    "Let's analyze each of the above points briefly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection:\n",
    "\n",
    "Let's first see what is an outlier. \n",
    "\n",
    "An outlier is an observation that appears to deviate markedly from other observations in the sample. The following figure makes the definition more prominent. \n",
    "\n",
    "<img src = \"https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/34795/versions/7/screenshot.png\">\n",
    "\n",
    "_**Source: MathWorks**_\n",
    "\n",
    "You can spot the outliers in the data as given the above figure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning algorithms are sensitive to the range and distribution of attribute values in the input data. Outliers in input data can skew and mislead the training process of machine learning algorithms resulting in longer training times, less accurate models and ultimately poorer results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identification of potential outliers is important for the following reasons:**\n",
    "\n",
    "- An outlier may indicate bad data. For example, the data may have been coded incorrectly or an experiment may not have been run correctly. If it can be determined that an outlying point is in fact erroneous, then the outlying value should be deleted from the analysis (or corrected if possible).\n",
    "\n",
    "- In some cases, it may not be possible to determine if an outlying point is bad data. Outliers may be due to random variation or may indicate something scientifically interesting. In any event, you typically do not want to simply delete the outlying observation. However, if the data contains significant outliers, you may need to consider the use of robust statistical techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, outliers are often not good for your predictive models (Although, sometimes, these outliers can be used as an advantage. But that is out of the scope of this post). You need statistical know-how  in order to handle outliers efficiently "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation:\n",
    "\n",
    "Well, most of the datasets now suffer from the problem of missing values. Your machine learning model may not get trained effectively if the data that you are feeding to the model contains missing values. Statistical tools and techniques come here for rescue.\n",
    "\n",
    "Many people tend to discard the data instances which contain missing value. But that is not a good practice because during that course you may lose important features/representations of the data. Although there are advanced methods for dealing with missing value problems these are the quick  techniques that one would go for: **Mean Imputation** and **Median Imputation**. \n",
    "\n",
    "It is very important that you understand what mean and median are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, you have a feature **X1** which has these values - 13, 18, 13, 14, 13, 16, 14, 21, 13\n",
    "\n",
    "The **mean** is the usual average, so I'll add and then divide:\n",
    "\n",
    "(13 + 18 + 13 + 14 + 13 + 16 + 14 + 21 + 13) / 9 = 15\n",
    "\n",
    "Note that the mean, in this case, isn't a value from the original list. This is a common result. You should not assume that your mean will be one of your original numbers.\n",
    "\n",
    "The **median** is the middle value, so first you will have to rewrite the list in numerical order:\n",
    "\n",
    "13, 13, 13, 13, 14, 14, 16, 18, 21\n",
    "\n",
    "There are nine numbers in the list, so the middle one will be the (9 + 1) / 2 = 10 / 2 = 5th number:\n",
    "\n",
    "13, 13, 13, 13, 14, 14, 16, 18, 21\n",
    "\n",
    "So the median is 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling:\n",
    "\n",
    "Data is the currency of applied machine learning. Therefore, it is important that it is both collected and used effectively.\n",
    "\n",
    "Data sampling refers to statistical methods for selecting observations from the domain with the objective of estimating a population parameter. In other words, sampling is an active process of gathering observations with the intent of estimating a population variable.\n",
    "\n",
    "Each row of a dataset represents an observation about something in the world. When working with data, you often do not have access to all possible observations.This could be for many reasons; for example:\n",
    "\n",
    "- It may difficult or expensive to make more observations.\n",
    "- It may be challenging to gather all observations together.\n",
    "- More observations are expected to be made in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many a times, you will not have the right proportion of the data samples. So, you will have to under-sample or over-sample based on the type of problem. \n",
    "\n",
    "You perform under-sampling when the data samples for a particular category are very higher compared to other meaning you discard some of the data samples from the category where they are higher. You perform over-sampling when the data samples for a particular category are very lower compared to the other. In this case you generate data samples. \n",
    "\n",
    "This applies to multi-class scenarios as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical sampling is a large field of study, but in applied machine learning, there may be three types of sampling that you are likely to use: simple random sampling, systematic sampling, and stratified sampling.\n",
    "\n",
    "- **Simple Random Sampling**: Samples are drawn with a uniform probability from the domain.\n",
    "- **Systematic Sampling**: Samples are drawn using a pre-specified pattern, such as at intervals.\n",
    "- **Stratified Sampling**: Samples are drawn within pre-specified categories (i.e. strata).\n",
    "\n",
    "Although these are the more common types of sampling that you may encounter, there are other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling: \n",
    "\n",
    "Often, the features of your dataset may largely vary in ranges. Some features may have a range of 0 to 100 while the other may have ranges of 0.01 - 0.001, 10000- 20000 etc. \n",
    "\n",
    "This is very problematic for efficient modeling. Because a small change in the feature which has lower value range than the other feature may not have a significant impact on those other features. It affects the process of good learning. Dealing with this problem is known as **data scaling**. \n",
    "\n",
    "There are different data scaling techniques such as Min-Max scaling, Absolute scaling, Standard scaling etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable encoding:\n",
    "\n",
    "At times, your datasets contain a mixture of both numeric and non-numeric data. And in order to speed up the computation process, you may have to supply all of the data in numeric format. \n",
    "\n",
    "Again, statistics comes for saving you. \n",
    "\n",
    "Techniques like Label encoding, One-Hot encoding etc. are used to convert non-numeric data to numeric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's time to apply the techniques!\n",
    "\n",
    "You have covered a lot theory for now. You will apply some of these to get the real feel. \n",
    "\n",
    "You will start off by applying some statistical methods to detect **Outliers**.\n",
    "\n",
    "But before you start, let's define a dataset which you can use to test the methods.\n",
    "\n",
    "You will generate a population 10,000 random numbers drawn from a Gaussian distribution (_Gaussian distributions are discussed later_) with a mean of 50 and a standard deviation (_also, mean and standard deviation_) of 5.\n",
    "\n",
    "Numbers drawn from a Gaussian distribution will have outliers. That is, by virtue of the distribution itself, there will be a few values that will be a long way from the mean, rare values that you can identify as outliers.\n",
    "\n",
    "You will use the randn() function of NumPy to generate random Gaussian values with a mean of 0 and a standard deviation of 1, then multiply the results by your own standard deviation and add the mean to shift the values into the preferred range.\n",
    "\n",
    "The pseudorandom number generator is seeded to ensure that you get the same sample of numbers each time you run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "# Seed the random number generator\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=50.049 stdv=4.994\n"
     ]
    }
   ],
   "source": [
    "# Generate univariate observations\n",
    "data = 5 * randn(10000) + 50\n",
    "\n",
    "# Smmarize\n",
    "print('mean=%.3f stdv=%.3f' % (mean(data), std(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know that the distribution of values in the sample is Gaussian or Gaussian-like, you can use the standard deviation of the sample as a cut-off for identifying outliers.\n",
    "\n",
    "The Gaussian distribution has the property that the standard deviation from the mean can be used to reliably summarize the percentage of values in the sample.\n",
    "\n",
    "For example, within one standard deviation of the mean will cover 68% of the data.\n",
    "\n",
    "So, if the mean is 50 and the standard deviation is 5, as in the test dataset above, then all data in the sample between 45 and 55 will account for about 68% of the data sample. You can cover more of the data sample if you expand the range as follows:\n",
    "\n",
    "1 Standard Deviation from the Mean: 68%\n",
    "2 Standard Deviations from the Mean: 95%\n",
    "3 Standard Deviations from the Mean: 99.7%\n",
    "\n",
    "A value that falls outside of 3 standard deviations is part of the distribution, but it is an unlikely or rare event at approximately 1 in 370 samples.\n",
    "\n",
    "Three standard deviations from the mean is a common cut-off in practice for identifying outliers in a Gaussian or Gaussian-like distribution. For smaller samples of data, perhaps a value of 2 standard deviations (95%) can be used, and for larger samples, perhaps a value of 4 standard deviations (99.9%) can be used.\n",
    "\n",
    "Sometimes, the data is standardized first (e.g. to a Z-score with zero mean and unit variance) so that the outlier detection can be performed using standard Z-score cut-off values. This is convenient and is not required in general, and you will perform the calculations in the original scale of the data here to make things clear.\n",
    "\n",
    "You can calculate the mean and standard deviation of a given sample, then calculate the cut-off for identifying outliers as more than 3 standard deviations from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified outliers: 29\n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics\n",
    "data_mean, data_std = mean(data), std(data)\n",
    "\n",
    "# Identify outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "\n",
    "# Summarize\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "print('Identified outliers: %d' % len(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can remove these outliers as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-outlier observations: 9971\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers\n",
    "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
    "print('Non-outlier observations: %d' % len(outliers_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You saw how you can use standard deviation and set its threshold in order to detect potential outliers in the data and how you can filter them out. Next you will see how to do some **missing value imputation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the famous [**Pima Indian Diabetes**](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv) dataset which is known to have missing values. But before proceeding any further, you will have to load the dataset into your workspace. \n",
    "\n",
    "You will load the dataset into a DataFrame object **data**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                6         148          72          35           0        33.6  \\\n",
      "count  767.000000  767.000000  767.000000  767.000000  767.000000  767.000000   \n",
      "mean     3.842243  120.859192   69.101695   20.517601   79.903520   31.990482   \n",
      "std      3.370877   31.978468   19.368155   15.954059  115.283105    7.889091   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   32.000000   32.000000   \n",
      "75%      6.000000  140.000000   80.000000   32.000000  127.500000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "            0.627          50           1  \n",
      "count  767.000000  767.000000  767.000000  \n",
      "mean     0.471674   33.219035    0.348110  \n",
      "std      0.331497   11.752296    0.476682  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243500   24.000000    0.000000  \n",
      "50%      0.371000   29.000000    0.000000  \n",
      "75%      0.625000   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are columns that have a minimum value of zero (0). On some columns, a value of zero does not make sense and indicates an invalid or missing value.\n",
    "\n",
    "Specifically, the following columns have an invalid zero minimum value:\n",
    "\n",
    "- Plasma glucose concentration\n",
    "- Diastolic blood pressure\n",
    "- Triceps skinfold thickness\n",
    "- 2-Hour serum insulin\n",
    "- Body mass index\n",
    "\n",
    "Let's confirm this my looking at the raw data, the example prints the first 20 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>148</th>\n",
       "      <th>72</th>\n",
       "      <th>35</th>\n",
       "      <th>0</th>\n",
       "      <th>33.6</th>\n",
       "      <th>0.627</th>\n",
       "      <th>50</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>88</td>\n",
       "      <td>41</td>\n",
       "      <td>235</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     6  148  72  35    0  33.6  0.627  50  1\n",
       "0    1   85  66  29    0  26.6  0.351  31  0\n",
       "1    8  183  64   0    0  23.3  0.672  32  1\n",
       "2    1   89  66  23   94  28.1  0.167  21  0\n",
       "3    0  137  40  35  168  43.1  2.288  33  1\n",
       "4    5  116  74   0    0  25.6  0.201  30  0\n",
       "5    3   78  50  32   88  31.0  0.248  26  1\n",
       "6   10  115   0   0    0  35.3  0.134  29  0\n",
       "7    2  197  70  45  543  30.5  0.158  53  1\n",
       "8    8  125  96   0    0   0.0  0.232  54  1\n",
       "9    4  110  92   0    0  37.6  0.191  30  0\n",
       "10  10  168  74   0    0  38.0  0.537  34  1\n",
       "11  10  139  80   0    0  27.1  1.441  57  0\n",
       "12   1  189  60  23  846  30.1  0.398  59  1\n",
       "13   5  166  72  19  175  25.8  0.587  51  1\n",
       "14   7  100   0   0    0  30.0  0.484  32  1\n",
       "15   0  118  84  47  230  45.8  0.551  31  1\n",
       "16   7  107  74   0    0  29.6  0.254  31  1\n",
       "17   1  103  30  38   83  43.3  0.183  33  0\n",
       "18   1  115  70  30   96  34.6  0.529  32  1\n",
       "19   3  126  88  41  235  39.3  0.704  27  0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see 0 values in the columns 2, 3, 4, and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a count of the number of missing values on each of these columns. You can do this my marking all of the values in the subset of the DataFrame you are interested in that have zero values as True. You can then count the number of true values in each column.\n",
    "\n",
    "You can do this my marking all of the values in the subset of the DataFrame you are interested in that have zero values as True. you can then count the number of true values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148       5\n",
      "72       35\n",
      "35      227\n",
      "0       373\n",
      "33.6     11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print((data[[1,2,3,4,5]] == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that columns 1,2 and 5 have just a few zero values, whereas columns 3 and 4 show a lot more, nearly half of the rows.\n",
    "\n",
    "This highlights that different \"missing value\" strategies may be needed for different columns, e.g. to ensure that there are still a sufficient number of records left to train a machine learning model.\n",
    "\n",
    "In Python, specifically Pandas, NumPy and Scikit-Learn, you mark missing values as NaN.\n",
    "\n",
    "Values with a NaN value are ignored from operations like sum, count, etc.\n",
    "\n",
    "You can mark values as NaN easily with the Pandas DataFrame by using the replace() function on a subset of the columns you are interested in.\n",
    "\n",
    "After you have marked the missing values, you can use the isnull() function to mark all of the NaN values in the dataset as True and get a count of the missing values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6          0\n",
      "148        0\n",
      "72         0\n",
      "35         0\n",
      "0          0\n",
      "33.6       0\n",
      "0.627      0\n",
      "50         0\n",
      "1          0\n",
      "1          5\n",
      "2         35\n",
      "3        227\n",
      "4        373\n",
      "5         11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mark zero values as missing or NaN\n",
    "import numpy\n",
    "data[[1,2,3,4,5]] = data[[1,2,3,4,5]].replace(0, numpy.NaN)\n",
    "\n",
    "# Count the number of NaN values in each column\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the number of missing values in each column. You can see that the columns 1-5 have the same number of missing values as zero values identified above. This is a sign that you have marked the identified missing values correctly.\n",
    "\n",
    "You can see that the columns 1 to 5 have the same number of missing values as zero values identified above. This is a sign that you have marked the identified missing values correctly.\n",
    "\n",
    "Let's take a look at the full dataset now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>148</th>\n",
       "      <th>72</th>\n",
       "      <th>35</th>\n",
       "      <th>0</th>\n",
       "      <th>33.6</th>\n",
       "      <th>0.627</th>\n",
       "      <th>50</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>197.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>30.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>125.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>168.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>189.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>30.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>166.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>25.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>118.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>45.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>107.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>43.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>34.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>88</td>\n",
       "      <td>41</td>\n",
       "      <td>235</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>39.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     6  148  72  35    0  33.6  0.627  50  1      1     2     3      4     5\n",
       "0    1   85  66  29    0  26.6  0.351  31  0   85.0  66.0  29.0    NaN  26.6\n",
       "1    8  183  64   0    0  23.3  0.672  32  1  183.0  64.0   NaN    NaN  23.3\n",
       "2    1   89  66  23   94  28.1  0.167  21  0   89.0  66.0  23.0   94.0  28.1\n",
       "3    0  137  40  35  168  43.1  2.288  33  1  137.0  40.0  35.0  168.0  43.1\n",
       "4    5  116  74   0    0  25.6  0.201  30  0  116.0  74.0   NaN    NaN  25.6\n",
       "5    3   78  50  32   88  31.0  0.248  26  1   78.0  50.0  32.0   88.0  31.0\n",
       "6   10  115   0   0    0  35.3  0.134  29  0  115.0   NaN   NaN    NaN  35.3\n",
       "7    2  197  70  45  543  30.5  0.158  53  1  197.0  70.0  45.0  543.0  30.5\n",
       "8    8  125  96   0    0   0.0  0.232  54  1  125.0  96.0   NaN    NaN   NaN\n",
       "9    4  110  92   0    0  37.6  0.191  30  0  110.0  92.0   NaN    NaN  37.6\n",
       "10  10  168  74   0    0  38.0  0.537  34  1  168.0  74.0   NaN    NaN  38.0\n",
       "11  10  139  80   0    0  27.1  1.441  57  0  139.0  80.0   NaN    NaN  27.1\n",
       "12   1  189  60  23  846  30.1  0.398  59  1  189.0  60.0  23.0  846.0  30.1\n",
       "13   5  166  72  19  175  25.8  0.587  51  1  166.0  72.0  19.0  175.0  25.8\n",
       "14   7  100   0   0    0  30.0  0.484  32  1  100.0   NaN   NaN    NaN  30.0\n",
       "15   0  118  84  47  230  45.8  0.551  31  1  118.0  84.0  47.0  230.0  45.8\n",
       "16   7  107  74   0    0  29.6  0.254  31  1  107.0  74.0   NaN    NaN  29.6\n",
       "17   1  103  30  38   83  43.3  0.183  33  0  103.0  30.0  38.0   83.0  43.3\n",
       "18   1  115  70  30   96  34.6  0.529  32  1  115.0  70.0  30.0   96.0  34.6\n",
       "19   3  126  88  41  235  39.3  0.704  27  0  126.0  88.0  41.0  235.0  39.3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that marking the missing values had the intended effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up till now, you analyzed important trends when data is missing and how you can make use of simple statistical measures in order to get a hold of it. Now, you will impute the missing values using **Mean Imputation** which is essentially imputing the mean of the respective column in place of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6        0\n",
      "148      0\n",
      "72       0\n",
      "35       0\n",
      "0        0\n",
      "33.6     0\n",
      "0.627    0\n",
      "50       0\n",
      "1        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "5        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values with mean column values\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Count the number of NaN values in each column\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [DataCamp article](https://www.datacamp.com/community/tutorials/preprocessing-in-data-science-part-1-centering-scaling-and-knn) effectively guides you about implementing **data scaling** as a data preprocessing step. Be sure to check it out. \n",
    "\n",
    "Next you will do **variable encoding**.\n",
    "\n",
    "Before that you need a dataset which actually contains non-numeric data. You will use the famous [Iris dataset](http://archive.ics.uci.edu/ml/datasets/Iris) for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset to a DataFrame object iris\n",
    "iris = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3            4\n",
       "0   5.1  3.5  1.4  0.2  Iris-setosa\n",
       "1   4.9  3.0  1.4  0.2  Iris-setosa\n",
       "2   4.7  3.2  1.3  0.2  Iris-setosa\n",
       "3   4.6  3.1  1.5  0.2  Iris-setosa\n",
       "4   5.0  3.6  1.4  0.2  Iris-setosa\n",
       "5   5.4  3.9  1.7  0.4  Iris-setosa\n",
       "6   4.6  3.4  1.4  0.3  Iris-setosa\n",
       "7   5.0  3.4  1.5  0.2  Iris-setosa\n",
       "8   4.4  2.9  1.4  0.2  Iris-setosa\n",
       "9   4.9  3.1  1.5  0.1  Iris-setosa\n",
       "10  5.4  3.7  1.5  0.2  Iris-setosa\n",
       "11  4.8  3.4  1.6  0.2  Iris-setosa\n",
       "12  4.8  3.0  1.4  0.1  Iris-setosa\n",
       "13  4.3  3.0  1.1  0.1  Iris-setosa\n",
       "14  5.8  4.0  1.2  0.2  Iris-setosa\n",
       "15  5.7  4.4  1.5  0.4  Iris-setosa\n",
       "16  5.4  3.9  1.3  0.4  Iris-setosa\n",
       "17  5.1  3.5  1.4  0.3  Iris-setosa\n",
       "18  5.7  3.8  1.7  0.3  Iris-setosa\n",
       "19  5.1  3.8  1.5  0.3  Iris-setosa"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See first 20 rows of the dataset\n",
    "iris.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily convert the string values to integer values using the [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). The three class values (Iris-setosa, Iris-versicolor, Iris-virginica) are mapped to the integer values (0, 1, 2).\n",
    "\n",
    "In this case, the fourth column/feature of the datset contains non-numeric values. So you need to separate it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a NumPy array\n",
    "iris = iris.values\n",
    "\n",
    "# Separate\n",
    "Y = iris[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Label Encode string class values as integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's study another area where the need of elementary knowledge of statistics is very crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics for model evaluation:\n",
    "\n",
    "You have designed and developed your machine learning model. Now, you want to evaluate the performance of your model on the test data. In this regards, you take help of various statistical metrics like Precision, Recall, ROC, AUC, RMSE etc. You also take help various data resampling techniques such as **k-fold Cross Validation**. \n",
    "\n",
    "Statistics can effectively be used to:\n",
    "- Estimate a hypothesis accuracy\n",
    "- Determine the error of two hypotheses\n",
    "- Compare learning algorithms\n",
    "\n",
    "_It is important to note that hypotheses are refer to learned models; the results of running a learning algorithm on a dataset. Evaluating and comparing hypotheses means comparing learned models, which is different from evaluating and comparing machine learning algorithms, which could be trained on different samples from the same problem or different problems._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's study Gaussian and Descriptive statistics now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Gaussian and Descriptive stats: \n",
    "\n",
    "A sample of data is nothing but a snapshot from a broader population of all possible observations that could be taken from a domain or generated by a process.\n",
    "\n",
    "Interestingly, many observations fit a common pattern or distribution called the normal distribution, or more formally, the Gaussian distribution. It is the bell-shaped distribution that you may be familiar with. Following figure denotes a Gaussian distribution:\n",
    "\n",
    "<img src = \"http://hyperphysics.phy-astr.gsu.edu/hbase/Math/immath/gauds.gif\"></img>\n",
    "\n",
    "_**Source: HyperPhysics**_\n",
    "\n",
    "A lot is known about the Gaussian distribution, and as such, there are whole sub-fields of statistics and statistical methods that can be used with Gaussian data.\n",
    "\n",
    "Any Gaussian distribution, and in turn any data sample drawn from a Gaussian distribution, can be summarized with just two parameters:\n",
    "\n",
    "- **Mean**: The central tendency or most likely value in the distribution (the top of the bell).\n",
    "- **Variance**: The average difference that observations have from the mean value in the distribution (the spread).\n",
    "\n",
    "The units of the mean are the same as the units of the distribution, although the units of the variance are squared, and therefore harder to interpret. A popular alternative to the variance parameter is the standard deviation, which is simply the square root of the variance, returning the units to be the same as those of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean, variance, and standard deviation can be calculated directly on data samples. You will be doing it right now using NumPy.\n",
    "\n",
    "The example below generates a sample of 100 random numbers drawn from a Gaussian distribution with a known mean of 50 and a standard deviation of 5 and calculates the summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you will import all the dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Dependencies\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import var\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you set the random number generator seed so that your results are reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate univariate observations\n",
    "data = 5 * randn(10000) + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 50.049\n",
      "Variance: 24.939\n",
      "Standard Deviation: 4.994\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "print('Mean: %.3f' % mean(data))\n",
    "print('Variance: %.3f' % var(data))\n",
    "print('Standard Deviation: %.3f' % std(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough, eh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's study the next topic now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable correlation:\n",
    "\n",
    "There may be complex and unknown relationships between the variables in your dataset.\n",
    "\n",
    "It is important to discover and quantify the degree to which variables in your dataset are dependent upon each other. This knowledge can help you better prepare your data to meet the expectations of machine learning algorithms, such as linear regression, whose performance will degrade with the presence of these interdependencies.\n",
    "\n",
    "Variables within a dataset can be related for lots of reasons.\n",
    "\n",
    "For example:\n",
    "\n",
    "One variable could cause or depend on the values of another variable.\n",
    "One variable could be lightly associated with another variable.\n",
    "Two variables could depend on a third unknown variable.\n",
    "It can be useful in data analysis and modeling to better understand the relationships between variables. The statistical relationship between two variables is referred to as their correlation.\n",
    "\n",
    "A correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variables value increases, the other variables values decrease. Correlation can also be neural or zero, meaning that the variables are unrelated.\n",
    "\n",
    "- Positive Correlation: both variables change in the same direction.\n",
    "- Neutral Correlation: No relationship in the change of the variables.\n",
    "- Negative Correlation: variables change in opposite directions.\n",
    "\n",
    "Correlation measurements form the fundamental of filter-based feature selection techniques. \n",
    "\n",
    "You can quantify the relationship between samples of two variables using a statistical method called Pearsons correlation coefficient, named for the developer of the method, **Karl Pearson**.\n",
    "\n",
    "The pearsonr() function of NumPy  can be used to calculate the Pearsons correlation coefficient for samples of two variables.\n",
    "\n",
    "You will now calculate where one variable is dependent upon the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First things first\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# seed random number generator\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data1 = 20 * randn(1000) + 100\n",
    "data2 = data1 + (10 * randn(1000) + 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons correlation: 0.888\n"
     ]
    }
   ],
   "source": [
    "# Calculate Pearson's correlation\n",
    "corr, p = pearsonr(data1, data2)\n",
    "# Display the correlation\n",
    "print('Pearsons correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, you will study non-parametric statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric statistics: \n",
    "\n",
    "A large portion of the field of statistics and statistical methods is dedicated to data where the distribution is known.\n",
    "\n",
    "Data in which the distribution is unknown or cannot be easily identified is called nonparametric.\n",
    "\n",
    "In the case where you are working with nonparametric data, specialized nonparametric statistical methods can be used that discard all information about the distribution. As such, these methods are often referred to as distribution-free methods.\n",
    "\n",
    "Before a nonparametric statistical method can be applied, the data must be converted into a rank format. As such, statistical methods that expect data in rank format are sometimes called _rank statistics_, such as rank correlation and rank statistical hypothesis tests. Ranking data is exactly as its name suggests.\n",
    "\n",
    "The procedure is as follows:\n",
    "\n",
    "- Sort all data in the sample in ascending order.\n",
    "- Assign an integer rank from 1 to N for each unique value in the data sample.\n",
    "\n",
    "A widely used nonparametric statistical hypothesis test for checking for a difference between two independent samples is the _**Mann-Whitney U test**_, named for Henry Mann and Donald Whitney.\n",
    "\n",
    "It is the nonparametric equivalent of the _Students t-test_ but does not assume that the data is drawn from a Gaussian distribution.\n",
    "\n",
    "You will implement this test in Python via the mannwhitneyu() SciPy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The dependencies that you need\n",
    "from scipy.stats import mannwhitneyu\n",
    "from numpy.random import rand\n",
    "\n",
    "# seed the random number generator\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics = 4077.000, p = 0.012\n",
      "Different distribution (reject H0)\n"
     ]
    }
   ],
   "source": [
    "# Generate two independent samples\n",
    "data1 = 50 + (rand(100) * 10)\n",
    "data2 = 51 + (rand(100) * 10)\n",
    "# Compare samples\n",
    "stat, p = mannwhitneyu(data1, data2)\n",
    "print('Statistics = %.3f, p = %.3f' % (stat, p))\n",
    "# Interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Different distribution (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other two popular non-parametric statistical significance tests that you can use are:\n",
    "\n",
    "- Friedman test\n",
    "- Wilcoxon signed-rank test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That calls for a wrap up!\n",
    "\n",
    "You have finally made it to the end. In this article, you studied a variety of essential statistical concepts that play very crucial role in your machine learning projects. So, understanding them is just important. \n",
    "\n",
    "From mere an introduction to statistics, you took it to statistical rankings that too with several implementations. That is definitely quite a feat. Next are some links for you if you want to take things further:\n",
    "\n",
    "- [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "- [Machine Learning book by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf)\n",
    "- [All for Statistics](https://www.ic.unicamp.br/~wainer/cursos/1s2013/ml/livro.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the resources I took help from for writing this blog: \n",
    "- [Machine Learning Mastery mini course on Statistics](https://machinelearningmastery.com/statistics-for-machine-learning-mini-course/)\n",
    "- https://www.khanacademy.org/math/statistics-probability\n",
    "- [Statistical Learning course by Stanford University](https://statlearning.class.stanford.edu/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me know your views/queries in the comments section. Also, check out [DataCamp's course on \"Statistical Thinking in Python\"](https://www.datacamp.com/courses/statistical-thinking-in-python-part-1) which is very practically aligned. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
