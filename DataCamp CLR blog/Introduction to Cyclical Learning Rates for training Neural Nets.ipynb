{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Cyclical Learning Rates for training Neural Nets\n",
    "\n",
    "**Learn what cyclical learning rate policy and how it can improve the training of a neural network.**\n",
    "\n",
    "(This tutorial assumes that the reader is familiar with the basics of neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network is no more an uncommon phrase to the Computer Science society or let's say to the society in general. The main reason that makes it so cool is not just the amount of real world problems it is solving, but also the kind of problems it is solving. How can they be so varied? \n",
    "\n",
    "Be it in the field of Cognitive Psychology, be it in the domain of Cyber Security, be it in the domain of Health-care (You are not considering Computer Vision, Computer Graphics, Natural Language Processing etc. for the time being. Let's name the more uncommon ones!), almost each and every industry is getting tremendously benefited by the intelligence and automation a neural network has to offer.  \n",
    "\n",
    "But why? This is question that keeps coming and coming! Well, the answer for this is still under active research because Neural Networks are quite blackbox in nature and its resemblance with a brain makes this question more complex. Anyway, answering that question is not the objective  of this post. \n",
    "\n",
    "One thing is for sure! In order to get expected results from a neural network the one thing that has to be ensured is its **Training**. And by now, you already might have discovered _Training a very large_ calls for a tremendous amount of computation power. Without good GPUs, SSDs it is _almost_ impossible to train a _very large_ neural network. Now by _very large_ how much large is meant? Well, large enough to produce good results on an [ImageNet](https://www.image-net.org/) dataset. Because, that is kind of a benchmark. \n",
    "\n",
    "But this very idea of training very large neural networks got completely revolutionized when a team of talented researchers from [Fast.ai](http://www.fast.ai) was able to beat Google's model achieving an accuracy of 93% accuracy in just 18 minutes that too in just $40.\n",
    "\n",
    "Sounds interesting? Read on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what were the key ingredients behind this to occur? State-of-the-art GPUs? State-of-the-art TPUS? State-of-the-art SSDs?\n",
    "\n",
    "Absolutely not. The team's configuration was quite simple as the cost is only $40. The key ingredient was the use of state-of-the-art algorithms to train the neural network. [In this blog](http://www.fast.ai/2018/08/10/fastai-diu-imagenet/), the great researcher and educator [Jeremy Howard](https://twitter.com/jeremyphowardref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) has discussed the main reasons of this big win. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is classic example where the power of costly hardwares gets lost to the power of powerful algorithms. In this post, you are going to uncover the details of one such technique that can ensure a neural network is trained with the best possible learning rate. This technique is known as **Cyclical Learning Rate**. This was proposed way back in 2015 by [Leslie N. Smith](https://mlconf.com/interview-with-leslie-n-smith-phd-senior-research-scientist-at-the-us-naval-research-laboratory-by-reshama-shaikh-program-committee-member/). You can check the original paper [here](https://arxiv.org/pdf/1506.01186).\n",
    "\n",
    "But why are you going to cover only learning rate when there are other important hyperparameters like dropout rate, activation functions? Because, learning rate is the most important one among them. Just that!\n",
    "\n",
    "In this post, you are going to study: \n",
    "- Quickly revisit why learning rates are needed? \n",
    "- What are the techniques available for finding the most suitable learning rate for a neural network?\n",
    "- Introduction to cyclical learning rates\n",
    "- Inner mechanics of cyclical learning rates\n",
    "- A case study in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are learning rates needed?\n",
    "\n",
    "Let's quickly revisit the main purpose of using learning rates for training a neural net. \n",
    "\n",
    "Learning rate is a [hyperparameter](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning) that controls how much you are adjusting the weights of our network with respect the loss gradient. What? Why gradients are coming in the picture? Because, you are in your way to optimize a neural network you have just created with [**gradient descent**](https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/). Now, essentially the  goal of gradient descent is to find the minima of the loss function your neural network is trying to optimize. Take a look at the following image [taken from Andrew Ng's Deep Learning course on Coursera]:\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/max/800/0*00BrbBeDrFOjocpK.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term in the rectangle is the update rule with which the network starts to learn its parameter $\\theta$1 where $\\alpha$ is the learning rate. \n",
    "\n",
    "In the first curve, the lower most point is the minima of the loss function. Suppose, in the current iteration, your network is in near the left topmost point. Now, in order to converge to the lowest point you take partial derivatives of the loss function $J(\\theta1)$ and compute the gradients for getting the directions towards arriving at that lowest point. \n",
    "\n",
    "In order to arrive a bit faster at that point, you add another term $\\alpha$ which is the learning rate. The lower the value, the slower you travel along the downward slope. While this might be a good idea (using a low learning rate) in terms of making sure that you do not miss any local minima, it could also mean that you’ll be taking a long time to converge — especially if you get stuck on a plateau region. \n",
    "\n",
    "That was a quick recap of the objectives of learning rates in simple words. Now you will study the techniques of choosing a good learning rate for your neural network.  \n",
    "\n",
    "<img src = \"https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png\"></img>\n",
    "**[Source: Jeremy Jordan's blog](https://www.jeremyjordan.me/nn-learning-rate/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the techniques available for finding the most suitable learning rate for a neural network?\n",
    "\n",
    "There is no fixed learning rate for a neural network. It depends on the kind of problem you are working on, the kind of data you are feeding to your network and most importantly the structure of the network which varies from problem to problem. Handpicking a learning rate is a very painful task because in case you are training a large network you can incur huge amount of costs for this. And it is very time consuming as well. \n",
    "\n",
    "Should you run standard hyperparameter optimization methods like Grid Search or Random Search? \n",
    "- That is again horrible for large network. But why you keep coming to large networks? Because, almost any real world complex problem will need a large neural network.\n",
    "\n",
    "Still now, the most common practice is to set the learning rate to a constant value and decrease it by an order of magnitude once the accuracy has plateaued.\n",
    "\n",
    "Therefore, there is a clear need of a systematic technique which can simplify the process of choosing a good learning rate for a particular neural network. Not only this, but also, there has to be sufficient amount of reasons which would support that approach as to why it is trust-worthy. \n",
    "\n",
    "It seems Cyclical Learning Rates (CLR) appeared just in time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to cyclical learning rates:\n",
    "\n",
    "The objectives of cyclical learning rate (CLR) are two-fold:\n",
    "- CLR gives  an approach for setting the global learning rates for training neural networks that eliminates the need to perform tons experiments to find the best values with no additional computation.\n",
    "- CLR provides a good learning rate range (LR range) for an experiment by introducing the concept of **LR Range Test**. \n",
    "\n",
    "You will study this section merely for building your intuition of how CLR works. In the next section, you will dive into more details. In the previous sections, you briefly understood why learning rates are anyway used. Let's again recall it.  \n",
    "\n",
    "An ideal learning rate would be the one which performs a steep decrease in the network's loss. Here comes the wizardry of CLR.  The original CLR paper talks about an experiment wherein you can observe the behavior of learning rate with respect to the loss. The experiment is very easy to visualize where you gradually increase the learning rate after each _mini-batch_, recording the loss at each increment. This gradual increase can be either be linear or be exponential. And yes, this is essentially the _LR Range Test_. \n",
    "\n",
    "After performing the experiment Leslie showed us, for too low learning rates, the loss may decrease, but at a very shallow rate. When entering the optimal learning rate zone, you'll observe a quick drop in the loss function. If you further increase the learning rate then it can cause parameter loss in the network which in turn might lead to loss increase. So, from this experiment it is clear that you are interested in a steep decrease of the loss function and for that you can analyze the gradients of the loss function at different stages of the training. \n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/freeze/max/1000/1*VAmbyfpR0_-gP0oIla0Vjw.png?q=20\"></img>\n",
    "\n",
    "**[Source: Jeremy Jordan's blog](https://www.jeremyjordan.me/nn-learning-rate/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, from the above graph, you can easily spot three different phases where the loss does not change much, then comes a times a steep decrease happen and then again the loss starts to increase again slowly. \n",
    "\n",
    "Can you see the steepest decrease among all the other decreases? Yes, essentially you want to end up in that range and CLR will give you a disciplined approach in finding it. Getting the feel? Let's find out more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delving more with CLR:\n",
    "\n",
    "The above observation gives you an important point to consider: \n",
    "\n",
    "The idea of letting the learning rate vary within a range of values rather than adopting a linearly or exponentially decreasing value. You can do this by setting a definite range of learning rates and then instead going for any linear or exponential variation, you cyclically vary the learning rates from the defined range. Leslie considered the following function forms in order to cyclically vary the learning rate: \n",
    "\n",
    "- Triangular window (linear)\n",
    "- Welch window (parabolic) \n",
    "- Hann window (sinusoidal) \n",
    "\n",
    "But all of the forms produced equivalent results and in the original paper the idea is only presented with triangular from \n",
    "(linearly increasing then linearly decreasing) because of its simplicity. This policy is referred to as _triangular learning rate policy_ as well. Refer to the following image in order to think about the triangular form visually:\n",
    "\n",
    "<img src = \"https://image.ibb.co/nqT4PK/Capture.jpg\">\n",
    "**Source: The original CLR paper mentioned in the beginning of the tutorial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the triangular learning rate policy, following policies were also presented in the paper:\n",
    "\n",
    "1. **triangular2** -  It is as same as the triangular policy except the learning rate difference is made half at the end\n",
    "of each cycle. This means the learning rate difference drops after each cycle.\n",
    "2. **exp range** - In this case, the learning rate varies between the minimum and maximum boundaries and each boundary value declines by an exponential factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question you might have quickly asked yourself at this point of time is - How can one estimate reasonable minimum and\n",
    "maximum boundary values? Remember the LR Range Test that you studied just a moment ago? Now, you should be able to find its relevance better. Let's do quick case study now to see how CLR can smoothen your experiments. \n",
    "\n",
    "(To the very end of this tutorial, you will discover some more advancements over CLR that have been proposed recently, but by now, you already have understood the worth of CLR. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A case study of CLR in Python:\n",
    "\n",
    "You will be doing this using the classic MNIST dataset which is probably the most popular dataset for getting started into Computer Vision and Deep Learning. Check out [this blog](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) if you want learning about MNIST in a very detailed manner. You will use `keras` extensively for all purposes of the experiment. `keras` provides a built-in version of the dataset. You will start off your experiment by importing that and by performing some basic EDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have imported the dataset successfully. Now, you will do some basic visualizations of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS8AAABcCAYAAAA/HzMDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMLzEGi/DMLzEGi/DMLzEGi/DMLzk/871LAiQWsa8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Plot 4 images as gray scale\n",
    "plt.subplot(152)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(153)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(154)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(155)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great! You will straight proceed towards building a simple multi-layer neural network. But before that, you will do some basic data preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you do?\n",
    "\n",
    "The images in the dataset are of 28*28 dimensions which is difficult to accommodate in a simple multi layer neural network. That is why, you converted the images to a single dimension where each image contains 784 pixel data using the `reshape()` function. \n",
    "\n",
    "The pixel values in the images are in the range of 0 - 255. A good idea will be to decrease this even further by normalizing the range to 0 - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output variable is an integer from 0 to 9. This is a multi-class classification problem. You will perform one-hot encoding of the class labels for getting a vector of class integers into a binary matrix. You will do this to do a “binarization” of the category and so that you can include it as a feature to train the neural network.\n",
    "\n",
    "You can easily do this using the built-in `np_utils.to_categorical()` helper function in `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will define the structure of your network. You will use a simple fully-connected network for this purpose. In `keras` this is typically three-step process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what all you did in the above code. You are constructing the network in a sequential manner (which is a linear stack of layers). Then you started to add the layer in your network where in the first layer you added the neurons (which is equal to the number of pixels in an image i.e. 784) and you specified the input dimension of the images which is in this case as same as the number of the pixels. You instructed your network to get itself initialized  with weights from a _normal_ distribution. Finally, you supplied `relu` as the activation function for the first layer. \n",
    "\n",
    "In the final layer, you kept the number of the neurons to 10 (which is the number of class labels) and you provided the activation to be `softmax` to turn the outputs into probability-like values and allow one class of the 10 to be selected as the model’s output prediction.\n",
    "\n",
    "You compiled the model as to decide the optimization method of the model (which is `ADAM` in this case) and the which kind loss this method will optimize (which is `categorical_loss` in this case).\n",
    "\n",
    "Now you will train the model and record the time it took to get trained. You will also test its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 10s - loss: 0.2774 - acc: 0.9207 - val_loss: 0.1362 - val_acc: 0.9610\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.1113 - acc: 0.9675 - val_loss: 0.0951 - val_acc: 0.9720\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0712 - acc: 0.9793 - val_loss: 0.0802 - val_acc: 0.9750\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0503 - acc: 0.9857 - val_loss: 0.0687 - val_acc: 0.9788\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0360 - acc: 0.9897 - val_loss: 0.0632 - val_acc: 0.9797\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0267 - acc: 0.9928 - val_loss: 0.0643 - val_acc: 0.9784\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.0201 - acc: 0.9951 - val_loss: 0.0633 - val_acc: 0.9802\n",
      "Epoch 8/10\n",
      " - 8s - loss: 0.0150 - acc: 0.9962 - val_loss: 0.0613 - val_acc: 0.9808\n",
      "Epoch 9/10\n",
      " - 8s - loss: 0.0108 - acc: 0.9978 - val_loss: 0.0625 - val_acc: 0.9806\n",
      "Epoch 10/10\n",
      " - 8s - loss: 0.0076 - acc: 0.9988 - val_loss: 0.0612 - val_acc: 0.9809\n",
      "Time taken for the Network to train :  86.4216202873591\n",
      "Baseline Error: 1.91%\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# For fixing the reproducibility\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "# Fit the model\n",
    "startTime = timeit.default_timer()\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "elapsedTime = timeit.default_timer() - startTime\n",
    "print(\"Time taken for the Network to train : \",elapsedTime)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple model did actually quite well achieving an error rate of just 1.91% in  approximate 87 seconds. Now you will see the power of CLR. You will start off by cloning the [`keras` implementation of CLR](https://github.com/bckenstler/CLR) from Github. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a successful clone, you should have the following files into your local working directory. \n",
    "\n",
    "<img src = \"https://image.ibb.co/nBjxBz/Capture.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLR policy is implemented as a [`keras callback`](https://keras.io/callbacks/) here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "from clr_callback import *\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# You are using the triangular learning rate policy and \n",
    "#  base_lr (initial learning rate which is the lower boundary in the cycle) is 0.1 \n",
    "clr_triangular = CyclicLR(mode='triangular')\n",
    "model.compile(optimizer=Adam(0.1), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will pass on this `clr_triangular` to the `callbacks` parameter while fitting the network. You will use a larger `batch_size` this time. You will record the time as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.0046 - acc: 0.9996 - val_loss: 0.0571 - val_acc: 0.9831\n",
      "Epoch 2/10\n",
      " - 4s - loss: 0.0030 - acc: 0.9998 - val_loss: 0.0575 - val_acc: 0.9829\n",
      "Epoch 3/10\n",
      " - 4s - loss: 0.0023 - acc: 0.9999 - val_loss: 0.0594 - val_acc: 0.9827\n",
      "Epoch 4/10\n",
      " - 4s - loss: 0.0018 - acc: 0.9999 - val_loss: 0.0589 - val_acc: 0.9835\n",
      "Epoch 5/10\n",
      " - 4s - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0595 - val_acc: 0.9831\n",
      "Epoch 6/10\n",
      " - 4s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0600 - val_acc: 0.9836\n",
      "Epoch 7/10\n",
      " - 4s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0612 - val_acc: 0.9832\n",
      "Epoch 8/10\n",
      " - 4s - loss: 8.2190e-04 - acc: 1.0000 - val_loss: 0.0621 - val_acc: 0.9829\n",
      "Epoch 9/10\n",
      " - 4s - loss: 6.6182e-04 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9836\n",
      "Epoch 10/10\n",
      " - 4s - loss: 5.7177e-04 - acc: 1.0000 - val_loss: 0.0635 - val_acc: 0.9836\n",
      "Time taken for the Network to train :  43.23223609056981\n",
      "Baseline Error: 1.64%\n"
     ]
    }
   ],
   "source": [
    "startTime = timeit.default_timer()\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=2000,callbacks=[clr_triangular], verbose=2)\n",
    "elapsedTime = timeit.default_timer() - startTime\n",
    "print(\"Time taken for the Network to train : \",elapsedTime)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you spot the advantage? This is something really amazing to see! You model took only 44 seconds to get trained and yielded even a better error rate than the previous one. Very good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have made it to the end. In this tutorial, you studied a very crucial problem of finding a suitable learning rate and how CLR completely changed the way you used to approach this problem. You studied CLR covering a good amount details and did a small experiments as to see how CLR can produce some very good results in a less time. \n",
    "\n",
    "Now what next?\n",
    "The two byproducts that have come out from CLR are:\n",
    "- [Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983)also known as **Cosine Annealing**\n",
    "- [Differential Learning Rates](https://blog.slavv.com/differential-learning-rates-59eff5209a4f)\n",
    "\n",
    "Study the above two approaches in order to get even more insights in this topic. Also, after CLR Leslie published a paper titled [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay](https://arxiv.org/abs/1803.09820) which revisits CLR and discusses efficient methods for choosing the values of other important hyperparameters of a neural network. Leslie also revisited one of his techniques called **Super Convergence** in this paper. This paper is a must read for anyone, who thinks, eats and sleeps neural networks.\n",
    "\n",
    "Limited applicability is one of the major shortcomings of CLR. It has to be made full-proof before one can use it in production levels. \n",
    "\n",
    "If you are interested in knowing more about Neural Networks, you should definitely take DataCamp's [Deep Learning in Python](https://www.datacamp.com/courses/deep-learning-in-python) course which is very well designed and is taught by Dan Becker (Head of Kaggle Learn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
