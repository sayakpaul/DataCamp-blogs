{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data in SQL\n",
    "\n",
    "In this tutorial, learn about techniques to clean messy data, a must-have skill for a data scientist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world data is almost always messy. As a data scientist or a data analyst or even as a developer, if you need to discover facts about data, it is vital to ensure that data is tidy enough for doing that. There is actually a well-rounded definition of _tidy data_ and you can follow [this paper](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf) by [Hadley Wickham](https://en.wikipedia.org/wiki/Hadley_Wickham) to know more about it. \n",
    "\n",
    "In this tutorial, you will be practicing some of the most common data cleaning techniques in SQL. You will create your own dummy dataset but the techniques can be applied to the real world data (of tabular form) as well. The contents of this tutorial are as follows:\n",
    "\n",
    "- Different data types and their messy values\n",
    "- Problems that can raise from messy numbers\n",
    "- Cleaning numeric values\n",
    "- Messy strings\n",
    "- Cleaning string values\n",
    "- Messy date values and cleaning them\n",
    "- Duplications and removing them\n",
    "\n",
    "Lots of things to cover. Let's begin! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that you should already be knowing how to write basics SQL queries in PostgreSQL (the RDBMS you will be using in this tutorial). If you need to revise the concepts then following might be some good resources - \n",
    "- DataCamp's [Intro to SQL for Data Science](https://www.datacamp.com/courses/intro-to-sql-for-data-science?tap_a=5644-dce66f&tap_s=357540-5b28dd) course\n",
    "- [Beginner's Guide to PostgreSQL](https://www.datacamp.com/community/tutorials/beginners-introduction-postgresql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different data types, their messy values and remedies\n",
    "\n",
    "In the tabular forms of data the most common data-types are - `string`, `numeric` or `date-time`. You can encounter messy values across all of these types. Let's now take each of these types and see some examples of their respective messy values. Let's start with the `numeric` type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messy numbers\n",
    "\n",
    "Numbers can be in messy forms in a number of ways. Here, you will be introduced to the most common ones - \n",
    "\n",
    "- **Undesired type/Type mismatch**: Consider there is a column named `age` in a dataset you are working with. You see the values that are present in that column are of `float` type - the sample values are like 23.0, 45.0, 34.0 and so on. In this case, you don't need the `age` column to be of `float` type. Isn't it so? \n",
    "\n",
    "- **Null values**: While this particular is common all the data-types mentioned above. Null values here simply means that the values are not available/blank. However, null values can be present in other forms as well. Take the [Pima Indian Diabetes dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database) for example. The dataset contains zero values for columns like `Plasma glucose concentration`, `Diastolic blood pressure` which are practically invalid. If you perform any statistical analysis on the dataset without taking care of this invalid entries your analysis will not be accurate. \n",
    "\n",
    "Let's now study the problems that can get raised from these issues and how to deal with them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with messy numbers and dealing with them\n",
    "\n",
    "Let's now take a look at the most common problems that you may face if you do not clean the messy data (w.r.t to the above-mentioned types).\n",
    "\n",
    "### 1. Data aggregation \n",
    "\n",
    "Suppose you have null entries for a numeric column and you calculating summary statistics (like mean, maximum, minimum values) on that column. The results will not get conveyed accurately in this case. Again consider the _Pima Indian Diabetes_ dataset with the invalid zero entries. If you calculated summary statistics on the columns as mentioned before, would you get the right results? Won't the results be erroneous? So, how to address this problem? There are several ways - \n",
    "\n",
    "    - Removing the entries containing missing/null values (not recommended)\n",
    "    - Imputing the null entries with a numeric value (typically with mean or median of the respective column)\n",
    "\n",
    "Let's now get hands-on with these problems and the second option for combating null values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following PostgreSQL table named `entries` - \n",
    "\n",
    "![](images/Capture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see two null entries in the above table. Suppose you want to the get the average weight value from this table and you executed the following query - \n",
    "\n",
    "```sql\n",
    "select avg(weight_in_lbs) as average_weight_in_lbs from entries;\n",
    "```\n",
    "\n",
    "You got `90.45` as the output. Is this correct? So, what can be done? Let's fill the null entry with this average value with the help of the `COALESCE()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill the missing values first with `COALESCE()` (remember that `COALESCE()` does not change the values in the original table, it just returns a temporary view of the table with the values changed) - \n",
    "```sql\n",
    "select *, COALESCE(weight_in_lbs, 90.45) as corrected_weights from entries;\n",
    "```\n",
    "\n",
    "You should get an output like - \n",
    "![](images/Capture2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can apply the `AVG()` again - \n",
    "\n",
    "```sql\n",
    "select avg(corrected_weights) from\n",
    "(select *, COALESCE(weight_in_lbs, 90.45) as corrected_weights from entries) as subquery;\n",
    "```\n",
    "\n",
    "This is a much more accurate result than the earlier one. Let's now study another problem that can take place if you have mismatches in the column data-types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Table joins\n",
    "\n",
    "Consider you are working with the following tables `student_metadata` and `department_details` - \n",
    "\n",
    "![](images/Merged1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the `student_mtadata` table, `dept_id` is of integer type and in the `department_details` table, it is present in text type. Now, suppose, you want to join these two tables and want to produce a report which will contain the following columns - \n",
    "- id\n",
    "- name\n",
    "- dept_name\n",
    "\n",
    "To do this, you run this query - \n",
    "\n",
    "```sql\n",
    "select id, name, dept_name from\n",
    "student_metadata s join department_details d\n",
    "on s.dept_id = d.dept_id;\n",
    "```\n",
    "\n",
    "You will encounter this error, then - \n",
    "> ERROR:  operator does not exist: smallint = text\n",
    "\n",
    "Here's an amazing infographic which depicts this problem (from DataCamp's [Reporting in SQL](https://www.datacamp.com/courses/reporting-in-sql) course) - \n",
    "\n",
    "![](images/Capture3.png)\n",
    "\n",
    "This happening because the data-types are not getting matched while joining the two tables. Here, you can `CAST` the `dept_id` column in the `department_details` table to integer while joining the tables. Here's how to do that - \n",
    "\n",
    "```sql\n",
    "select id, name, dept_name from\n",
    "student_metadata s join department_details d\n",
    "on s.dept_id = cast(d.dept_id as smallint);\n",
    "```\n",
    "\n",
    "And you get your desired report - \n",
    "![](images/Capture6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now discuss how strings can be present in messy forms, their problems and ways to deal with them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messy strings and cleaning them\n",
    "\n",
    "String values are also very common. Let's start this section by looking at the values of a column `dept_name` (denoting department names) taken from a table named `student_details` - \n",
    "\n",
    "![](images/Capture7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String values like the above can cause a lot of unexpected problems. `I.T`, `Information Technology` and `i.t` all mean the same department i.e. _Information Technology_ and suppose the specification document requires the values to be present as `I.T` only.  Now, say, you want to count the number of students belonging to the department of `I.T.` and you run this query - \n",
    "\n",
    "```sql\n",
    "select dept_name, count(dept_name) as student_count\n",
    "from student_details\n",
    "group by dept_name;\n",
    "```\n",
    "\n",
    "And you get - \n",
    "![](images/Capture8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a right report? - No! So, how can you address this problem? \n",
    "\n",
    "Let's first identify the problem in a more detailed way: \n",
    "- You have `Information Technology` as a value which should be converted to `I.T` and\n",
    "- You have `i.t` as another value which should be converted to `I.T`. \n",
    "\n",
    "In the first case, you can simply `REPLACE` the value `Information Technology` to `I.T` and in the second case you convert the character to `UPPER` case. You can accomplish this in a single query though it is advised to address this kind of problems in a step-by-step fashion. Here's the query to address the problem - \n",
    "\n",
    "```sql\n",
    "select upper(replace(dept_name, 'Information Technology', 'I.T')) as dept_cleaned, \n",
    "count(dept_name) as student_count\n",
    "from student_details\n",
    "group by dept_cleaned;\n",
    "```\n",
    "\n",
    "And the report - \n",
    "![](images/Capture9.png)\n",
    "\n",
    "You read more about PostgreSQL string functions [here](https://www.postgresql.org/docs/9.1/functions-string.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now discuss some example where `date` values can be messy and what you can do to clean them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messy dates and cleaning them\n",
    "\n",
    "Consider you are working with a table named `employees` which contains a column called `birthdate` but not in an appropriate date type. Now, you want execute queries with dedicated `date` functions like `DATE_PART()`. You will not be able to do that until and unless you `CAST` the `birthdate` column to `date` type. Let's see this in action. \n",
    "\n",
    "**Consider** the `birthdate`s to be in the `YYYY-MM-DD` format.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the `employees` table looks like - \n",
    "\n",
    "![](images/Capture10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you run the following query to extract the months from the birthdates - \n",
    "\n",
    "```sql\n",
    "select date_part('month', birthdate) from employees;\n",
    "```\n",
    "\n",
    "And you instantly get this error - \n",
    "> ERROR:  function date_part(unknown, text) does not exist\n",
    "\n",
    "Along with the error, you get a very good hint also - \n",
    "> HINT:  No function matches the given name and argument types. You might need to add explicit type casts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's follow the hint and `CAST` the birthdate to the appropriate `date` type and then apply `DATE_PART()` - \n",
    "\n",
    "```sql\n",
    "select date_part('month', CAST(birthdate AS date)) as birthday_months from employees;\n",
    "```\n",
    "\n",
    "You should get the result - \n",
    "![](images/Capture11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now proceed to the pre-concluding section of this tutorial where you will study the effects of data duplications and how you can tackle them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data duplications: Causes, effects and solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will be studying some of the most common causes which lead to data duplications. You will also see their effects and some of the ways using which prevent them. Consider the following two tables `band_details` and `some_festival_record` - \n",
    "\n",
    "![](images/Merged2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table `band_details` conveys information about musical bands, it contains their identifiers, names and the total shows they have delivered. On the other hand, the table `some_festival_record` portrays a hypothetical music festival and contains records about the bands performed there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose you want to produce a report which should contain band names, their show counts, and the total number of times they have performed at the festival. `INNER` joining is needed here. You run the following query - \n",
    "\n",
    "```sql\n",
    "select band_name, sum(total_show_count) as total_shows, sum(performed) as total_times_performed\n",
    "from band_details b join some_festival_record s\n",
    "on b.id = s.band_id\n",
    "group by band_name;\n",
    "```\n",
    "\n",
    "And the query produces - \n",
    "\n",
    "![](images/Capture14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't you think the `total_shows` values are erroneous here? Because from the `band_details` table, you know that `Band_1` has delivered total 36 shows. Then what went wrong here? **Duplicates**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While joining the two tables, you mistakenly aggregated `total_show_count` column which caused a data duplication in the intermediate join results. If you remove the aggregation and modify the query accordingly you should get the desired results - \n",
    "\n",
    "```sql\n",
    "select band_name, total_show_count, sum(performed) as total_times_performed\n",
    "from band_details b join some_festival_record s\n",
    "on b.id = s.band_id\n",
    "group by band_name, total_show_count;\n",
    "```\n",
    "\n",
    "You get your expected results now - \n",
    "![](images/Capture15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way to prevent data duplication i.e. add another field in your `JOIN` clause so that the tables get joined on stricter conditions. \n",
    "\n",
    "You can use [this `.SQL`](https://bit.ly/2Ii34p0) file to generate the tables used here and also the values shown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking things further\n",
    "\n",
    "Thank you for reading the tutorial till the end. The tutorial introduced you to one of the most vital steps in the data analysis pipeline - _data cleaning_. You saw different forms of messy data and ways to tackle them. However, there is more advanced techniques to deal with more complex data cleaning problems and if you want to pursue something in that regard, the following are some excellent DataCamp courses which you can take - \n",
    "\n",
    "- [Joining Data in SQL](https://www.datacamp.com/courses/joining-data-in-postgresql?tap_a=5644-dce66f&tap_s=357540-5b28dd)\n",
    "- [Reporting in SQL](https://www.datacamp.com/courses/reporting-in-sql?tap_a=5644-dce66f&tap_s=357540-5b28dd)\n",
    "\n",
    "Feel free post your views in the `Comments` section. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
